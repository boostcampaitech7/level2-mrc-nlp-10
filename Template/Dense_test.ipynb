{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 학습 데이터셋 크기: 3340\n",
      "중복 제거 후 검증 데이터셋 크기: 235\n",
      "중복 제거 후 테스트 데이터셋 크기: 235\n"
     ]
    }
   ],
   "source": [
    "# Dense.py 파일의 모듈을 불러옴\n",
    "from Dense import DenseRetrievalModel, DenseRetrievalArguments, create_dataloader, train_dense, ContrastiveLoss, load_datasets\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dense_args = DenseRetrievalArguments(\n",
    "    data_route='/data/ephemeral/template_2/data/train_dataset',\n",
    "    test_data_route='/data/ephemeral/data/test_dataset',\n",
    "    model_name=\"klue/bert-base\",\n",
    "    num_negatives=1,\n",
    "    margin=0.5\n",
    ")\n",
    "\n",
    "# 데이터셋 로드\n",
    "train_dataset, validation_dataset, test_dataset = load_datasets(\n",
    "    sample_size=None,  # 원본 크기의 데이터셋 로드\n",
    "    data_path=dense_args.data_route\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DenseRetrievalModel(model_name=dense_args.model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '미국 상원', 'context': '미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\\\n\\\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\\\n\\\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05', 'question': '대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?', 'id': 'mrc-1-000067', 'answers': {'answer_start': [235], 'text': ['하원']}, 'document_id': 18293, '__index_level_0__': 42}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 형태 확인\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계\n",
    "all_contexts = [doc['context'] for doc in train_dataset]\n",
    "train_loader = create_dataloader(\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=model.tokenizer,\n",
    "    all_contexts=all_contexts,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_negatives=dense_args.num_negatives\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs 기준\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = ContrastiveLoss(margin=dense_args.margin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 209/209 [03:54<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 209/209 [03:54<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 209/209 [03:54<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#학습\n",
    "epochs = 3  # 학습 에포크 수\n",
    "train_dense(model, train_loader, optimizer, scheduler, criterion, device, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 저장되었습니다: ./retrieval_result/dense_retrieval_model.pth\n"
     ]
    }
   ],
   "source": [
    "#모델 저장\n",
    "model_save_path = \"./retrieval_result/dense_retrieval_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"모델이 저장되었습니다: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치가 ./retrieval_result/dense_retrieval_model.pth에서 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Documents: 100%|██████████| 1895/1895 [09:24<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 임베딩과 문서 ID, 내용을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "from Dense import DenseRetrievalModel, encode_documents\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 모델 경로 설정 및 로드\n",
    "model_path = \"./retrieval_result/dense_retrieval_model.pth\"  # 저장된 모델 경로\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = \"klue/bert-base\"  # 학습에 사용한 모델 이름\n",
    "model = DenseRetrievalModel(model_name=model_name).to(device)\n",
    "\n",
    "# 저장된 모델 가중치 로드\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # 모델을 평가 모드로 전환 (임베딩 생성 시 필요)\n",
    "\n",
    "print(f\"모델 가중치가 {model_path}에서 로드되었습니다.\")\n",
    "\n",
    "\n",
    "# 임베딩 생성 및 저장)\n",
    "# 문서 데이터 로드 (위키 문서 등)\n",
    "# 커스텀 위키 데이터셋 \n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, docs):\n",
    "        self.docs = docs\n",
    "        self.doc_ids = list(docs.keys())\n",
    "        self.contexts = [docs[doc_id]['text'] for doc_id in self.doc_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'id': self.doc_ids[idx],\n",
    "            'text': self.contexts[idx]\n",
    "        }\n",
    "\n",
    "# JSON 파일 로드\n",
    "with open(\"/data/ephemeral/data/wikipedia_documents.json\", 'r', encoding='utf-8') as f:\n",
    "    wiki_docs = json.load(f)\n",
    "\n",
    "# WikiDataset으로 변환\n",
    "wiki_dataset = WikiDataset(wiki_docs)\n",
    "\n",
    "# 문서 임베딩 생성\n",
    "doc_embeddings, doc_ids, contexts = encode_documents(\n",
    "    model=model,\n",
    "    wiki_docs=wiki_dataset,  \n",
    "    tokenizer=model.tokenizer,\n",
    "    device=device,\n",
    "    batch_size=32  # 필요에 따라 배치 크기를 조정\n",
    ")\n",
    "\n",
    "# 임베딩을 저장할 경로 설정\n",
    "embedding_save_path = \"./retrieval_result/Dense_embedding.npy\"\n",
    "np.save(embedding_save_path, doc_embeddings)\n",
    "\n",
    "# 문서 ID와 내용을 저장\n",
    "doc_ids_path = \"./retrieval_result/doc_ids.json\"\n",
    "contexts_path = \"./retrieval_result/contexts.json\"\n",
    "with open(doc_ids_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(doc_ids, f)\n",
    "with open(contexts_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(contexts, f)\n",
    "\n",
    "print(f\"문서 임베딩과 문서 ID, 내용을 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
