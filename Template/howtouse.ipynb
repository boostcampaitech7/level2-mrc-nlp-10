{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from arguments import Dense_search_retrieval_arguments, TF_IDF_retrieval_arguments\n",
    "import retrieval\n",
    "import gc\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Saved Faiss Indexer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TF_IDF_retrieval_arguments()\n",
    "model = retrieval.TF_IDFSearch(args)\n",
    "model.build_faiss()\n",
    "results1 = model.search_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3952\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 988\n",
      "  Number of trainable parameters = 136181760\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='988' max='988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [988/988 05:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.197351</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.980892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.051232</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.991597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-494\n",
      "Configuration saved in ./results/checkpoint-494/config.json\n",
      "Model weights saved in ./results/checkpoint-494/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-988\n",
      "Configuration saved in ./results/checkpoint-988/config.json\n",
      "Model weights saved in ./results/checkpoint-988/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-988 (score: 0.05123165622353554).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Faiss Indexer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer_config.json\n",
      "100%|██████████| 3504/3504 [00:29<00:00, 117.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss Indexer Saved.\n",
      "모든쿼리에 대해 탑 k를 찾습니당\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 251457.07it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Dense_search_retrieval_arguments()\n",
    "model = retrieval.Dense_embedding_retrieval(args)\n",
    "model.train()\n",
    "model.build_faiss()\n",
    "results2 = model.search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리 : 레이가 골트 이후에 사용한 가짜 이름은 무엇인가요?\n",
      "------TFIDF-------\n",
      "('1997년 킹 목사의 아들 덱스터 킹은 레이와 만나 그에게 \"기록을 위하여 나는 당신에게 묻고 싶은 데 당신이 나의 부친을 살인했나요?\"라고 의문하였다. 레이는 \"아니, 난 안 죽였다.\"고 답하였고, 덱스터는 레이에게 킹 가족과 더불어 그를 믿었다고 말했으며, 킹 가족은 또한 레이가 새로운 재판을 승인해야 한다고 몰아냈다.\\\\n\\\\n킹의 일생의 마지막 세월에 그의 친구 윌리엄 페퍼는 레이가 전혀 받지 않은 재판을 그에게 승인하는 시도에서 텔레비전으로 방영된 조롱 재판에서 레이를 대표하였다. 조롱 재판에서 검사는 힉먼 유잉이었다. 조롱 재판의 배심원은 레이를 무죄로 하였다.\\\\n\\\\n1999년 11월 페퍼는 로이드 조어스에 대항하는 부당한 사망 시민 재판에서 킹 가족을 대표하였다. 멤피스에서 식당 주인 조어스는 그해 12월 시민 재판소로 데려와져 마틴 루터 킹 주니어를 살해하는 데 음모의 일부로서 기소되었다. 그는 법적으로 책임있는 것으로 찾아지고, 킹 가족은 재정적 획득을 위하여 자신들이 사건을 추구하지 않았던 것을 보이는 데 선택된 총액 100 달러를 받아들였다. 12월 8일에 종결된 배심은 정부의 대리들을 포함한 다른이들은 물론 조어스가 음모의 일부였다는 것을 찾아냈다. 킹 가족은 킹 목사의 살해와 함께 레이가 아무 것도 관련되지 않은 것을 결론을 내렸다.\\\\n\\\\n코레타 스콧 킹 여사는 \"조어스에게 추가로 마피아, 지방, 주립과 연방 정부의 대리들의 음모가 나의 남편의 암살 사건에 깊게 연루된 재판이 있는 동안 나온 광대한 증거에 의하여 배심은 명확하게 확신되었습니다. 배심은 또한 암살자로서 제임스 얼 레이가 아닌 다른이었다는 것과 레이가 책임을 지는 데 위험한 처지에 빠진 것을 증명한 압도적인 증거를 단언하였습니다.\"라고 말하였다.\\\\n\\\\n음모의 어떤 주장을 받아들인 킹 가족에 의하여 자극을 받은 미국의 법무장관 재닛 리노는 1998년 8월 26일 새로운 조사를 명령하였다. 2000년 6월 9일 미국 법무부는 멤피스의 시민 재판소 배심의 사실 인정들을 포함하여 킹을 암살하는 데 음모가 있었다는 진술들을 거절한 150 페이지의 보고문을 발간하였다.', 1.5974026)\n",
      "('투시노에 자리를 잡은 가짜 드미트리는 그곳에 자신만의 정부를 세웠다. 그는 행정부와 귀족회의를 세웠으며, 자신을 지지하는 세력들로부터 세금을 걷기도 했다. 두 번째 가짜의 개인 병력을 모아 훈련시킨 것은 물론 투시노에 농업투자를 하는 등 국왕으로 즉위하지만 않았을 뿐 투시노에서의 두 번째 가짜 드미트리는 모든면에서 한 나라의 국왕과 완전히 똑같았다. 첫 번째 가짜와 마찬가지로 많은 지역들이 두 번째 가짜에 복속하거나, 혹은 투시노와 모스크바 양쪽과 관계를 맺었다.\\\\n\\\\n그러나 가짜 드미트리의 정부는 바실리 4세의 지역들 한가운데에 포위된 상태였으며, 이러한 사태를 극복하기 위해 가짜 드미트리는 모스크바 북동부의 성 삼위일체-성 세르기우스 수도원을 공격하였다. 그러나 1년이 넘은 공격에도 불구하고 이 곳을 함락시키지는 못하고, 모스크바와 투시노 양쪽 모두 교착상태에 빠진다.\\\\n\\\\n바실리 4세는 난국을 타개하기 위하여 스웨덴의 칼 9세에게 도움을 요청한다. 1609년 2월에 스웨덴이 참전하였으나, 바실리 4세의 조카인 미하일 스코핀-슈이스키의 대활약으로 모스크바는 북러시아의 지배권을 되찾는 데 성공한다. 이듬해에 가짜 드미트리는 추종자를 이끌고 모스크바 남동쪽의 칼루가로 피신한다.', 1.6333736)\n",
      "('가짜 드미트리가 칼루가로 피신한 이후, 이번에는 폴란드의 지그문트 3세가 자신의 아들 브와디스와프 4세 바사를 내세워 러시아에 개입하기 시작하였다. 투시노의 반모스크바적 인사들이 지기스문트 3세와 접촉하였으며, 1605년부터 계속된 폴란드와 루스 차르국 사이의 전쟁이 재개되었다. 이런 틈을 타서 가짜 드미트리는 모스크바 근교까지 진출하여 자리를 잡았다.\\\\n\\\\n드디어 1610년 7월, 바실리 4세는 성직자, 귀족, 일부 평민들이 참여한 회의를 통해 강제로 폐위되고, 이후 7명의 귀족회의를 통해 새로운 차르를 선출하기로 한다. 여러 인물들이 차르의 자리에 도전하였다. 가장 강력한 후보는 브와디스와프였으며, 이반 4세 처가의 후손인 미하일 1세 역시 입후보했다. 가짜 드미트리 역시 차르의 자리에 도전하게 된다. 회의 결과 브와디스와프가 새로운 차르로 선출되었으며, 가짜 드미트리는 다시 칼루가로 피신한다.\\\\n\\\\n그런데 브와디스와프는 러시아 정교회로의 개종 등의 이유를 들어 차르의 자리에 오르지 않았으며, 루스 차르국은 다시 혼란에 빠진다. 뒤이어 스웨덴의 칼 9세도 폴란드의 왕자를 차르로 선출한 데 반발하여 루스 차르국에 전쟁을 선포한다. 이러한 혼란상황을 틈타 가짜 드미트리는 다시 러시아 동부의 외곽 지역을 중심으로 다시 세력을 규합한다. 그러나 1610년 12월 11일, 그는 자신의 타타르인 부하에게 살해당한다.\\\\n\\\\n당시 폴란드의 장군이었던 스타니슬라브 졸키에프스키는 이 일을 다음과 같이 회상한다.\\\\n\\\\n저녁에 만취한 상태에서 그는 썰매를 하나 대령시켜 꿀술을 싣고 밖으로 나갔다. 탁 트인 밖으로 나온 후, 그는 보야르 몇 명과 술을 마셨다. 그때 피터 우루소프 공이 십여 명의 말 탄 이들과 함께 드미트리를 호위하고 있었다. 그 사기꾼은 보야르들과 즐겁게 술을 마신 뒤였는데, 그때 우루소프는 준비해 두었던 권총을 꺼내 썰매로 말을 달려 그를 쏘았다. 그러고는 그의 머리와 손을 칼로 베어 길에 버렸다.', 1.705833)\n",
      "\n",
      "------DenseEmbedding------\n",
      "('2007년 8월에 드라마 《대조영》을 촬영중이던 최수종은 때 아닌 학력 위조 의혹 논란이 뒤늦게 밝혀지면서 네티즌들을 분노하게 했다. 1981년 한국외대 무역학과를 합격하여 이후 졸업한 것으로 알려졌지만 학교 측에 확인 결과 사실이 아닌 것으로 밝혀졌다. 사태가 심각해지자 2007년 8월 22일을 기하여 소속사 ‘소프트랜드’는 해명한즉 “최수종은 서울 배재중학교와 서울 배명고등학교를 졸업한 뒤 한국외대 무역학과에 지원하여 합격을 하였지만, 집안 사정으로 인해 등록하지 못하였다”는 해명과 아울러, “최수종은 지금까지 학력 프리미엄을 얻어 본 적도 없으며, 외대를 졸업하였다고 직접 언급을 한 적도 없다”고 해명했다. 이어 소속사 측에서는 해명한즉 “최수종은 외대 등록을 하지 못하고 미국으로 건너가 콜로라도에 위치한 포트모건 컬리지 경영학과 비학위속성 수료 과정을 1년 정도 다닌 것이 전부이며, 그마저 부친상을 당하여 귀국하게 됐다”며 당시 상황을 해명하였고, “최수종의 학력을 제대로 확인하지 않고 각 포털 사이트에 일방적 오등록 기재된 것에 대응하지 않았던 것이 실수”라고 해명 언급하며 \"각 포털 사이트에 기재된 것은 오등록이자 실수”라고 주장하였다. \\\\n\\\\n2007년 12월 31일 《KBS 연기대상》 시상식 당시 최수종은 대상 수상 소감과 감사 인사를 전할 때 이 의혹 사건도 간접 언급하며 자신의 본의 아닌 사태이기는 하지만 어쨌건 소위 불미스런 사태로 인하여 실망시킨 점 다시 한 차례 송구스럽다는 사과 말씀을 올린다는 소감을 전하였다.', 73.891014)\n",
      "('1943년에 경상북도 칠곡군에서 출생하였다. 1970년에 사진가로 입문하여 초기에는 인간의 삶을 다룬 다큐멘터리 사진을 촬영했고, 1989년에 백두산에서 사진 촬영을 하면서 산 사진에 뛰어들어 6개월 동안 산 속에 살면서 작업을 해왔다. 그리고 산 사진 촬영을 통해 터득한 모습으로 높고 험준한 산에서 모습을 드러낸 바 없는 걸작 소나무를 찾아내어 사진에 담고 있었다.\\\\n\\\\n그러나 2011년~2013년 사이에 경상북도 울진군에 소재한 산림유전자원보호구역에서 사진을 촬영하던 도중, 사진 구도에 방해된다는 이유로 200년이 넘은 금강송과 그 외의 나무들을 무단으로 벌목한 것에 대해 논란이 되었다. 이 사건이 언론에 보도되면서 많은 사람들이 분노를 겪였으며, 그는 형사에게 기소되면서 500만원의 벌금형을 선고받았다. 그와 동시에 한국사진작가협회에서 영구제명을 당했으며, 대다수의 환경단체와 사진작가단체에서 사진전 개최에 반대의사는 물론 보이콧까지 일으켰다.\\\\n\\\\n이후 본래 예술의 전당에서 개최하려고 했던 그의 사진전을 미술과 비평에 취소되었다는 소식이 전해졌으나, 이를 상대로 전시회 금지 취소 요청을 하면서 가처분 신청을 냈고, 4월 6일에 서울중앙지법이 이를 받아들여 전시회를 열었다. 이 소식을 들은 환경 단체, 사진 작가 단체, SNS 이용자들이 또 다시 분노를 일으켰으며, 예술의 전당 디자인미술관 정문에서 현역 사진작가들이 릴레이 1인 시위를 했을 정도다.', 73.960625)\n",
      "('1957년 울산광역시에서 태어나 경남고등학교와 서울대학교 법과대학을 졸업하고 제22회 사법시험 합격해 사법연수원 12기를 수료했다. 1985년 부산지방법원 판사에 임용된 이후 부산지방법원 동부지원, 부산고등법원 판사를 거쳐 마산지방법원 거창지원장 직무대리와 대법원 재판연구관을 지내다가 부장판사로 승진하여 창원지방법원, 부산지방법원, 울산지방법원, 부산고등법원에서 재판장을 했다. 2006년 8월에 부산지방법원 동부지원장에 임명되었으며 2009년 2월에 부산고등법원 수석부장판사로 지내다가 2011년 2월에 임명된 창원지방법원장에 재직하면서 제37대 경상남도 선거관리위원회 위원장을 2년동안 겸직했다. 2015년 2월에 제21대 부산고등법원장에 임명되어 2년동안 재직하다가 퇴직하여 법무법인 해인 대표 변호사로 활동하고 있다.\\\\n\\\\n울산지방법원 제10민사부 재판장을 하던 윤인태 부장판사)는 2004년 4월 선고공판에서 \"천성산에 희귀동식물이 서식하고, 중-고층 습지가 있고, 상수원 보호 필요성이 있고, 환경영향평가의 하자가 있다하더라도 도롱뇽에게 사법상의 권리가 생긴다고 할 수 없다\"고 하면서 도롱뇽을 소송의 당사자로 인정하지 않았다. \\\\n\\\\n부산지법 형사3부 재판장으로 재직하던 2002년 7월 16일 아파트 사업주에게서 거액의 뇌물을 받고 다대지구를 택지로 전환하도록 부산시 등에 압력을 행사한 혐의로 구속 기소된 김운환씨(56·전 민주당 국회의원)에 대해 \"김씨가 아파트 사업주인 옛 동방주택 대표 이영복에게서 5억원의 뇌물을 받은 사실은 인정되지만 이를 뇌물로 보지 않는다\"며 무죄를 선고했다. \\\\n\\\\n울산지방법원 행정부에서 재판장으로 재직하던 2004년 1월 14일 울산경제정의실천시민연합이 울산시를 상대로 낸 ‘태화들 용도변경 회의록에 관한 정보공개 거부 취소소송’에서 “정보공개 거부 처분을 취소하라”는 원고승소 판결을 내렸다 \\\\n부산고등법원 제5민사부 재판장으로 재직하던 2004년 5월 18일에 거창 양민학살사건 희생자와 유족 등이 국가를 상대로 제기한 손해배상 청구소송 항소심 선고공판에서 \"거창사건은 1951년 2월 발생해 같은 해 12월 학살 책임자에 대한 판결이 선고된 만큼 판결 선고일로부터 3년, 사건 발생일로부터 5년인 손해배상 소멸시효가 지나 국가가 배상금을 지급할 의무가 없다\"고 하면서 \"국가는 위자료를 지급할 의무가 있다\"고 했던 원심을 파기하는 원고패소 판결을 하면서 :국가가 진상규명과 명예회복, 손해배상 등 보호조처를 소흘히 하는 등의 불법행위를 저질렀다\"는 유족 측의 주장에 대해 \"국가나 공무원이 마땅히 해야 할 일을 하지 않았다고 해서 이에 대해 손해배상을 청구하는 것은 불가능하다\"고 했다. \\\\n\\\\n부산고등법원 행정1부 재판장으로 재직하던 2010년 8월 27일 롯데쇼핑이 김해시장을 상대로 제기한 \\'취득세 등 부과처분취소\\' 소송의 항소심에서 \"롯데아울렛 김해점의 경우 유통사업을 영위하려는 자가 유통사업용 부동산을 취득한 경우에 해당하고, 판매 정보관리시스템을 구축하고 통합적으로 관리하는 쇼핑센터로 임대 부분도 유통사업 용도로 사용한 것으로 인정된다\"라며 \"일정 매출액을 지불하는 방식의 임대라 해서 유통사업 용도로 사용하지 않았다고 볼 이유가 없어 취득세 등의 부과 처분은 위법하다\"며 \"1심 판결을 취소하고, 취득세와 농어촌특별세 14억 원의 부과 처분을 취소한다\"고 했다. \\\\n\\\\n창원지방법원장으로 재직하던 때에 석궁 사건 재판 합의 과정을 공개한 이정렬 부장판사에 대해 대법원에 징계를 청구했다.', 74.050766)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(1, 600)\n",
    "d = load_from_disk(args.test_data_route)\n",
    "print('쿼리 :', d['validation']['question'][idx])\n",
    "\n",
    "print('------TFIDF-------')\n",
    "for i in results1[idx]:\n",
    "    print(i)\n",
    "print()\n",
    "print('------DenseEmbedding------')\n",
    "for i in results2[idx]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
