{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from arguments import Dense_search_retrieval_arguments, TF_IDF_retrieval_arguments\n",
    "import retrieval\n",
    "import gc\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Saved Faiss Indexer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TF_IDF_retrieval_arguments()\n",
    "model = retrieval.TF_IDFSearch(args)\n",
    "model.build_faiss()\n",
    "results1 = model.search_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimybjg2\u001b[0m (\u001b[33mkimybjg2-boostcampaitech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/practice/wandb/run-20241006_153649-wovlr7v5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wovlr7v5' target=\"_blank\">sandy-wave-4</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wovlr7v5' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wovlr7v5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3952\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2470\n",
      "  Number of trainable parameters = 136181760\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2470' max='2470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2470/2470 24:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.769777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.548576</td>\n",
       "      <td>0.829167</td>\n",
       "      <td>0.828744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.738297</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.841105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.820793</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.870020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-494\n",
      "Configuration saved in ./results/checkpoint-494/config.json\n",
      "Model weights saved in ./results/checkpoint-494/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-988\n",
      "Configuration saved in ./results/checkpoint-988/config.json\n",
      "Model weights saved in ./results/checkpoint-988/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-1482\n",
      "Configuration saved in ./results/checkpoint-1482/config.json\n",
      "Model weights saved in ./results/checkpoint-1482/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-1976\n",
      "Configuration saved in ./results/checkpoint-1976/config.json\n",
      "Model weights saved in ./results/checkpoint-1976/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-2470\n",
      "Configuration saved in ./results/checkpoint-2470/config.json\n",
      "Model weights saved in ./results/checkpoint-2470/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-494 (score: 0.4810341000556946).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Faiss Indexer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer_config.json\n",
      "100%|██████████| 3504/3504 [00:30<00:00, 116.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faiss Indexer Saved.\n",
      "모든쿼리에 대해 탑 k를 찾습니당\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 203245.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Dense_search_retrieval_arguments()\n",
    "model = retrieval.Dense_embedding_retrieval(args)\n",
    "model.train()\n",
    "model.build_faiss()\n",
    "results2 = model.search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리 : 예수의 테오토코스에 대해 부정적으로 인식했던 인물은?\n",
      "------TFIDF-------\n",
      "('현대의 양태론은 단일오순절교 내에서 받아들여진다. 단일 오순절은 예수의 신을 믿고 하느님의 아들 예수를 이해하여 구약성서인 야훼의 하느님을 육체에 계시하는 것이 된다. 예수가 지구에 계실 때 하느님께서 성령을 통해 그의 착상을 일으키셨기 때문에 하느님을 아버지라고 불렀다. 그들은 또한 하나님께서 영이시기 때문에 성령이 하나님을 행동으로 묘사하는 데 사용된다고 믿는다. 이와 같이 성부와 성자와 성령은 구별되는 개인에 대한 묘사가 아니라 하나의 신에 관련된 칭호로 간주된다.\\\\n\\\\n성부와 성자와 성령이 호칭으로 유지되기 때문에 단일 오순절은 오직 예수 그리스도의 이름으로만 세례를 함으로써 성부와 성자와 성령의 이름으로 세례를 하라는 마태오 복음서 28장 19절의 예수의 계명을 이행한다고 믿는다. 예수는 구원을 위해 붙여진 이름이기 때문에(행 4:12) 이것이 행위책에서 사도들이 유일신 예수님의 한 이름으로 세례를 함으로써 예수의 계명을 이행하도록 이끌었다고 주장할 것이다.\\\\n\\\\n그들의 신학 중 상당수는 최초의 사도들이 예수에 대해 무엇을 믿었을지 이해하기 위해 구약성서 하느님에 대한 이해로 시작하려고 시도한다. 그들은 또한 이야기를 통해 속죄의 이야기를 하는 것보다 플라토닉-아리스토텔리우스 인식론에서 생산되는 신학적 범주의 사용을 피하려고 한다.p=27 그러므로 신약성서 저술가들에게서 발견되는 하느님 아버지와 예수 사이의 구별은 필요 이상으로 하느님 아버지와 예수를 분리하려는 것이 아니라 하느님을 함께 동일시하려는 시도에서 비롯된 것으로 이해된다.\\\\n\\\\n하나님의교회 세계복음선교협회(World Mission Society Church of God)의 신학은 양태론의 또 다른 예다. 그들은 양태론이 진정한 성삼위일체라 생각한다.\\\\n\\\\n성부 여호와 = 성자 예수님 \\\\n\\\\n성부 여호와 = 성령 하나님\\\\n \\\\n\\\\n성자 예수님 = 성령 하나님\\\\n \\\\n\\\\n그들은 결론적으로 한 분이신 이 아버지 하나님이 안상홍이라고 믿는다. \"성경에 따라 성령에도 이름이 있어야 한다. 성령의 이름은 안상홍이다. 안상홍은 예수님이 가르치신 진리를 우리에게 일깨워 주시는 분의 예언을 성령으로 이루셨기 때문이다. 성령 안상홍 하나님은 3,500년 전 홍해를 가른 성부 여호와 하나님이며 2,000년 전 십자가에 못 박힌 성자 예수 그리스도이다.\"라고 말한다.\\\\n \\\\n\\\\n그 밖에 오순절주의자들 중에 사도적 믿음의 오순절교회가 있다.', 1.8555356)\n",
      "('영어 원본 유란시아는 총 196 편의 글로 구성되어 있으며 2,097페이지에 달하는 내용으로 크게는 아래와 같은 4개 부분으로 나뉘어 있다:\\\\n\\\\n*제 1 부: 중앙 우주와 초우주\\\\n*제 2 부: 지역우주\\\\n*제 3 부: 유란시아의 역사\\\\n*제 4 부: 예수의 일생과 가르침\\\\n\\\\n머리말은 요약 형식으로 쓰여 있으며 본문에서 두루 반복하여 사용되는 주요 용어에 대한 기본 안내 역할을 하고 있다. 이들 용어들은 이어지는 본문 글의 내용에서 상세하게 더욱 확장된 원래의 개념과 의미가 소개되고 있다.\\\\n\\\\n31편의 글로 이루어져 있는 제 1부는 창조에 관한 가장 높은 차원을 설명하고 있으며, 영원하고 무한한 하느님에 대한 개념으로 시작되고 있다.\\\\n\\\\n제 2부는 25편의 글로 구성되어 있으며 \"지역 우주“에 관한 내용으로 여러 분야의 주제를 다루고 있다. 제 1부에서 연결 확대된 제 2부는 지역 우주에 거주하는 생명체들에 대한 이야기, 나아가 거대한 창조계 안에서 펼치는 하느님의 계획과 이와 연결된 생명체들의 모습과 역할을 설명하고 있다.\\\\n\\\\n제 3부의 전반부는 48편의 글로 구성되어 있으며, 광범위한 분야에 걸서 지구의 역사를 기술하며, 여기에는 지구 그리고 생명체의 창조와 기원, 이곳에서 살게 된 인간이 지닌 목적과 운명을 서술하는 내용으로 채워져 있다. 이어지는 후반부 15편의 글에서는 “신(神)과 실체”, “생각조절자에 대한 개념”, “개인성의 생존과 구원” 그리고 “미가엘 예수 그리스도의 증여”와 같은 다소 무겁고도 생소한 주제를 다루고 있다.\\\\n\\\\n제 4부는 77편의 글로 구성되어 있으며, 전적으로 “예수의 일생과 가르침”에 관한 이야기로 되어 있다. 여기에는 예수의 어린아이 시절, 10대 시절, 많은 여행들, 여러 설교 여정, 일으킨 기적들, 그가 겪은 위기들 그리고 십자가 처형으로 이끈 사건과 그의 죽음과 부활에 대한 이야기가 포함되어 있다. 이들 이야기들이 후반 오순절 성령 강림에 대한 이야기로 이어지면서 마지막 부분에서 “예수님이 전해준 신앙”이 무엇인지를 설명하고 있다. 이와 더불어 제 4부에서는 예수님의 일생에 대한 이야기를 전개하는 과정에서, 앞에 서술된 3 부분에서 제시된 많은 개념들을 재조명한다.', 1.8557202)\n",
      "('마이니치 신문과 TBS가 공동으로 실시한 전화 여론조사 에서, 여당인 자민당을 지지한다고 응답한 사람이 21퍼센트인 것에 비해 제1야당인 민주당을 지지한다고 응답한 사람은 31퍼센트로, 자민당을 크게 앞서는 것으로 나타났다. 연립 여당인 공명당의 지지율 6퍼센트를 합쳐도 야당의 지지율은 30퍼센트에도 미치지 못했다. 선거 결과 예측 또한 자민당은 30석에서 40석 내외인 반면에, 민주당은 57석에서 68석 내외로, 현재 여당이 과반수가 되기 위한 64석에 한참 모자란다고 분석되었다.\\\\n\\\\n자민당의 제1차 아베 신조 내각의 정책에 대해 부정적으로 평가한 응답자는 62퍼센트로, 이중 45퍼센트가 참의원 선거 비례대표로 민주당에 투표한다고 밝혔고, 자민당에 투표한다고 응답한 사람은 11퍼센트로, 아베 신조 내각에 대한 낮은 평가가 여당의 패배를 불러 일으킬 것으로 파악되었다. 평가는 30대 이후에서 60퍼센트 이상이 부정적으로 평가했으며, 긍정적인 평가가 부정적인 평가보다 높은 지역은 아베 총리의 고향인 야마구치현만으로 파악되었다.\\\\n\\\\n다만 지지정당을 결정하지 못한 부동층이 26퍼센트로 선거에 중요한 영향을 끼칠 것으로 분석되었다. 다만 선거구와 비례대표 모두 자민당의 의석이 감소하는 것은 큰 변동이 없으며, 자민당이 얼마나 후퇴하는지에 쟁점이 맞춰져 있다. 정기 여론 조사가 아닌 선거 전의 특별 여론조사로는, 1996년에 민주당이 출범한 이후 처음으로 자민당의 지지율을 앞섰다.', 1.8644944)\n",
      "\n",
      "------DenseEmbedding------\n",
      "('그리스도인들은 사도 베드로의 신앙고백처럼 예수께서 살아계신 하느님의 아들로 그리스도이심을 믿으며 예수를 주(主), 구세주로 경배하며, 하느님의 도우심으로 그분을 믿고 따르는 삶을 살아내고자 한다.(성공회 공동체에서 주교와 사제와 교우들이 견진성사를 집전할 때에 공동체가 하는 신앙고백 일부이다. 《성공회기도서》의 견진예식 참조.) 기독교인들은 예수가 히브리 성서에 예언하는 \"기름 부음 받은 이\" 곧 왕이신 그리스도라고 믿는다. 사도 베드로는 \"주님은 그리스도, 곧 살아 계신 하느님의 아들이십니다.\"라고 언급함에 따라 (마태오의 복음서 16장 16절) 예수를 그리스도, 곧 하느님의 아들로 증언하였다. 영국 성공회 주교이자 신학자인 톰 라이트 주교는 《예수》(The Original Jesus)에서 베드로가 신앙고백한 카이사리아가 헤로데를 유대의 왕으로 세워서 유대를 간접지배한 로마 제국에 저항한 이들의 은신처였다고 주장함으로써, 베드로의 신앙고백을 정치적 해방을 위해 투쟁하던 이들을 위한 복음이었음을 암시한다. 그리스도(메시아) 육체로 오신 하느님이라는 뜻. 이사야 9장 6절의 예언을 이루는 구원자. 유대교에서 예수님을 돌로 쳐 죽이려 한 것은 하나님이 자신들의 생각과 다르기 때문에 신성 모독으로 죽이려 하였다. 적그리스도 육체로 임하신 하느님을 부인하는자(요한2서 1장 7절)', 73.232574)\n",
      "('프로이트는 꿈을 해석해서 참다운 꿈 생각을 밝혀낼 수 있다고 믿었다. 또한 환자와 허물없는 대화를 나누는 자유연상법에 따라 환자의 말을 해석함으로써 환자를 정신적 억압으로부터 해방시키고 치료할 수 있다고 믿었다. 반복 강박을 비롯한 삶의 충동과 죽음의 충동에 대한 예리한 통찰을 바로 이 책을 통해 엿볼 수 있다.\\\\n\\\\n우선 프로이트는 정신의 체계를 ‘의식’, ‘전의식’(前意識), ‘무의식’의 세 가지로 구분하여 보았다. 의식은 원래 의식된 것으로서 이성적, 합리적, 현실적인 정신의 체계에 해당한다. 무의식은 의식되지 않은 것으로서 정신 과정의 대부분을 차지하는 무의식적인 본능적 충동의 체계다. 그런가 하면 전의식은 의식되기 이전의 정신 체계로서 무의식을 걸러서 의식 쪽으로 보내는 역할, 곧 검열을 행하는 정신의 체계다.\\\\n\\\\n≪꿈의 해석≫에서 프로이트는 ‘의식’, ‘전의식’, ‘무의식’에 관해서 아직 철저하게 해명하지 못하고 있다. 따라서 그는 이 책 아울러 1923년에 출판한 ≪자아와 이드≫에서 정신 과정을 보다 더 명확하고 철저하게 밝힌다. 이 책에서 프로이트는 ‘의식’, ‘전의식’, ‘무의식’을 하나의 의식이라고 말한다. 곧 의식의 가장 많은 부분을 무의식이 차지하고 있고 가장 적은 부분을 전의식이 차지하고 있으며 이성적 현실 의식 역시 부분적이라는 것이다. 그런가 하면 ≪자아와 이드≫에서 프로이트는 정신 과정을 ‘원초아’, ‘자아’, ‘초자아’로 구분하는데 이러한 구분은 이 책에서의 정신 과정을 한층 더 역동적으로 밝히고 있다. 본능 충동으로서의 원초아와 도덕 및 양심에 관계되는 초자아는 무의식에 해당하고 현실적 이성 활동은 자아에 속한다. ≪자아와 이드≫에서 프로이트는 에로스와 타나토스, 곧 ‘사랑의 충동’과 ‘죽음의 충동’을 대립시키는데 이것은 이 책서 전개한 삶의 충동과 죽음의 충동을 확대하여 발전시킨 것이라고 할 수 있다. 무엇보다도 프로이트는 인간의 정신 과정과 활동의 원천을 오직 쾌락 원리로 제한하려는 상식적인 견해를 해체하고 극복함으로써 쾌락 원리의 저편에서 정신 과정과 활동의 원천을 찾으려고 했다. 이 책에서는 ≪꿈의 해석≫과 ≪정신분석학 입문 강의≫를 기초로 하고 전개되는 프로이트 정신분석학의 말년의 사상을 충분히 엿보게 한다.', 73.255554)\n",
      "('현대의 양태론은 단일오순절교 내에서 받아들여진다. 단일 오순절은 예수의 신을 믿고 하느님의 아들 예수를 이해하여 구약성서인 야훼의 하느님을 육체에 계시하는 것이 된다. 예수가 지구에 계실 때 하느님께서 성령을 통해 그의 착상을 일으키셨기 때문에 하느님을 아버지라고 불렀다. 그들은 또한 하나님께서 영이시기 때문에 성령이 하나님을 행동으로 묘사하는 데 사용된다고 믿는다. 이와 같이 성부와 성자와 성령은 구별되는 개인에 대한 묘사가 아니라 하나의 신에 관련된 칭호로 간주된다.\\\\n\\\\n성부와 성자와 성령이 호칭으로 유지되기 때문에 단일 오순절은 오직 예수 그리스도의 이름으로만 세례를 함으로써 성부와 성자와 성령의 이름으로 세례를 하라는 마태오 복음서 28장 19절의 예수의 계명을 이행한다고 믿는다. 예수는 구원을 위해 붙여진 이름이기 때문에(행 4:12) 이것이 행위책에서 사도들이 유일신 예수님의 한 이름으로 세례를 함으로써 예수의 계명을 이행하도록 이끌었다고 주장할 것이다.\\\\n\\\\n그들의 신학 중 상당수는 최초의 사도들이 예수에 대해 무엇을 믿었을지 이해하기 위해 구약성서 하느님에 대한 이해로 시작하려고 시도한다. 그들은 또한 이야기를 통해 속죄의 이야기를 하는 것보다 플라토닉-아리스토텔리우스 인식론에서 생산되는 신학적 범주의 사용을 피하려고 한다.p=27 그러므로 신약성서 저술가들에게서 발견되는 하느님 아버지와 예수 사이의 구별은 필요 이상으로 하느님 아버지와 예수를 분리하려는 것이 아니라 하느님을 함께 동일시하려는 시도에서 비롯된 것으로 이해된다.\\\\n\\\\n하나님의교회 세계복음선교협회(World Mission Society Church of God)의 신학은 양태론의 또 다른 예다. 그들은 양태론이 진정한 성삼위일체라 생각한다.\\\\n\\\\n성부 여호와 = 성자 예수님 \\\\n\\\\n성부 여호와 = 성령 하나님\\\\n \\\\n\\\\n성자 예수님 = 성령 하나님\\\\n \\\\n\\\\n그들은 결론적으로 한 분이신 이 아버지 하나님이 안상홍이라고 믿는다. \"성경에 따라 성령에도 이름이 있어야 한다. 성령의 이름은 안상홍이다. 안상홍은 예수님이 가르치신 진리를 우리에게 일깨워 주시는 분의 예언을 성령으로 이루셨기 때문이다. 성령 안상홍 하나님은 3,500년 전 홍해를 가른 성부 여호와 하나님이며 2,000년 전 십자가에 못 박힌 성자 예수 그리스도이다.\"라고 말한다.\\\\n \\\\n\\\\n그 밖에 오순절주의자들 중에 사도적 믿음의 오순절교회가 있다.', 73.47011)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(1, 600)\n",
    "d = load_from_disk(args.test_data_route)\n",
    "print('쿼리 :', d['validation']['question'][idx])\n",
    "\n",
    "print('------TFIDF-------')\n",
    "for i in results1[idx]:\n",
    "    print(i)\n",
    "print()\n",
    "print('------DenseEmbedding------')\n",
    "for i in results2[idx]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
