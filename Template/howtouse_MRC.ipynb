{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import retrieval\n",
    "import All_dataset\n",
    "from arguments import Extraction_based_MRC_arguments\n",
    "from MRC import Extraction_based_MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 3952/3952 [00:13<00:00, 303.42 examples/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 319.91 examples/s]\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf를 로드했습니다.\n",
      "Faiss Indexer을 로드했습니다.\n",
      "쿼리당 3개의 문서를 faiss indexer을 통해 찾습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 600/600 [00:00<00:00, 33204.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = retrieval.TF_IDFSearch()\n",
    "tfidf.get_sparse_embedding()\n",
    "tfidf.build_faiss()\n",
    "retrieval_results = tfidf.search_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'베피콜롬보\\n베피콜롬보는 수성 탐사 계획 중 하나로 ESA와 JAXA가 공동으로 계획했다. 소형 탐사선 2기를 보유하고 있으며, 유럽(MPO)과 일본MMO)에서 각각 한 기씩 제공했으며, 또한 한 기는 사진을 찍고, 다른 한 기는 자기장을 연구하는 등 역할이 확실히 구별되어 있다. \\n\\n#태양 성운, 행성계에 있어서, 수성에 대해 연구해야 할 것은 무엇인가?\\n#왜 수성의 밀도는 다른 지구형 행성보다 높은가?\\n#수성의 핵은 액체인가? 고체인가?\\n#오늘날도 수성 구조는 활동적인가?\\n#금성과 화성, 달도 가지고 있지 못 한 작은 행성이 왜 자기장을 가지고 있는가?\\n#수성의 주 성분이 철임에도, 분광 관측으로는 발견되지 않았던 이유는 무엇인가?\\n#극점의 영구 동토에는 황 혹은 얼음이 존재하는가?\\n#외기권의 형성 원리는 무엇인가?\\n#이온층이 없는데도, 자기장과 태양풍이 어떻게 상호 작용을 하는가?\\n#수성의 자화(磁化)된 환경이 지구에서 관측되는 오로라, 밴 앨랜대, 자기 폭풍 등이 존재한다는 것을 암시하는가?\\n#공간의 왜곡으로 인한 수성의 근일점 변화가 일반상대성이론에 근거한 결과의 오차값을 더 줄일 수 있는가\\n\\n매리너 10호나 메신저와 같이, 베피콜롬보는 금성과 지구에서 플라이바이를 사용할 예정이다. 특히, 태양 에너지 추진을 이용하여 달, 금성을 지나 수성에 느린 속도로 도달 할 전망이다. 이런 기술은 태양 중력의 영향을 최소화하여 수성에 접근하기 위해서는 필수적이다\\n\\n베피콜롬보는 2018년 10월 경에 발사 되어, 2025년 12월 5일, 수성 궤도로 진입 할 예정이다. 그 후, 2년동안 수성에 대한 정보를 모으고 연구를 행할 것이다. 페가수스자리 51 b가 발견되기 이전 폴란드 천문학자 알렉산데르 볼시찬이 이미 펄서 행성 PSR 1257을 발견했으나, 이 행성은 태양처럼 평범한 주계열성 주위를 도는 행성으로서 최초로 발견되었다는 점에서 의미가 있다.\\n\\n1995년 이 행성의 존재가 밝혀진 이후, 천문학자들은 이 행성을 자세히 연구했고 몇 가지 특징을 더 밝혀 냈다. 이 행성은 어머니 항성에 매우 바싹 붙어서 돌고 있다. 그 때문에 행성의 표면 온도는 섭씨 1000도 이상으로 달구어져 있다. 또한 가까운 거리 때문에 4일에 한 번 공전한다. 공전 시간에서 계산한 공전 속도는 1초에 136km로, 지구의 30km와 비교하면 4배 이상 빠른 속도이다. 이 행성의 질량은 목성의 절반 정도이다. 질량이 매우 큰 것으로 미루어 보아 이 행성은 뜨거운 가스 행성일 것으로 추측된다.\\n\\n처음 이 행성이 발견되었을 때, 기존의 행성탄생 이론으로는 이토록 어머니 항성에 가까이 붙어서 돌고 있는 현상을 설명할 수 없었다. 기존 이론에서는 지구형 행성이 가까이서 생겨나고, 목성과 같이 질량이 큰 가스 행성은 멀찍이 떨어져서 생겨난다고 설명했기 때문이다. 그런데 51b 이후로 발견된 많은 외계 행성들은 \\'뜨거운 가스 행성\\'이었다. 불타는 목성이 흔한 것임이 밝혀지자, 새로운 학설(행성이주 이론)이 정립될 필요가 있었다.\\n\\n이 행성을 지구형 행성으로 보는 시각도 있었으나, 현재는 가스 행성으로 인정하고 있다. 질량은 지구의 150배 정도로 이는 목성 질량의 절반이 채 되지 않는다. 다만 이 정도 질량이면 대기를 붙잡는 힘이 강하여 어머니 별의 항성풍에 질량을 잃지 않는다. 반면 51b의 반지름은 목성의 1.5배 정도일 것으로 추측된다. 이 행성은 항성으로부터의 열로 달아올라 있기 때문에 외곽 대기가 부풀어서 더 높은 고도까지 확장되어 있는 상태이다. 밤의 반구 쪽을 바라보아도 내부에서 나오는 열 때문에 붉게 빛날 것으로 생각된다.\\n\\n이 행성은 어머니 항성의 기조력으로 인해, 한 쪽 면이 항성만을 바라보도록 고정되었을 것이다. 뜨거운 목성|슈퍼 목성|갈색왜성\\n\\n현재 외계 행성을 탐지하는 데 쓰이는 기술이 제한되어 있는 이유로 현재까지 발견된 대부분의 외계 행성은 태양계의 거대 행성과 비슷한 크기의 행성이다. 이렇게 거대한 행성들은 다른 거대 행성보다 목성과 더욱 많은 공통점을 공유하는 것으로 추정되기 때문에, 일부는 이들에 대한 용어로 \"목성형 행성\"이 더욱 적절하다고 주장하여 왔다. 외계 행성의 대부분은 모성과 훨씬 더 가까우며, 따라서 태양계의 거대 행성보다 훨씬 더 뜨겁기 때문에 이들 행성 중 일부가 태양계에서 확인되지 않은 유형일 수도 있다. 우주에서 상대적인 원소 함량(약 98%가 수소와 헬륨)을 고려하면, 목성보다 더 무거운 암석형 행성을 발견하는 것은 놀라운 일일 것이다. 반면에, 행성계의 형성에 관한 모형들은 많은 외계 거대 행성의 공전을 관측해 옴으로써 이들이 모성에 가까울수록 거대 행성의 형성이 저지됨을 보여주어 왔다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(retrieval_results['validation']['context'][0])) # question, [문서1] [문서2] [문서3] 의 형태로 저장된 test dataset입니다.\n",
    "retrieval_results['validation']['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Extraction_based_MRC()\n",
    "model.train()\n",
    "model.inference(retrieval_results)\n",
    "# train이 끝나면 inference에 retrieval_results를 넣어 predict_result에 json 파일을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
