{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cqbmw6nj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sun-711</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/cqbmw6nj' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/cqbmw6nj</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_174413-cqbmw6nj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cqbmw6nj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/test_mrc/wandb/run-20241021_174743-fs3fj0ww</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/fs3fj0ww' target=\"_blank\">eager-dawn-712</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/fs3fj0ww' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/fs3fj0ww</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 3952\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1482\n",
      "  Number of trainable parameters = 68090880\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1482' max='1482' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1482/1482 03:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.137787</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.127290</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.112149</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./Dense_embedding_retrieval_model_results/checkpoint-494\n",
      "Configuration saved in ./Dense_embedding_retrieval_model_results/checkpoint-494/config.json\n",
      "Model weights saved in ./Dense_embedding_retrieval_model_results/checkpoint-494/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./Dense_embedding_retrieval_model_results/checkpoint-988\n",
      "Configuration saved in ./Dense_embedding_retrieval_model_results/checkpoint-988/config.json\n",
      "Model weights saved in ./Dense_embedding_retrieval_model_results/checkpoint-988/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 240\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./Dense_embedding_retrieval_model_results/checkpoint-1482\n",
      "Configuration saved in ./Dense_embedding_retrieval_model_results/checkpoint-1482/config.json\n",
      "Model weights saved in ./Dense_embedding_retrieval_model_results/checkpoint-1482/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Dense_embedding_retrieval_model_results/checkpoint-1482 (score: 0.11214937269687653).\n"
     ]
    }
   ],
   "source": [
    "from retrieval import Dense_embedding_retrieval, BM25Search\n",
    "from MRC import Extraction_based_MRC\n",
    "\n",
    "\n",
    "dense_retriever = Dense_embedding_retrieval()\n",
    "dense_retriever.get_trainer()\n",
    "dense_retriever.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data route: /data/ephemeral/data/train_dataset/\n",
      "Test data route: /data/ephemeral/data/test_dataset\n",
      "Wiki route: /data/ephemeral/data/wikipedia_documents.json\n",
      "Data path: ./bm25_retrieval_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-small/snapshots/5fe1f0cb3946f0ea1c01e657cd1688771cf47802/tokenizer_config.json\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "위키 컨텍스트의 수: 56737\n",
      "BM25 임베딩을 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 retrieval: 100%|██████████| 600/600 [00:00<00:00, 13028.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build or load BM25 index\n",
    "bm25 = BM25Search()\n",
    "bm25.get_sparse_embedding()\n",
    "\n",
    "bm25_result = bm25.search_query_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gwlc3own) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-feather-713</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/gwlc3own' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/gwlc3own</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241022_010206-gwlc3own/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gwlc3own). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/test_mrc/wandb/run-20241022_013451-sz9fyuc5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/sz9fyuc5' target=\"_blank\">rural-tree-717</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/sz9fyuc5' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/sz9fyuc5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data route: /data/ephemeral/data/train_dataset/\n",
      "Test data route: /data/ephemeral/data/test_dataset\n",
      "Wiki route: /data/ephemeral/data/wikipedia_documents.json\n",
      "Data path: ./bm25_retrieval_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "위키 컨텍스트의 수: 56737\n",
      "제일 마지막 checkpoint: /data/ephemeral/test_mrc/Dense_embedding_retrieval_model_results/checkpoint-1482/trainer_state.json\n",
      "best checkpoint: ./Dense_embedding_retrieval_model_results/checkpoint-1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Dense_embedding_retrieval_model_results/checkpoint-1482 were not used when initializing RobertaModel: ['q_model.bert.encoder.layer.4.intermediate.dense.bias', 'q_model.bert.embeddings.position_embeddings.weight', 'p_model.bert.encoder.layer.5.attention.self.query.bias', 'q_model.bert.encoder.layer.5.attention.self.value.weight', 'p_model.bert.encoder.layer.3.attention.output.dense.bias', 'p_model.bert.encoder.layer.1.output.dense.bias', 'q_model.bert.encoder.layer.5.attention.output.dense.weight', 'p_model.bert.encoder.layer.2.attention.output.dense.weight', 'p_model.bert.encoder.layer.2.intermediate.dense.weight', 'q_model.bert.encoder.layer.4.attention.output.dense.weight', 'q_model.bert.encoder.layer.2.attention.self.value.bias', 'p_model.bert.encoder.layer.4.intermediate.dense.bias', 'q_model.bert.encoder.layer.5.attention.self.query.bias', 'p_model.bert.encoder.layer.5.attention.self.value.bias', 'q_model.bert.encoder.layer.5.intermediate.dense.weight', 'p_model.bert.encoder.layer.1.attention.self.value.weight', 'p_model.bert.encoder.layer.5.attention.output.dense.bias', 'p_model.bert.encoder.layer.1.output.LayerNorm.weight', 'q_model.bert.encoder.layer.3.intermediate.dense.weight', 'q_model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.2.attention.self.query.bias', 'p_model.bert.encoder.layer.2.attention.self.query.weight', 'q_model.bert.encoder.layer.2.output.dense.weight', 'p_model.bert.encoder.layer.1.attention.self.query.weight', 'p_model.bert.encoder.layer.3.attention.self.value.bias', 'p_model.bert.encoder.layer.4.attention.self.query.bias', 'q_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.1.attention.self.key.bias', 'q_model.bert.encoder.layer.2.output.LayerNorm.bias', 'p_model.bert.embeddings.token_type_embeddings.weight', 'p_model.bert.encoder.layer.1.attention.output.dense.bias', 'p_model.bert.encoder.layer.2.intermediate.dense.bias', 'q_model.bert.encoder.layer.1.attention.output.dense.weight', 'p_model.bert.encoder.layer.0.attention.self.value.weight', 'p_model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.2.output.LayerNorm.bias', 'q_model.bert.encoder.layer.4.attention.self.key.weight', 'q_model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.0.output.dense.bias', 'q_model.bert.encoder.layer.0.output.LayerNorm.bias', 'q_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.5.attention.output.dense.bias', 'p_model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.5.attention.self.value.bias', 'q_model.bert.encoder.layer.3.attention.self.value.weight', 'p_model.bert.encoder.layer.3.intermediate.dense.bias', 'q_model.bert.pooler.dense.bias', 'p_model.bert.encoder.layer.0.attention.self.key.bias', 'q_model.bert.embeddings.word_embeddings.weight', 'p_model.bert.encoder.layer.4.output.dense.bias', 'q_model.bert.encoder.layer.4.output.dense.bias', 'p_model.bert.encoder.layer.2.attention.output.dense.bias', 'p_model.bert.encoder.layer.3.attention.self.key.bias', 'q_model.bert.encoder.layer.3.attention.self.query.weight', 'p_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.1.output.dense.weight', 'q_model.bert.encoder.layer.3.intermediate.dense.bias', 'q_model.bert.encoder.layer.1.output.LayerNorm.weight', 'q_model.bert.encoder.layer.3.output.LayerNorm.weight', 'q_model.bert.encoder.layer.0.attention.self.value.weight', 'p_model.bert.encoder.layer.5.output.LayerNorm.bias', 'q_model.bert.encoder.layer.3.attention.self.key.weight', 'q_model.bert.encoder.layer.4.output.LayerNorm.bias', 'q_model.bert.encoder.layer.1.attention.self.value.bias', 'q_model.bert.encoder.layer.2.output.LayerNorm.weight', 'q_model.bert.encoder.layer.1.attention.self.query.bias', 'q_model.bert.encoder.layer.2.intermediate.dense.bias', 'p_model.bert.encoder.layer.4.attention.self.query.weight', 'p_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.3.attention.self.value.bias', 'p_model.bert.encoder.layer.3.output.LayerNorm.weight', 'p_model.bert.encoder.layer.5.output.dense.bias', 'q_model.bert.encoder.layer.0.attention.self.query.bias', 'q_model.bert.encoder.layer.4.attention.self.value.weight', 'q_model.bert.encoder.layer.2.intermediate.dense.weight', 'q_model.bert.encoder.layer.0.attention.output.dense.weight', 'p_model.bert.encoder.layer.3.attention.self.query.bias', 'p_model.bert.encoder.layer.3.attention.self.value.weight', 'q_model.bert.encoder.layer.2.attention.self.value.weight', 'q_model.bert.encoder.layer.3.output.dense.bias', 'q_model.bert.encoder.layer.5.attention.self.key.weight', 'q_model.bert.encoder.layer.3.attention.output.dense.bias', 'q_model.bert.encoder.layer.1.attention.output.dense.bias', 'q_model.bert.encoder.layer.1.output.dense.weight', 'p_model.bert.encoder.layer.0.intermediate.dense.bias', 'q_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.5.output.LayerNorm.bias', 'q_model.bert.encoder.layer.0.attention.self.query.weight', 'q_model.bert.encoder.layer.4.attention.self.value.bias', 'p_model.bert.encoder.layer.2.output.LayerNorm.weight', 'q_model.bert.encoder.layer.0.intermediate.dense.bias', 'q_model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.1.output.dense.bias', 'p_model.bert.encoder.layer.5.output.LayerNorm.weight', 'q_model.bert.encoder.layer.4.intermediate.dense.weight', 'q_model.bert.encoder.layer.4.attention.self.query.weight', 'p_model.bert.encoder.layer.3.output.LayerNorm.bias', 'p_model.bert.encoder.layer.4.output.LayerNorm.bias', 'q_model.bert.embeddings.position_ids', 'q_model.bert.encoder.layer.4.output.LayerNorm.weight', 'p_model.bert.encoder.layer.5.attention.self.key.weight', 'p_model.bert.encoder.layer.4.intermediate.dense.weight', 'p_model.bert.encoder.layer.0.intermediate.dense.weight', 'q_model.bert.encoder.layer.3.output.dense.weight', 'q_model.bert.encoder.layer.1.intermediate.dense.bias', 'q_model.bert.encoder.layer.2.attention.output.dense.weight', 'p_model.bert.embeddings.LayerNorm.bias', 'p_model.bert.embeddings.word_embeddings.weight', 'q_model.bert.encoder.layer.5.attention.self.query.weight', 'q_model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.4.output.dense.weight', 'q_model.bert.pooler.dense.weight', 'p_model.bert.encoder.layer.2.attention.self.value.weight', 'p_model.bert.encoder.layer.5.intermediate.dense.weight', 'p_model.bert.encoder.layer.4.output.dense.weight', 'q_model.bert.embeddings.LayerNorm.weight', 'p_model.bert.embeddings.LayerNorm.weight', 'p_model.bert.encoder.layer.3.attention.output.dense.weight', 'q_model.bert.encoder.layer.2.output.dense.bias', 'q_model.bert.encoder.layer.5.attention.self.key.bias', 'q_model.bert.encoder.layer.0.output.LayerNorm.weight', 'p_model.bert.encoder.layer.5.attention.self.value.weight', 'p_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'p_model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.0.output.LayerNorm.bias', 'p_model.bert.encoder.layer.0.attention.output.dense.bias', 'p_model.bert.encoder.layer.3.attention.self.query.weight', 'p_model.bert.encoder.layer.5.output.dense.weight', 'q_model.bert.encoder.layer.0.output.dense.bias', 'p_model.bert.encoder.layer.1.attention.self.key.weight', 'q_model.bert.encoder.layer.0.attention.output.dense.bias', 'q_model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'p_model.bert.encoder.layer.4.attention.self.key.weight', 'p_model.bert.encoder.layer.1.output.LayerNorm.bias', 'p_model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.3.attention.self.query.bias', 'p_model.bert.encoder.layer.2.attention.self.key.weight', 'p_model.bert.encoder.layer.4.attention.self.value.weight', 'p_model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.2.attention.self.key.bias', 'q_model.bert.encoder.layer.5.intermediate.dense.bias', 'q_model.bert.encoder.layer.5.output.dense.bias', 'p_model.bert.encoder.layer.3.attention.self.key.weight', 'p_model.bert.encoder.layer.2.output.dense.bias', 'p_model.bert.encoder.layer.2.attention.self.key.bias', 'q_model.bert.encoder.layer.0.output.dense.weight', 'p_model.bert.encoder.layer.0.attention.self.value.bias', 'p_model.bert.encoder.layer.4.output.LayerNorm.weight', 'p_model.bert.encoder.layer.1.intermediate.dense.bias', 'p_model.bert.pooler.dense.weight', 'p_model.bert.encoder.layer.2.output.dense.weight', 'p_model.bert.encoder.layer.0.output.LayerNorm.weight', 'p_model.bert.encoder.layer.1.intermediate.dense.weight', 'q_model.bert.encoder.layer.0.attention.self.key.weight', 'p_model.bert.encoder.layer.1.attention.self.query.bias', 'p_model.bert.encoder.layer.5.intermediate.dense.bias', 'q_model.bert.encoder.layer.0.attention.self.key.bias', 'q_model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.5.output.LayerNorm.weight', 'p_model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.4.attention.self.key.bias', 'q_model.bert.embeddings.token_type_embeddings.weight', 'q_model.bert.encoder.layer.2.attention.self.key.weight', 'p_model.bert.encoder.layer.2.attention.self.query.bias', 'p_model.bert.encoder.layer.0.attention.self.query.weight', 'q_model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'p_model.bert.encoder.layer.5.attention.output.dense.weight', 'p_model.bert.encoder.layer.0.attention.output.dense.weight', 'p_model.bert.encoder.layer.0.attention.self.query.bias', 'p_model.bert.encoder.layer.3.output.dense.bias', 'p_model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'q_model.bert.encoder.layer.1.attention.self.query.weight', 'p_model.bert.encoder.layer.3.output.dense.weight', 'p_model.bert.encoder.layer.0.output.dense.weight', 'q_model.bert.encoder.layer.1.output.LayerNorm.bias', 'q_model.bert.encoder.layer.4.attention.output.dense.bias', 'q_model.bert.embeddings.LayerNorm.bias', 'q_model.bert.encoder.layer.3.attention.self.key.bias', 'p_model.bert.encoder.layer.5.attention.self.key.bias', 'p_model.bert.encoder.layer.1.attention.output.dense.weight', 'q_model.bert.encoder.layer.1.intermediate.dense.weight', 'q_model.bert.encoder.layer.4.attention.self.query.bias', 'p_model.bert.encoder.layer.2.attention.self.value.bias', 'q_model.bert.encoder.layer.2.attention.self.query.weight', 'p_model.bert.encoder.layer.4.attention.self.key.bias', 'p_model.bert.encoder.layer.5.attention.self.query.weight', 'q_model.bert.encoder.layer.2.attention.output.dense.bias', 'p_model.bert.encoder.layer.4.attention.output.dense.bias', 'p_model.bert.encoder.layer.4.attention.output.dense.weight', 'q_model.bert.encoder.layer.1.attention.self.key.bias', 'p_model.bert.encoder.layer.1.attention.self.value.bias', 'q_model.bert.encoder.layer.0.attention.self.value.bias', 'p_model.bert.embeddings.position_embeddings.weight', 'q_model.bert.encoder.layer.1.attention.self.key.weight', 'q_model.bert.encoder.layer.5.output.dense.weight', 'p_model.bert.embeddings.position_ids', 'p_model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'p_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'p_model.bert.encoder.layer.3.intermediate.dense.weight', 'q_model.bert.encoder.layer.1.attention.self.value.weight', 'q_model.bert.encoder.layer.3.attention.output.dense.weight', 'p_model.bert.encoder.layer.4.attention.self.value.bias', 'p_model.bert.pooler.dense.bias', 'q_model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'q_model.bert.encoder.layer.3.output.LayerNorm.bias', 'q_model.bert.encoder.layer.0.intermediate.dense.weight', 'p_model.bert.encoder.layer.0.attention.self.key.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./Dense_embedding_retrieval_model_results/checkpoint-1482 and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestmodel 체크포인트로 모델과 Trainer가 로드되었습니다.\n",
      "BM25로 100개를 가져온 뒤 Dense retrieval로 20개의 문서를 찾습니다.\n",
      "Data route: /data/ephemeral/data/train_dataset/\n",
      "Test data route: /data/ephemeral/data/test_dataset\n",
      "Wiki route: /data/ephemeral/data/wikipedia_documents.json\n",
      "Data path: ./bm25_retrieval_result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "위키 컨텍스트의 수: 56737\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './Dense_embedding_retrieval_model_results/checkpoint-1482'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './Dense_embedding_retrieval_model_results/checkpoint-1482' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dense_retriever\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Perform re-ranking using the dense retriever\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m reranked_result \u001b[38;5;241m=\u001b[39m \u001b[43mdense_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential_reranking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ephemeral/test_mrc/retrieval.py:654\u001b[0m, in \u001b[0;36mDense_embedding_retrieval.sequential_reranking\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m passages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatas\u001b[38;5;241m.\u001b[39mget_context()\n\u001b[1;32m    653\u001b[0m BM25 \u001b[38;5;241m=\u001b[39m BM25Search()\n\u001b[0;32m--> 654\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# get_relevant_doc_bulk_bm25 메서드를 사용하여 문서 점수와 인덱스 가져오기\u001b[39;00m\n\u001b[1;32m    657\u001b[0m _, doc_indices \u001b[38;5;241m=\u001b[39m BM25\u001b[38;5;241m.\u001b[39mget_relevant_doc_bulk_bm25(queries, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:619\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    617\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m         )\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1761\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1756\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1758\u001b[0m     )\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1762\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1764\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1765\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1766\u001b[0m     )\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for './Dense_embedding_retrieval_model_results/checkpoint-1482'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './Dense_embedding_retrieval_model_results/checkpoint-1482' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from retrieval import Dense_embedding_retrieval, BM25Search\n",
    "from MRC import Extraction_based_MRC\n",
    "\n",
    "\n",
    "dense_retriever = Dense_embedding_retrieval()\n",
    "bm25 = BM25Search()\n",
    "\n",
    "# Load the trained dense retriever model\n",
    "dense_retriever.load_model()\n",
    "\n",
    "# Perform re-ranking using the dense retriever\n",
    "reranked_result = dense_retriever.sequential_reranking(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reranked_result type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "reranked_result sample: {'context': ['마가렛 턴불과 질 타터는 공작자리 델타를 지구에서 가까운 G형 항성 100개 중 \\'최고의 SETI 탐사 대상\\'으로 선정했다.  델타는 금속함량이 높으나 자기장 활동이 최소 수준이고, 자전 속도가 느리며 운동학적으로 우리은하 내 얇은 원반 종족에 들어간다. 델타별의 거주가능영역 근처를 돌거나 혹은 관통하는 가스 행성은 거주가능영역 내를 도는 암석 행성들의 궤도를 불안정하게 만들 수 있다. 시선속도의 변화가 감지되지 않는 것으로 보아 공작자리 델타를 도는 가스 행성은 없는 것 같다. 다만 인공적인 전파원이 관측된 바는 없다 태양과 비슷한 측광적 특질을 보이는 델타는 쌍성이나 다중성계가 아닌 항성들 중 지구에서 가장 가까운 유사태양이다 이 행성은 발견일 기준으로 누적된 발견사례 중에서도 매우 중요한 천체로 평가받았다. 글리제 1132 b는 대기 조성물, 바람의 속도, 일몰시 어머니 별의 색 등을 쉽게 예측할 수 있다.    이는 글리제 1132 b가 발견된 외계 행성 중에서 지구에 매우 가까우며 어머니 별의 크기가 작기 때문에(태양의 21%) 별의 빛이 줄어드는 정도를 보다 쉽게 파악할 수 있기 때문이다. \\n\\n행성의 크기는 지구보다 약 16% 가량 커서 약 1만 4806km이다. 크기는 지구와 비슷하지만 특징은 금성과 비슷하다. 어머니 별과의 거리가 225만km 로 가까운 편이여서 온도는 섭씨 232도에 달할 것으로 예상되기 때문에 물이 존재할 가능성은 희박하지만 대기가 존재할 가능성은 크다고 보고되었다. \\n\\n글리제 1132 b는 단위면적당 지구의 약 19배 복사 에너지를 받는다 이로부터 계산한 글리제 1132 b의 대기 상층부 온도는 금성보다 뜨거우며 지표면으로 하강할수록 온도는 더 올라갈 것이다. \\n\\n2017년 4월에 글리제 1132 b 에 대기가 존재한다는 사실이 확인되었다. 현재까지 알려진 대기가 존재하는 행성 중에 가장 지구와 유사하다. 외계행성 탐사 초기 천문학자들은 태양과 같은 단독성만이 행성을 거느릴 수 있고, 쌍성계에서는 행성이 생겨날 수 없다고 믿었다. 그러나 이후 고니자리 16과 같은 쌍성계에서도 행성이 발견되면서 기존의 믿음이 잘못되었음이 밝혀졌다. 짧은 주기로 공전하는 쌍성계에서 최초로 발견된 행성은 2003년 발견 사실이 검증되었던 PSR B1620-26 b였다. 이 행성의 어머니 항성들은 하나는 펄서, 다른 하나는 백색 왜성으로 상대적으로 가까이 붙어서 서로의 질량 중심을 공전하고 있다.(이 행성은 지금까지 알려진 4개의 펄서 행성 중 하나이다) \\n\\n1999년 미시중력렌즈를 이용, 근접쌍성계인 MACHO-1997-BLG-41 주위 외계 행성 하나를 발견했다는 주장이 제기되었다. 이 행성은 가까이 붙어 서로를 도는 적색 왜성으로부터 멀리 떨어져 공전하고 있는 것으로 추측되었으나, 이후 검증에 실패했다.  \\n\\n쌍성계 용자리 CM 주위를 행성 하나가 돌고 있다는 주장이 몇 년 전 제기되어 지금까지 검증 중이다.  과학자들은 비록 실제 사례가 등장하지 않았지만 근접 쌍성계를 도는 행성을 발견하는 것은 시간문제일 것으로 예상해 왔다.\\n\\n2007년 쌍성계 HD 98800 주위를 먼지 원반이 두르고 있음이 공식적으로 검증되었다. 이 사실은 쌍성주위 행성이 존재할 것이라는 새로운 희망을 안겨주었다.  HD 98800은 근접 쌍성 두 무리로 이루어져 있으며 이들 두 쌍은 다시 서로의 공통 질량 중심을 기준으로 공전 운동을 하고 있는, 다소 복잡한 구조를 갖고 있다. 여기서 한 쪽 쌍은 먼지 원반이 있는 반면 다른 쪽 쌍은 아무것도 주위에 없다.\\n\\n2008년 처녀자리 HW의 근접 쌍성계가 행성 하나와 갈색 왜성 하나를 자식으로 거느리고 있는 것으로 밝혀졌다. 행성 처녀자리 HW c는 슈퍼목성으로 분류할 수 있으며 자신의 어머니 항성 둘 주위를 9년 주기로 한 바퀴 공전한다. 후기 대폭격\\n달 및 다른 지구형 행성에 남아있는 충돌구는 후기 대폭격의 대표적인 증거로, 태양계가 형성된 후 6억 년 후에 행성과 충돌한 소행성의 수가 급증한 현상이다. 니스 모형에서는 초기에 외곽의 미행성대가 천왕성 및 해왕성의 영향으로 흐트러져 내행성의 궤도로 침투하여 얼음 미행성에 의한 충돌이 급증하고, 이후 목성형 행성의 영향으로 소행성대의 소행성도 태양계 안쪽으로 향해 암석질 미행성과의 충돌이 급증한다 이 과정을 통해 달과 충돌하리라 예상되는 미행성 수는 후기 대폭격의 충돌구 기록에서 계산되는 수와 비슷하지만 남아있는 소행성의 분포는 계산이 관측에 부합하지 않는다\\n\\n후기 대폭격로 인해 목성의 위성 가니메데에서는 행성 분화가 촉진되었지만, 칼리스토에서는 행성 분화가 일어나지 않았다.  토성의 위성에는 얼음 미행성이 대량으로 충돌했지만, 결과적으로 얼음은 모두 승화되어 없어졌다. 소설 여명의 로봇 이후 인류가 지구를 벗어나 제 2의 우주개척을 시작하여 많은 행성으로 이주를 한 이후로 많은 시간이 흘러 파운데이션 시리즈에서 말하는 트랜터 행성의 영향력이 커져 은하제국이 성립되기 전 독립행성인 Sark 행성은 식민행성인 Florina에서만 재배되는 섬유작물인 kyrt를 통해서 부를 쌓게된다. 이러한 독점을 막기위한 다른 곳의 재배 시도는 이전까지 실패로 돌아갔었다. 릭(Rik)이라는 한 사람이 기억상실 상태에서 점차 기억을 되찾게되면서 kyrt 의 비밀이 우주에 존재하는 기류에 의한 Florina 행성계 태양이 신성으로 되면서 나오는 특수한 빛에 의한 것임을 밝혀지는 동시에, 항성이 신성이 된다면 그 항성의 주위를 도는 행성들은 파멸적인 종말을 맞이하게되기 때문에 Florina 행성의 위기가 닥쳤다는 것을 알게된다. Florina 와는 다르게 Sark 행성의 입장은 Florina 행성을 잃는 것과 그 재배의 비밀이 밝혀져 독점이 무너지는, 두가지 다 경제적으로 막대한 손실을 의미하는 것이었다. 그 파국을 막기위해 트랜터 행성 측이 행성을 높은 가격으로 구매하겠다는 제안(협박)을 하여 그 문제를 해결해주게된다. 이로서 트랜터는 정치적 영향력을 늘려 은하제국으로 한걸음을 더 나아가는 모습을 보여준다. 케플러 탐사 이전에 케플러-62에는 2MASS J18525105+4520595라는 2MASS 성표번호가 부여되어 있었다. 케플러 인풋 카탈로그(Kepler Input Catalog)에서는 KIC 9002278이 부여되어 있었으며 이 별이 항성면 통과행성을 거느리는 후보로 지목되고 나서 케플러 관심 천체(Kepler object of interest) 번호 KOI-701을 받았다.\\n\\nNASA의 케플러 계획은 항성 앞을 지나가는 행성들을 발견할 목적으로 시작되었으며 케플러-62의 앞을 지나가는 것으로 추정되는 행성들 또한 이 계획 수행 중 발견되었다. 행성 발견에 이용되는 통과법(transit method)은 지구에서 바라볼 때 행성이 항성 앞을 지나가면서 별의 밝기가 살짝 약해지는 것을 이용한 것이다. 다만 \\'후보\\' 명칭을 붙인 이유는 항성의 밝기가 변하는 데에는 행성뿐 아니라 다른 원인도 작용 가능하기 때문이다. \\n\\n행성 발견 논문이 통과된 뒤 케플러 팀은 이 항성에 케플러-62 명칭을 부여했다.  케플러 계획은 행성을 거느리는 것으로 예상되는 후보였다가 실제로 거느리는 것으로 검증되면 그 항성에 \\'케플러(Kepler)\\' 명칭을 붙인다. 케플러-62 또한 이러한 규칙에 따라 붙인 이름이다\\n\\n케플러 계획 탐사대상 항성을 도는 것으로 추정하는 \\'행성 후보\\'들은 발견된 순서대로 항성 이름 뒤에 \".01\", \".02\", \".03\", \".04\", \".05\"...가 붙는다 행성 후보들이 동시에 발견될 경우 공전주기가 짧은 것부터 순서대로 번호를 붙인다 케플러 우주선은 2011년 케플러-62(당시에는 KOI-701)를 5.714932일, 18.16406일, 122.3874일 주기로 공전하는 행성 후보 셋을 발견했으 2012년에는 12.4417일, 267.29일 주기를 갖는 행성 후보 둘을 추가로 발견했다 행성 명칭 b, c, d, e, f는 발견한 순서대로 붙인다. 이 중 b는 어떤 별을 도는 행성 중 최초로 발견된 존재에게 붙이는 기호로 같은 계에서 추후 새로운 행성이 발견되면 로마자 소문자 기호가 순서대로 붙는다.  케플러-62의 경우 발견된 행성 전부가 동시에 공표되었기 때문에 b는 항성에서 제일 가깝고 f가 제일 멀다 케플러-62는 케플러 관심 물체(KOI) 중 행성의 존재가 검증된 항성으로서 62 번째로 등재되었다는 뜻이다. 적색왜성계의 생명체 거주가능성\\\\n \\\\n센타우루스자리 프록시마 b의 발견 이전 TV 다큐멘터리 Alien Worlds 는 생명체를 품는 행성은 프록시마 혹은 다른 적색왜성들의 주변 궤도상에서도 존재할 수 있다는 가정을 세웠다. 이런 행성은 프록시마의 생물권(어머니 별로부터 0.023|–|0.054|AU|e6km|abbr=unit 거리) 안에 있으며 공전 주기는 3.6 ~ 14 일 사이일 것이다. 이 영역 안에서 항성을 도는 외계 행성은 항성에 대해 조석적으로 고정되어 있을 것이다. 만약 이 가상 행성의 궤도 이심률이 작으면 프록시마는 행성의 하늘에서 거의 움직이지 않을 것이며 행성의 영역 대부분에서는 영원한 밤과 낮이 지속될 것이다. 다만 대기가 존재한다면 항성으로부터 빛을 받는 부분의 에너지를 반대쪽으로 재분배할 수 있을 것이다. \\\\n\\\\n센타우루스자리 프록시마의 플레어 폭발은 생물권 내에 있는 행성의 대기를 벗겨낼 수 있으나 이 다큐멘터리의 과학자들은 이 장벽은 극복 가능하다고 생각한다. 예로 나온 우려들 중 하나는 항성 플레어에서 나오는 가속된 입자들의 흐름이 근처 행성의 대기를 벗겨낸다는 것이었다. 만약 행성에 강력한 자기장이 있다면 이 자기장은 입자들을 반사해 낼 것이며, 천천히 자전하는(조석고정이 되어 항성을 1회 돌면서 1회 자전) 행성에도 내부가 녹은 상태로 유지된다면 자기장이 충분히 형성될 수 있을 것이다. \\\\n\\\\n희귀한 지구 가설을 지지하는 과학자들은 적색왜성계가 생명체를 품을 수 있다는 주장에 동의하지 않는다. 프록시마의 생물권 안을 도는 외계 행성은 조석적으로 고정되어 회전 속도가 느려지기에 행성의 자기 활동이 상대적으로 약해질 것이고, 이로 인해 어머니 별이 일으키는 코로나 질량 방출에 대기를 빠른 속도로 잃어버릴 것이다. 목성의 대기에서 보이는 줄무늬는 적도와 평행하면서 행성을 둘러싸는 대(zone)와 띠(belt)라고 불리는 물질의 반대 순환류에 의한 것이다. 대는 밝은 줄무늬로, 대기에서 상대적으로 고도가 높은 곳에 있다. 이들은 내부의 상승 기류를 가지고 있는 고기압 영역이다. 띠는 어두운 줄무늬로, 대기에서 상대적으로 고도가 낮은 곳에 있으며, 내부의 하강 기류를 가진다. 이들은 저기압 영역이다. 이러한 구조는 지구 대기의 고기압 및 저기압 세포와 어느정도 유사하나, 국지 작은 기압 세포와 상반되는 행성 전체를 둘러싸는 위도 줄무늬로서 매우 다른 구조를 가지고 있다. 이는 행성의 빠른 자전과 근본적인 대칭으로 인한 결과로 보인다. 행성에는 국지적인 가열을 일으키는 바다나 육지가 없으며 자전 속도는 지구보다 훨씬 빠르다.\\n\\n행성에는 서로 다른 크기와 색상을 갖는 점과 같은 작은 구조들이 있다. 목성에서, 그러한 특색 중에서 가장 유명한 것은 대적점으로, 적어도 300년 동안 존재해 왔다. 이러한 구조의 실체는 거대한 폭풍이다. 그러한 점 중에 일부는 적란운이기도 하다.\\x7f 시계자리 요타(ι Hor / ι Horologii)는 시계자리 방향으로 지구에서 약 56광년 떨어진 곳에 있는, 태양과 비슷한 항성이다. 분광형은 G0V 로 황색 왜성이다(예전에는 분광형 G3에 준거성으로 분류되었다). 질량과 크기는 태양보다 조금씩 더 크며 밝기는 1.5배 정도이다.\\n\\n1998년 기준으로 이 별 주위를 외계 행성 하나가 돌고 있는 것이 확인되었다. 이 행성의 궤도는 지구 궤도 정도 크기이기 때문에 시계자리 요타는 NASA가 선정한 지구형 행성 탐사 계획의 69번째 항성으로 지목되었다. 2000년 항성 주위에 원시행성계 원반이 있다는 발표가 있었으나 이후 관측 기구의 결함 때문에 생긴 착오로 밝혀졌다. 프톨레마이오스는 다음과 같은 가장 안쪽에서 시작하는 행성구(行星球)의 순서를 할당했다.:\\n# 달\\n# 수성\\n# 금성\\n# 태양\\n# 화성\\n# 목성\\n# 토성\\n# 항성구(恒星球)\\n\\n다른 순서를 제안한 고대의 저자들도 있다. 플라톤(기원전 427~347년경)은 태양을 달 다음의 순서로보았다. 마르티아누스 카펠라(5세기)는 금성과 수성이 태양을 공전하는 것으로 보았다. 프톨레마이오스의 전거는 대부분의 중세 이슬람의 그리고 중세 후기 유럽의 천문학자들에게 선호되었다.\\n\\n프톨레마이오스는 그의 그리스인 선임자들로부터 지구중심적 도구와 행성이 하늘에서 나타날 곳의 위치를 예측하는데 사용되는 일부분의 모형을 물려받았다. 페르게의 아폴로니오스(기원전 262~190년)은 천문학에 이심원과 주전원 및 편심적 대원을 소개했다. 히파르코스(기원전 2세기)는 달과 태양의 이동에 관한 수학적 모형을 만들었다. 히파르코스는 메소포타미아 천문학에 대한 얼마간의 지식을 갖고 있었고, 그는 그리스식 체계가 바빌로니아의 것과 정확하게 일치한다고 보았다. 그는 남은 다섯 행성에 관한 정확한 모형을 제작하지는 못하였다.\\n\\n알마게스트는 단순한 편심적 대원으로 구성된 히파르코스의 태양 모형을 채택했다. 프톨레마이오스는 달에 관하여 히파르코스의 \\'이심원의 주전원\\'을 적용한 것을 시작으로, 천문 역사가들이 \"크랭크 기구\"라고 부르는 장치를 추가했다.  그는 동시심이라고 불리는 세 번째 장치를 소개함으로써 히파르코스가 실패했던 다른 행성의 모형을 제작하는데 성공했다.\\n\\n프톨레마이오스는 알마게스트를 수학적 천문학의 교과서라고 기술했다. 그것은 천구의 객체의 이동을 예측하는데 사용될 수 있었던, 주기의 원들의 조합에 기초하는 지구중심적 행성 모델이다. 《행성에 관한 가설(Planetary Hypotheses)》이라는 후일의 한 서적에서, 프톨레마이오스는 그의 지구중심적 모형이 어떻게 삼차원의 천구 또는 부분적 천구로 변환되는지를 설명했다. 수학적인 알마게스트와는 반대로, 《행성에 관한 가설》은 때때로 우주 철학에 관한 서적으로 묘사된다. 이 뮤지컬은 전편 오페라의 유령(\"The Phantom of the Opera\")에 참여했던 무대 디자이너 마리아 비욘슨의 \\'오페라의 유령\\'의 결말이 마음에 들지 않는다는 아쉬움 섞인 투덜거림에 의해 시작된 것이라 해도 과언이 아니다. 이로인해 앤드루 로이드 웨버에게 그 결말에 대해 다시 생각해볼 계기를 만들어주었기 때문이다.\\n\\n뮤지컬은 전편에서 연기처럼 사라졌던 괴신사 오페라의 유령(팬텀)과 그의 오랜 뮤즈이자 사랑의 대상인 가수, 크리스틴 다에를 중심으로 하고 있다. 뮤지컬 속에서 주 무대가 되는 \\'코니 아일랜드(놀이공원)\\'는 회전목마, 서커스 천막, 대관람차 같은 볼거리들이 화려한 이미지에 일가견이 있는 무대 디자이너 밥 크로울리를 통해 재현되며 관객들의 눈을 홀린다. 전편에서의 가장 큰 갈등구조였던 \\'세상을 증오하는 팬텀과 팬텀을 두려워하고 혐오하는 세상\\'과는 달리 러브 네버 다이즈에서는 팬텀과 크리스틴 다에의 애정선과 맥 지리와의 갈등, 구스타프의 출생의 비밀이 주를 이룬다. 또한 주인공들이 과거를 회상할 때나 앞으로 벌어진 상황에 대한 복선으로 오페라의 유령에 쓰인 곡들이 짤막하게 삽입된다. 작품의 시대적, 공간적 배경을 반영하는 쇼툰과 재즈뿐만 아니라 강렬한 록 넘버가 가미되었고 구스타브를 연기하는 보이 소프라노를 다양한 곡에서 적극적으로 활용하여 색다른 매력을 더했다. Oph 162225-240515(줄여서 Oph 1622라고 한다.)는 서로의 질량 중심을 도는 갈색 왜성 두 개로 이루어진 계(系)이다. 1622는 뱀주인자리에 있으며 지구에서 약 400광년 떨어져 있다. B는 A로부터 지구에서 관찰할 때 약 1.94초각 떨어져 있다. 이들의 나이는 5백만 년 정도에 불과한 것으로 보인다. 이들은 갈색 왜성도 쌍성처럼 서로 공전할 수 있다는 사실을 입증한 최초의 천체였다.\\n\\n이 행성이 처음 발견되었을 때는 질량이 각각 목성의 14배, 7배로 일반 외계 행성과 크게 다를 것이 없는 것처럼 보였으나, 이후 관측을 통해 실제 질량은 이보다 더 큼이 드러났다. 행성과 갈색 왜성을 가르는 기준은 목성 질량의 13배로, 이들의 질량은 갈색왜성의 하한선에서 약간 위 정도이다. 구체적으로 Oph1622A는 목성의 12~21배, Oph1622B는 목성의 9~20배 정도이다.\\n\\n두 갈색 왜성이 떨어져 있는 거리는 실제로 240AU이다. 멀리 떨어져 있기 때문에 Space.com에서는 “중력적으로 헐겁게 연결되어 있기 때문에, 만약 이들 근처를 질량이 큰 제3의 천체가 스쳐 갈 경우 이들의 연결은 끊어질 것이다.”라고 언급했다. 동시에 “떠돌이 행성(free-floating planet)들이 중력이 강한 항성으로부터 탈출했다는 가설은 실현 가능성이 별로 없다.”라고 하여, 떠돌이 행성이 있더라도 그것은 느슨하게 묶인 Oph 1622계(系) 같은 경우에 적용된다고 했다. HD 100546 또는 파리자리\\xa0은 지구로부터 320광년 떨어져 있는 항성이다\\xa0이 별은 목성 질량의 20배 정도 되는 동반천체 HD 100546 b를 거느리고 있는데 \\xa0추가 연구결과에 따라 일반적인 외계 행성이 아니라 갈색 왜성으로 판명될 수도 있다. \\xa0행성 외에도 이 별로부터 0.2 ~ 4 천문단위 거리에 별주위 원반이 둘리어 있으며 13 ~ 수백 천문단위 거리에 다시 원반이 형성되어 있다. 항성으로부터 약 47 천문단위 거리에 원시행성 하나가 만들어지고 있다는 증거가 발견되었다. \\n\\n항성의 나이는 대략 1천만 년 정도로 보이는데 이는 이 별이 속한 허빅 Ae/Be 별 중에서는 거의 진화단계 막바지에 이른 나이이다. 이 별은 허빅 Ae/Be 별 중에서 태양계로부터 가장 가까운 존재이기도 하다 일부 행성은 쌍성 중 한 별을 돌며  몇몇 쌍성주위 행성은 두 별의 주위를 돌고 있었다. 삼중성계 내 행성 여러 개가 확인된바 있으며  케플러-64와 같은 사중성계를 도는 행성도 한 개 발견되었다.\\n\\n케플러 우주선의 조사결과에 따르면 쌍성주위 행성 시스템은 상대적으로 흔하다.(2013년 10월 기준으로 케플러는 대략 1000개의 식쌍성에서 7개의 쌍성주위 행성을 발견했음) 한 가지 의문점은, 연구대상 쌍성의 공전주기는 최대 2.7일임에 비해 쌍성주위 행성을 거느린 쌍성의 공전주기는 최소 7.4일 이상이었다. 케플러가 발견한 또다른 놀라운 사실은 이들을 도는 행성은 임계 불안정 반경(이론적인 계산으로 근접쌍성 주변을 도는 행성은 쌍성이 서로 떨어져 있는 거리의 대략 2~3배만큼 항성군으로부터 떨어져 있어야 안정되게 공전을 지속할 수 있음)에 매우 가까이 붙어 돌고 있었다는 것이다. \\n\\n2014년 통계를 통해 짝별을 연구한 결과 외계 행성 주인별의 절반 정도가 짝별을 100 천문단위 거리 이내에 데리고 있었다.   이는 홑별로 생각했던 외계행성의 주인별 상당수가 사실은 쌍성계였고, 실제로 쌍성 중 어느 별이 행성을 실질적으로 데리고 있는지 불확실함을 의미한다. 따라서 지금까지 발표된 ‘통과행성’ 다수의 매개변수가 매우 부정확할 수 있는데 이는 외계행성의 반지름과 항성으로부터의 거리는 항성의 매개변수로부터 이끌어내는 것이기 때문이다. 스펙클 이미징과 같은 사진촬영이나 아주 가까이 붙은 짝별의 존재를 감지해내는 시선속도 기법으로 짝별의 존재를 배제해야 정확한 외계행성의 존재가 입증될 터인데, 지금까지 발견된 행성 다수는 이런 검증과정을 거치지 않았다. 어느 별이 행성을 거느리는 주인인지 확실히 밝혀지지 않은 쌍성계의 예로 케플러-132와 케플러-296이 있다. 이 별의 겉보기등급은 5.49로 관측에 적합한 환경에서 맨눈으로 볼 수 있다.\\n\\n페가수스자리 51은 1989년 《차가운 항성들의 개정 MK 분광형 퍼킨스 항성목록》(The Perkins catalog of revised MK types for the cooler stars)에 분광형 G2IV 표준별로 수록되었다. 역사적으로 이 별은 일반적으로 분광형 G5V를 받아 왔으 보다 최근 항성목록들에도 보통 주계열성으로 수록되어 있다 아직은 중심핵의 열핵 반응을 통해 에너지를 생산하고 있지만 태양보다 진화가 더 많이 이뤄진 것으로 보인다 채층의 유효온도는 약 5571\\xa0K으로 이 온도에서 항성은 G형의 노란 색으로 빛난다 예상 나이는 61 ~ 81억 년으로 태양보다 좀 더 많으며 반지름은 태양보다 24% 크고 질량은 11% 더 크다. 수소/헬륨보다 무거운 원소가 항성 구성물질 중 차지하는 비율이 태양보다 더 크다.(과학자들은 이 개념을 항성의 \\'금속함량\\'으로 부른다.) 페가수스자리 51처럼 금속함량이 높은 항성들은 행성을 거느릴 확률이 보다 높아진다 1996년 천문학자 Baliunas, Sokoloff, Soon은 이 별의 자전주기가 37일임을 알아냈다\\n\\n1981년 연구에서는 변광성으로 의심되기도 했으 후속 연구를 통해 1977년부터 1989년 사이 채층 활동이 없었음이 드러났다. 1994년부터 2007년 사이 수행된 추가 관측에서도 활동량은 미미하거나 없었다. 상대적으로 낮은 엑스선 방출과 위 연구 결과로 보아 이 별은 몬더 극소기 단계를 지나고 있어 흑점의 수가 줄어든 상태일 가능성이 있다\\n\\n지구에서 바라본 별의 궤도경사각은 30 도이다 이 항성의 시선속도는 매우 일정해서 천문학자이자 행성 사냥꾼 제프리 마시는 랄랑드 21185를 적색왜성의 안정성 단계들 중 \\'평범함\\'의 완벽한 예시로 취급하고 있다.  이런 부정적인 결과들과 기타 연구들이 랄랑드 21185에 행성계가 존재할 가능성을 전적으로 차단하는 것은 아니지만, 존재 가능한 행성의 질량에 상한선을 설정한다. 앞으로 수행될 지상 및 우주 기반 관측들은 이 상한선을 분명히 낮출 것이며 질량 작은 행성을 발견할 수도 있을 것이다.\\n\\n랄랑드 21185의 생명체 거주가능 영역(지구 비슷한 행성에 액체 물이 존재할 수 있는 위치)은 0.11 ~ 0.24 천문단위 영역에 걸쳐 형성된다. 여기에서 1 천문단위는 지구로부터 태양까지의 평균 거리이다 행성의 역행 운동은 지구가 보다 느린 외행성을 지날 때나, 보다 빠른 행성이 지구를 지날 때, 하늘을 통해서 행성이 거꾸로 이동하는 것으로 보이는 것이다. 부차적 진행 또는 일년에 하루의 진행 기법에 있어서, 역행 운동은 차트에서 한 행성이 하루 앞으로 진행하는 것이 그 행성이 반시계 방향인 거꾸로 이동함을 야기한다는 의미이다. 점성술에서, 전통적으로 그러한 반대방향으로의 이동은 그것이 \\'자연의\\' 이동 규칙에 거스르고 있었으므로, 불운하거나 불긴한 것으로 여겨졌고 출생 때에 역행 하는 행성은 출생 천궁도의 취약점으로 여겨졌다.\\\\n\\\\n행성의 역행 운동은 주로 통과에 관해서만 언급되는데도 불구하고, 대부분의 점성가들은 그것이 시련과 곤경을 의미한다고 여긴다. 예를 들어, 일반적으로 수성의 역행은 우편이나 이메일이 잘못 전달되는 것과 말의 오해와 같은 같은 의사소통의 곤란과 여행의 지연이나 무산을 뜻한다. 그러나, 어떤 점성가들은 순행에서 역행으로의 변화가 무엇에 대해 자동적으로 억제와 제한을 (또는 급작스러운 반대방향으로의 전향을) 야기하는 것으로 여기지 않는다. 그들에 의해서는 오히려 어느 이동 방항으로의 변화는 다만 삶의 어떤 부분에 대한 개인의 조정에 있어서의 변화를 나타낸다고 여겨진다. 사실, 특히 외행성은 40% 이상의 시간은 역행하기 때문에, 많은 점성가들은 역행 운동에 어떤 특별한 중요성을 부여하지 않는다. 만일, 진행된 행성에 역행의 완전한 내포가 있다해도 그들에게는 그것이 상대적으로 크게 고려되지는 않는다. 고전적인 열적 탈출 메커니즘 가운데 하나로 진스 탈출 이 있다. 기체 분자의 평균 속도는 온도에 따라 결정되지만, 분자들끼리 충돌하면서 운동 에너지를 얻거나 잃는 과정에서 개별 분자의 속력은 크게 달라질 수 있다. 분자들의 운동 에너지의 분포는 맥스웰 분포로 기술할 수 있다. 운동 에너지와 분자의 질량을 알면 E_}={\\\\frac  {1}{2}}mv^{2} 식으로부터 속도를 구할 수 있다.\\n맥스웰 분포의 긴 꼬리 부분에 해당되는 속도로 움직이는 분자들은 탈출 속도에 다다를 수 있는데, 이런 분자가 대기권에서 평균자유이동경로가 높이척도 수준에 달하는 높이까지 올라간다면 대기를\\xa0벗어날 수 있다.\\n기체 분자의 질량이 클수록 주어진 온도에서 그 분자들의 평균속도가 낮으므로 탈출 속도 수준으로 빨라질 가능성도 낮아진다.\\n수소가 이산화탄소보다 더 쉽게 대기를 빠져나가는 이유가 바로 이 때문이다. 또한 행성의 질량이 클수록 탈출 속도도 커져서 빠져나가기가 어려워진다. 지구 대기에는 수소와 헬륨이 별로 없는데 반해 거대 가스 행성에는 여전히 많은 것도 바로 이런 이유 때문이다. 행성과 그 행성이 공전하고 있는 별 사이의 거리도 중요한 요인이다. 별에서 가까운 행성에서는 대기가 더 뜨겁기 때문에 분자들의 속도 분포가 더 높은 쪽으로 치우치게 되어 탈출할 가능성이 높다. 별에서 멀리 떨어진 행성에서는 대기가 더 차가우므로 속도가 낮은 쪽으로 분포하게 되어 탈출 가능성이 낮다. 토성의 위성인 타이탄은 지구에 비해 작지만 태양에서 멀기 때문에 대기가 더 두껍다.\\n압력과 온도가 충분히 높은 대기에서는 “유체역학적인 탈출”이라는 다른 탈출 메커니즘으로 대기를 탈출할 수 있다. 열 에너지가 누적되어 생기는 압력의 차이 때문에 대기가 바람 불듯 우주로 흘러나가 버리는 방식이다. 이런 경우에는 일반적으로 대기를 빠져나가기 힘든 무거운 분자도 같이 쓸려나갈 수 있다. 유체역학적인 탈출은 일부 뜨거운 목성(HD 209458 b, HD 189733 b)이나 뜨거운 해왕성(GJ 436 b)을 비롯하여 행성 가까이 있는 태양계 밖 행성에서 관찰된 바 있다. 이 정의는 엄밀해야 할 정의가 모호성을 띄고 있다는 점에서 비판을 많이 받았다. 천문학자 필 플래이트와 NCSE 작가 닉 마츠케는 이 정의에 따르면 행성이 항성 궤도 바깥으로 밀려나거나 떠돌이 행성처럼 항성 궤도상에서 형성되지 않았다면 행성이라고 부르지 못한다는 점을 비판했다.   하지만 같은 맥락에서 위성은 행성 궤도 바깥으로 나가면 행성으로 여긴다는 점에서, 이 의견은 논란이 있다.\\n\\n또한 이중행성의 정의도 반론이 있었다. 현재 지구와 달의 질량중심은 지구 내부에 있지만 조석 가속을 통해 결과적으로 지구 바깥으로 밀려나게 되는데, 만약 그렇다면 달 또한 행성으로 분류될 수 있었다. 하지만 실질적으로 이 과정은 시간이 아주 오래 걸려, 태양이 적색거성이 되어 지구와 달 모두 소멸되기까지 일어나지는 않을 것으로 예측된다. \\n\\n마이클 브라운은 2006년 8월 18일 라디오 프로그램 \"Science Friday\"에서 이 문제는 \"대륙\"이라는 단어와 비슷하며, \"대륙이라는 단어 자체는 과학적 의미가 없는 문화적인 단어일 뿐이고, 자신은 지질학계에서 대륙이라는 단어의 엄밀한 정의를 부여하려고 하지 않은 것이 잘 한 일\"이라고 생각한다고 언급하였다. \\n\\n8월 18일 오언 깅거리치는 자신이 받은 서신에서 찬반이 고르게 갈렸다고 말했다. 용암 행성 대다수는 어머니 별을 아주 가까이에서 돌고 있는 암석 행성일 것이다. 공전 궤도가 찌그러져 있는 행성의 경우 가까이 있는 항성으로부터의 중력이 행성을 주기적으로 뒤틀어 놓아 마찰을 발생시키고 이는 내부열로 바뀐다. 이 조석열은 암석을 녹여 마그마로 만들고 마그마는 화산 폭발로 표면으로 흘러나온다. 이 과정은 태양계 위성 이오가 어머니 행성 목성과 가까이 붙어 있어 기조력으로 뒤틀리는 것과 비슷하다. 이오는 태양계에서 지질학적으로 가장 활발한 천체로 표면에 수백 개의 화산과 거기서 흘러나온 황 용암 구조가 널려 있다. 어머니 별에 가까이 붙어 있는 용암 행성들은 이오보다도 화산 활동이 더 왕성할 것이다. 일부 천문학자는 이로부터 \\'슈퍼이오\\'라는 신조어를 만들어 내기도 했다. 이 슈퍼이오는 화산 폭발이 끊임없이 일어나고 표면에는 황이 넓게 퍼져 있는 등 이오와 닮았을 것이다. \\\\n\\\\n그러나 조석열 외에도 용암 행성을 만드는 요인은 또 있다. 어머니 별 가까이에서 조석력을 받는 것 말고도 항성으로부터 나오는 막대한 복사열로 표면이 녹아 곧장 용암이 될 수도 있다. 조석 고정된 행성에서 항성의 빛을 받는 쪽은 용암의 바다로 덮여 있고 혹은 열기로 증발한 암석이 응축되어 암석의 비가 내릴 것이다. 빛을 받지 않는 밤의 반구 쪽에는 용암이 고여 있는 호수가 존재할 수도 있다. 행성 자체의 질량도 원인이 된다. 암석 행성 표면의 지각판 활동량은 천체의 질량과 연관이 있으며 지구보다 무거운 행성은 지구보다 활발한 화산 활동과 지질 활동을 할 것이다.\\\\n\\\\n별과 가깝거나 행성 질량이 크지 않더라도 태어난 지 얼마 안 된 원시행성은 내부열이 막대하여 화산활동이 매우 빈번히 일어나고 표면은 용암으로 덮인다. 또는 대규모의 충돌 사건으로도 용암 행성이 될 수 있다. 예로써 과거 지구는 화성 정도 크기 천체와 부딪쳤고 표면은 용암의 바다로 뒤덮였다.(이 충돌 결과 달이 만들어졌다.) 또한 과학자들이 시뮬레이션 실험을 한 결과 큰 운석과 충돌할 경우 지각에 구멍이 뚫려 암석 기체가 빠져나와 지구가 암석 기체로 뒤덮이는 내용인데, 이 일이 일어난 후 시간이 지나면 암석 기체가 용암이 되며 식는다.', 'R. J. 셰퍼는 목격자의 증언을 검증하는 점검 목록을 제공한다. \\n\\n# 저술이 실제로 의미하는 것이 문자 그대로의 의미와 다른 것인가 ? 언어는 현재 사용되는 의미와 다른 것인가 ? 문장이 풍자적이지는 않은가 ? (즉, 말하는 것과 다른 의미를 갖고 있는 것은 아닌가 ?)\\n# 저자는 보고하는 사항을 어떻게 관찰하였나 ? 저자가 느끼는 것과 관찰 대상은 같은 것인가 ? 저자가 보고, 듣고, 만져보기에 적당한 위치에 있었나 ? 저자는 적절한 사회적 관찰 능력을 갖고 있는가 ? 즉, 그 언어를 이해할 수 있는가 ? 그 외에 전문적인 지식이 필요한 것은 아닐까 ? (예를 들면 법률이나 군사) 저자는 배우자나 비밀경찰에 위협받고 있지 않았나 ?\\n# 저자는 어떻게 기록하였나 ? 저자의 기록 능력은 어떠한가 ?\\n## 기록 능력과 관련하여, 저자는 편견을 갖고 있지는 않은가 ? 저자는 기록 작성을 위해 충분한 시간이 있었나 ? 기록에 적합한 장소가 있었나 ? 적절한 기록 용구가 있었나 ?\\n## 저자가 관찰했을 때부터 기록했을 때까지의 시간은 ? 꽤 시간이 지나지는 않았나 ?\\n## 저자가 보고하려는 의도는 ? 누구를 위해 보고하였나 ? 그 때에 주변의 인물이 왜곡을 요구하거나 권장하지는 않았는가 ?\\n## 의도하였던 진실성에 외부의 관여가 없었나 ? 보고 사항에 무관심하여 의도하지 않은 왜곡의 가능성은 없는가 ? 저자 자신이 손해가 되는 내용을 적게 되어 왜곡할 가능성이 있지는 않았나 ? 저자는 우연으로 또는 항상 정보가 주어져 의도적으로 오류를 유발할 가능성은 없었는가 ?\\n# 저자의 언급은 본질적으로 현실성이 결여되어 있지는 않나 ? 즉, 인간성에 반하거나 일반 상식에 배치(背馳)되지 않는가 ?\\n# 정보의 유형에 따라 관찰과 보고가 쉬운 경우가 있음을 명심한다.\\n# 글 내부적으로 모순이 있지 않은가 ?\\n\\n루이스 곳샬크는 추가적으로 생각할 사항으로, \\'해당 사실이 널리 알려지지 않았더라도, 사안에 따라서는 흔히 일어나거나 충분히 가능한 일이어서 오류나 거짓이 있을 것 같지 않은 경우도 있다\\'는 것을 지적하였다. \\n\\n갸라한은 대부분의 정보가 \\'간접적인 목격자\\'로부터 나옴을 지적하였다. 이들은 해당 장소에 있지 않았으나 다른 이들로부터 그 내용을 전해들은 것이다.  곳샬크는, \\'역사가는 이따금 소문에 의한 증거를 사용할 수 있음\\'을 지적한다. 어쨌거나, 2차 목격자의 정보를 사용하는 경우 이들을 전적으로 신용할 수는 없으며, 오히려 1) 목격자는 어떠한 1차 증언을 근거로 하고 있는지, 2) 2차 목격자가 1차 증언을 대체적으로 정확히 보고하는지, 3) 그렇지 못하다면, 목격자는 1차 증언을 얼마나 상세하게 보고하는지 물어보아야 한다는 것이다. 그는 2)번과 3)번 질문에서 만족할 만한 대답을 얻었다면 역사가는 1차 증언의 전체 또는 요점을 제공받은 것이며, 이러한 경우에는 목격자의 정보에 대하여 1차 증거와 같이 테스트를 한다고 설명하였다. 밀리 위트캅(Milly Witkop)은 우크라이나 태생의 유대인이자 아나코 생디칼리스트이며 여성주의자 작가이자 활동가였다. 그녀는 루돌프 로커의 사실혼 아내였다. Association of German Trade Unions (FVdG)의 의장 Fritz Kater는 아나코 생디칼리스트 노동조합인 Free Workers\\' Union of Germany를 만들기 위해 로커를 초대했고, 1918년 11월 로커와 위트캅은 베를린으로 옮겨가게 된다.  위트캅과 로커 모두 이후 FAUD의 멤버가 된다. 조합이 설립된 이후인 1919년 초반 조합내에서 여성과 소녀의 역할에 대한 토론이 시작되었다. 남성지배적인 조직에서 처음에는 젠더 이슈가 무시되곤 했지만, 여성들은 곧 정규 노조와 평행하게 그들 자신의 노조를 결성하기 시작했으며 이는 FAUD의 일부를 형성하게 된다. 위트캅은 1920년 베를린에서의 여성노조의 주요 설립자 중의 한명이였다. 1921년 10월 15일 여성노조는 뒤셀도르프에서 전국대회를 열고 Syndicalist Women\\'s Union(SFB)를 전국적인 레벨로 창립한다. 얼마 뒤에 위트캅은 SFB를 위한 강령으로 Was will der Syndikalistische Frauenbund?(생디칼리스트 여성노조가 원하는 것은 무엇인가?)의 초안을 작성한다. 1921년 부터 Frauenbund는 FAUD의 기관 SFB에 출판되기 시작했고, 위트캅은 주요한 작가중의 한명이였다\\n\\n위트캅은 프롤레타리아 여성은 남성 노동자처럼 자본주의에 의해서만 착취되는 것이 아니라 남성에 의해서도 착취 받는 것이라고 추론했다. 그녀는 노동자들이 자본주의에 대항해 싸우는것 처럼, 여성들은 그들 자신의 권리를 위해서도 싸워야 한다고 주장했다. 그녀는 또한 여성의 계급 투쟁에 참여에 대한 필요성을 주장했다. 주부들 역시 이런 투쟁을 지지하기 위해 보이콧을 사용할 수 있다. 이로서 그녀는 FAUD 내부의 자율적인 여성 조직의 필요성을 결론 지었다. 위트캅은 가사 노동 역시 똑같이 임금노동의 간주 되어야 한다고 생각했다. 블라디미르 레닌은 1902년 소책자 『무엇을 할 것인가?』(Что делать?)를 통하여 당시 러시아 사회민주노동당 다수가 갖고 있던 투쟁 방식을 비판하였다. 그는 특히 노동자가 주로 진행하는 임금 투쟁만으로는 혁명을 이룰 수 없다고 하였다. 그리고 노동자의 임금 투쟁에 의존하는 운동이 결국에는 독일 사회민주당의 사례와 같이 기회주의 경향을 낳을 수 있다고 경고하였고 이러한 임금 투쟁이 어떠한 혁명적 변화를 불러일으키는 주된 원인이 될 수 없다고 하였다. \\n\\n이와 더불어 레닌은 러시아 사회민주노동당의 주요 이론가들이 독일 사회민주당의 합법주의에 빠져 있는 것을 비판하였다. 러시아 제국은 1861년 3월 3일 농노 해방령을 통하여 일부 자본주의적 소유 구조를 도입하였으나, 실질적으로는 봉건적 사회 구조를 유지하고 있었다. 농노 해방령 이후 노동자의 수는 증가하였고 명목상 노동조합 성립이 합법으로 되었으나, 노동자의 단체 행동권은 사실상 보장되지 않았다. 이러한 상황에서 독일식 합법주의에 기초한 사회주의자들은 노동자의 단체 행동권, 파업권 보장 등과 같은 자유주의 정책이 들어서게 되면 노동자들은 임금 투쟁을 더욱 가열차게 할 수 있고 그 결과 모든 것이 해결될 것이라고 여기고 있었다. 이런 이유로 인해 당시 수많은 사회주의자들은 러시아 제국 정부에 유화적인 태도로 노동자의 권리를 보장할 것을 요구한 것이다.\\n\\n레닌은 저본주의 사회에서 노동자 투쟁이 합법적으로 보장받는다고 하더라도 그것은 어디까지나 자본가의 권력이 유지되는 선에서만 허용될 것이라고 하였다. 그는 단순히 합법적 활동을 모두 부정하는 방법론만으로는 혁명을 이룰 수는 없다는 것을 강조하였지만, 마르크스주의자는 합법의 영역과 비법(非法)의 영역을 나눠놓고, 매 상황에 따라 변칙적으로 이를 적용하여 최대한 혁명이 이루어질 수 있는 활동을 해야 한다고 주장하였다. 그리하여 레닌은 그것의 기반이 정당으로서 의회 내에서 투쟁하는 것보다는, 노동자 하위 조직을 단단히 조직하고 이러한 조직에서 마르크스주의를 널리 설파하여 노동자의 계급의식화를 강력히 형성해야 함을 주장하였다. 그리고 이러한 활동이 바탕이 되어야 의회 투쟁도 의미가 생긴다는 것이 『무엇을 할 것인가?』의 주된 논점이었다. 하지만 독일 사회민주당의 이론가인 에두아르트 베른슈타인(Eduard Bernstein)의 영향을 받은 사회주의자들은 오로지 합법적 임금 투쟁과 의회 활동에 주력해야 함을 강조하였다. 시간이 흐를수록 러시아 사회민주노동당의 흐름은 크게 합법투쟁을 중점으로 하여 투쟁을 진행하자는 의견을 신봉하는 쪽, 그리고 모든 수단을 동원하여 투쟁을 진행하자는 의견을 신봉하는 쪽, 두 가지로 나눠졌다. 교구장 승좌식 당시 김근상 주교가 한 연설은 다음과 같다. \\n\\n우선 여러분 모두에게 마음을 다해 감사의 인사를 전합니다. 며칠 상당히 추웠는데, 참석해 주셔서 감사합니다. 앞으로 나는 무엇을 하여야 할 것인가? 무엇을 할 수 있을까? 그 일을 해야 진정 하느님이 좋아하시고 만족해하실까? 그 일을 할 수 있는 능력이 있기는 한 것인가? 여러 가지 생각을 하면서 이 취임사를 만들었습니다. 김근상 주교가 오늘 서울 교구장으로 취임하면서 이렇게 말합니다.\\n\\n지금은 그 어느 때보다 어려운 시기입니다. 파산하는 자본주의의 끝을 말하려는 것은 아닙니다. 이미 무한 경쟁과 힘의 축적으로 표현되는 신자유주의 경제 질서는 이렇게 무너지도록 예견된 것입니다. 그 결과 돈이 필요해서 전쟁을 일으키고, 포탄을 쏟아 붓는 전쟁은 그치지 않고 있습니다. 오늘도 저 멀리 팔레스타인에서는 무고한 수백만 명의 어린이들이 어처구니없이 하느님의 이름으로 자국을 보호한다는 이유로 죽어가고 있습니다. 그곳뿐만이 아니지요. 미얀마에서도, 그루지야에서도 우리의 반쪽 저 북에도 우리 하느님은, 우리 예수님은 고통을 당하고 계십니다.\\n\\n우리나라는 어떻습니까? 회사가 부도가 나고 수많은 일용직 노동자들이 거리를 해매며, 졸업생들은 일자리가 없어서 쩔쩔매는 어려운 상황이 개선될 기미가 보이지 않습니다. 그런데 문제는 우리의 위기가 꼭 경제적인 문제로부터 기인하느냐 하는 것입니다. 아닐 것입니다. 단지 경제적인 문제가 우리의 삶을 위협하는 것은 아닐 것입니다. 무엇이 우리의 삶에 더 소중한 것인지 잊고 살았다는 것입니다.\\n\\n더 안타까운 것은 우리 교회가 이런 문제로 자유로울 수 없다는 것입니다. 언제부터인지 우리 교회도 맘몬을 욕하면서도 맘몬의 노예가 되어가는 성장주의 신화에서 벗어나지 못했습니다. 하지만 여기까지여야 합니다. 더 이상은 안 됩니다. 지금이야말로 하느님의 나라를 향한 우리의 노력이 근본적으로 새로워져야 할 것입니다.\\n\\n예수님께서 쓰러져가는 세상을 향해 외치신 하느님 나라의 비전을 다시 한 번 기억해내야 합니다. 세상을 구하기 위해 예수님이 스스로 어떻게 사셨는지를 기억해야 합니다. 하느님은 비교가치에 따라 은혜를 베풀지 않습니다. 하느님은 환율에 따라 은혜를 내리지 않습니다. 하느님은 기름 값에 따라 나라를 선택하지도 않으시고, 집값에 따라 살 집을 정하시지도 않으시며, 증권의 부침에 따라 투자를 결정하지 않으십니다. 하느님에게는 더 특별한 민족이 없으며, 이슬람이나, 유대인이나, 개신교 신자나, 로마 가톨릭 신자나, 성공회 교인이냐 하는 이름표도 소용이 없습니다. 오직 십자가의 무조건적인 사랑으로 은혜를 베풀어 주실 뿐입니다. 우리는 이 은혜를 붙잡고 값을 매길 수 없는 하느님의 은혜만 가지고 감동하며 살아가는 교회를 꿈꾸고 있습니다.\\n\\n세상의 눈으로는 보잘것없이 보이겠지만 그런 눈으로 하느님을 만날 것입니다. 그런 손으로 예수님을 만날 것입니다. 교회는 세상과 구별된 거룩한 삶을 누리기 위해 존재하는 게 아닙니다. 교회는 교회 자체를 유지하기 위해 존재하는 것은 더더욱 아닙니다. 교회는 세상질서를 거슬러 인간과 세계를 그리스도 안에서 화해시키고 하나 되도록 하기 위해서 존재합니다. 예수님께서 당신의 몸을 바쳐 갈등의 몸을 헐고 화해를 가능하게 하셨듯이, 설사 자기는 썩어 없어질지라도 세상을 향해 화해의 증인이 되어야 하는 존재가 바로 교회입니다. 저는 여러분과 함께 그런 교회를 만들어 나갈 수 있기를 희망합니다.\\n\\n여러분은 오늘 저의 교구장 취임을 축하하기 위해 모였습니다. 그러나 이 자리는 저 개인의 영광을 넘어 있습니다. 물론 영광스러운 일입니다만 저는 기꺼이 이 영광스러움을 여러분과 함께 나누고 싶습니다. 동시에 부담스러우시겠지만 그 무거운 책무를 함께 나누고 싶습니다. 이 자리에 함께 한 성직자, 수도자, 교우 여러분 그리고 한 시대를 함께 살아가는 동지 여러분! 제가 훌륭한 주교직을 수행할 수 있도록 도와주십시오. 아니지요. 제가 여러분을 도와드리겠습니다. 이 주교직은 여러분과 함께 나눌 수 있어야 비로소 완성되는 교회의 시작이며 끝입니다. 저는 오늘 수여받은 이 주교직이 여러분으로부터 비록 되어 여러분으로 완성된다는 것을 결코 잊지 않겠습니다. 감사합니다.}} 경기가 침체하고 있는데도 불구하고 물가가 상승하고 있는 상태를 일반적으로 스태그플레이션(stagflation)이라고 한다. 정체를 의미하는 스태그네이션(stagnation)과 물가상승을 뜻하는 인플레이션(inflation)의 합성어이다. 전통적인 케인스(Keynes) 경제학에 의하면 경제활동이 정체하면 실업률이 높아지고 유효수요도 감퇴하게 되므로 물가는 하락하게 된다는 것이 일반적인 통설이었다. 다시 말해 경기가 좋아지고 실업률이 제로에 가까워지게 되면 임금이 높아지고 물가도 상승하게 된다는 것이다. 이같은 실업률(경기)과 물가상승률의 관계를 설명한 것이 ‘필립스곡선(曲線)’이다.  그런데 1970년대에 들어와 이러한 전통적 경기이론이 들어맞지 않는 이상사태가 일어났다. 미국, 일본, 서구 선진 제국들이 긴축정책을 채택하여 경기는 나빠지고 실업률도 높아졌지만 물가는 하락하기는커녕 상승을 계속하는 사태가 벌어진 것이다. 이같은 경향은 1973년 말의 석유파동 이후 더욱 뚜렷해져 1974년에는 OECD 가맹 주요 7개국의 실질경제성장률이 전년비 평균 ‘마이너스’를 보이는 한편 소비자물가 상승률은 2배로 늘어났다. 주요 선진국이 스태그플레이션, 즉 불황과 물가고에 시달리게 된 원인은 다음의 몇 가지로 알려져 있다.  첫째, 코스트 푸시 인플레이션(cost－push inflation ： 임금을 중심으로 한 기업의 생산비 상승에 의한 인플레이션)의 등장이다. 임금이 상승하면 기업은 그 상승 폭만큼 상품가격에 전가함으써 물가가 상승하는 사태가 발생한 것이다. 임금이 노동의 수급여하에 따라 결정되던 시대에는 경기가 나빠져 실업률이 높아지면 임금도 떨어지므로 기업은 생산비를 인하시킬 수 있었다. 그러나 최근에는 경기가 좋을 때에는 물론 임금이 상승하고, 경기가 나빠져도 임금이 떨어지지 않는(賃金의 下方硬直性) 사태가 벌어진 것이다. 선진국 중에서도 영국이 그 전형적인 예로 되어 있다.  둘째, 재정지출 중 사회보장관계비 등의 이전지출이 경기동향에 관계없이 늘어남으로써 수요초과요인(需要超過要因)으로 작용하고 있다는 사실이다. 선진제국에서는 복지정책을 지향하고 있기 때문에 이전지출은 계속 증가하고 있다. 공공투자 등 투자적 지출은 수요를 증대시키는 한편 공급능력의 증가에도 작용하고 있지만 사회보장관계비 등의 이전지출은 수요의 증가 효과밖에 기대하지 못한다. 즉 재정지출구조가 인플레이션 촉진형이라고 해석하는 견해인 것이다. 이밖에도 대기업에 의한 가격지배, 산업계의 카르텔적인 체질이 현대 인플레이션(inflation)의 원흉이라는 견해도 있다. 또한 에너지 가격이 수급(需給)에 의하여 결정되지 않고 산유국의 정치가격화되어 있는 점, 선진국의 인플레이션을 촉진하는 원인으로 분석되고 있기도 하다.  이상의 논술한 바와 같이 스테그플레이션의 원인은 궁극적으로 현대 인플레이션의 원인을 어떻게 해명하는가에 따라 달라질 것이므로 현재까지는 명확한 분석이나 일관된 이론이 없는 상태이다.  이러한 스테그플레이션이 나타난 이유는 무엇보다도 전후에 있어서는 경기가 하강 국면에 접어들면 항상 정부가 개입하여 전통적 4국면이 그 전 과정을 완료하지 못하는 데 있다. 전후 각국에 있어서는 경기변동의 각 과정에 재정정책, 금융정책, 소득정책 등이 간단없이 적용되고 있는 것이다.  이 경우에 새로운 경기변동의 양상을 더욱 부각시키기 위하여 전통적인 명칭보다는 ① 회복(recoery) 국면, ② 수요견인(需要牽引) 인플레이션 국면, ③ 스테그플레이션 국면, ④ 경기후퇴(recession) 국면으로 나눌 수 있다.  가록축에 경제성장률을, 세로축에 물가상승률을 나타내고 경기변동의 4국면은 4개의 원형으로 표시되며, 그 4국면을 구획하는 선 A와 B가 그려져 있다. 경기회복 국면에서는 성장률은 높으나 물가상승률은 낮은 현상이 나타나고, 수요견인 인플레이션 국면은 성장률과 물가수준이 모두 높은 현상, 스테그플레이션 국면에는 물가상승률은 높고 성장률은 낮은 현상, 경기후퇴 국면은 물가상승과 성장률이 모두 낮은 현상이 나타나는 것으로 되어 있다. 전후의 경기변동은 전통적인 그것과 달라서 스테그플레이션 국면과 경기후퇴 국면에 있어서도 실질생산은 증가하고 있는 것이 그 특징이라 할 수 있다. 비잔티움 내전 (1341년~1347년)\\n요안니스 5세는 즉위 당시 9세로, 어머니 안나 황후, 요안니스 6세, 콘스탄티노폴리스 총대주교(요한 14세)로 구성된 섭정에 의해 인도되었다\\n\\n야심가 알렉시오스 아포카우코스|en|Alexios Apokaukos의 도움을 받은 총대주교는 요안니스 5세의 통치가 요안니스 6세의 야심에 의해 위협받고 있다고 황후를 설득하면서 내분을 촉발시켰다. 1341년 9월, 요안니스 6세가 트라키아에 있는 동안 요한 14세는 자신을 섭정이라고 선언하고 요안니스 6세와 그의 지지자, 가족을 대상으로 공격을 개시했다\\n10월에 안나 황후는 요안니스 6세에게 그의 지휘권들 포기하라고 명령했다. \\n요안니스 6세는 거절했을 뿐만 아니라, 요한 14세로부터 요안니스 5세의 통치를 지키기 위한것으로 알려진 디디모티호에서 스스로 황제라고 선언했다. 요안니스 6세가 황제가 되길 바랬는지 아닌지는 알 수 없지만, 총대주교의 도발적인 행동은 요안니스 6세가 권력을 유지하기 위해 싸우도록 부추기고 내전을 일으켰다.\\n\\n당시 비잔티움 제국은 국경을 방어할 병력이 두 파벌이 분열할 만큼 거의 없었고, 결과적으로 외국인 용병들이 투입되었다. 요안니스 6세는 튀르크인과 세르비아인을 고용했다. 그가 튀르크 용병을 주로 공급받은 곳은 안드로니코스 3세가 세운 명목상의 동맹국인 아이든 베이국에서 왔다. 요안니스 5세의 섭정은 튀르크 용병에게도 의존했다. 그러나 요안니스 6세는 1345년 그의 딸과 결혼한 오스만 베이국의 술탄 오르한 1세로부터 지지를 얻기 시작했다. 1347년까지, 요안니스 6세는 승리를 거두고 콘스탄티노폴리스에 입성했다. 그러나 승전기에 안나 황후와 아들 요안니스 5세(당시 15세)와 합의를 이루게 되었고, 요안니스 6세는 이 관계의 하급이 되겠지만 공동 황제로써 통치하게 된다.  이 어울리지 않는 협력 관계는 오래가지 못할 운명이였다. 블라디미르 레닌은 『무엇을 할 것인가?』에서 사회주의 혁명가와 일반적인 근로대중을 구분하였고, 전자가 후자를 통일적으로 지도해야 한다고 주장했다. 이를 ‘전위당론’이라고 부른다.\\\\n\\\\n그는 공산주의 운동에서 산업 노동자에 기반한 모든 직업 혁명가들은 스스로의 이상에 따라 소부르주아적 편견을 가질 수 있다고 주장했다. 이와 더불에 노동자의 이익을 대변하는 모든 종류의 운동에 참가하는 산업노동자가 주도하는 \\'아래로부터의 사회주의\\'(생디칼리즘과 같은)에 근거한 모든 운동은 실패했다는 점을 들어 노동자 대중의 파편화에 근거한 순수 자발적 혁명은 불가능하다고 보았다. 심지어 교육 수준이 낮은 노동자들은 스스로 자본의 노예 생활을 자처했으며, 당의 지도 없이는 그들이 룸펜프롤레타리아 대열에 설 수 있음을 블라디미르 레닌은 경고했다. 또한, 노동자 대다수가 아직 사회주의와 공산주의를 이해하지 못하기 때문에 단결체가 없는 혁명 행위에서 그들은 공산주의를 멋대로 해석하여, 그런 사상 분열이 혁명 행위의 분열로도 이어질 것이라 믿었다.\\\\n\\\\n후일 레닌은 『공산주의 운동에서 ‘좌익’소아병』에서 전위당론에 반대하는 자들에 대해 다음과 같이 비판하였다. \\\\n\\\\n당 독재인가 아니면 계급독재인가, 노동운동 지도자들의 독재인가 아니면 대중들의 독재인가?”라는 하나의 문제 제기는 이미 도저히 믿을 수 없는 끝없는 사고의 혼란을 증명하고 있다. 이 사람들은 완전히 특별한 무엇인가를 발명하기를 원하고 있으며 현명해지려고 노력하는 가운데 스스로 웃음거리가 되고 있다. 당 원칙과 당 규율의 거부는 반혁명주의자들의 주요 정치 테제이다. 그리고 이것은 부르주아의 이익을 위해 프롤레타리아트를 완전히 무장해제시키는 것과 똑같다. 이것은 바로 내버려두면 필연적으로 어떤 프롤레타리아 혁명운동도 박살내 버릴, 저 소부르주아적 분열과 동요로 귀결되며, 또한 지속성, 통일 및 조직적 행동에 대한 저 소부르주아적 무능으로 귀결된다. 프롤레타리아 독재는 투쟁 속에서 단련된 철의 당 없이, 일정 계급의 모든 정직한 사람들의 신뢰를 누리는 당 없이, 대중의 분위기를 지켜보고 그것에 영향력을 발휘할 수 있는 당 없이, 그와 같은 투쟁을 성공적으로 수행할 수 없다.|블라디미르 레닌, 『공산주의 운동에서 ‘좌익’소아병』에서\\\\n\\\\n블라디미르 레닌의 이런 비판은 당시 러시아 내에서 맹렬한 활동을 하고 있었던 멘셰비키와 아나키스트들 그리고, 서유럽에서 활동하는 좌익공산주의자들에 대한 철저한 비판이었다. 그는 일부 활동성이 높은 노동자, 직업 혁명가, 사회주의 학자들이 전위대를 조직해 다수의 산업 노동자, 농민들을 이끌어야 한다고 주장했다. 이 과정에서 레닌은 산업 노동자, 농민과 직업 혁명가 간의 위계질서와 상명하복 관계를 주장했다. 실제로 볼셰비키는 노동자와 농민으로 이루어진 소수 혁명가가 지휘하는 레닌의 전위대였고, 러시아 제국을 철저히 전복시켰다.\\\\n\\\\n그러나, 전위당론은 주로 아나키스트들과 좌파공산주의자들의 비판 거리가 되었다. 이 전위대 체계론으로 여러 마르크스주의자, 레닌주의 외의 공산주의자들은 레닌주의를 권위주의의 일종일 뿐이라고 비판했다. 아나키스트 FAQ의 저자들은 다음과 같이 언급하기도했다. \"이 순전한 엘리트주의는 사회주의는 노동계급의 자기해방이라는 관점을 전면적으로 위배한다. 더욱이 이것은 권력을 지닌 자의 선의에 대한 맹목적인 믿음에 기반을 둔 유토피아적 발상이다.\" 손 마사요시\\n컴퓨터 프로그램 도매업으로 시작해서 여러 사업분야에 진출해있던 소프트뱅크는 1995년 초창기의 야후!에 2백만 달러를 투자하였고, 2000년 당시 작은 벤처회사에 불과하던 중국 알리바바에 2천만달러를 투자하였다. 또한 2006년 2조엔을 들여, 일본의 3위 이동통신사이자 전전년도 약 1000억엔의 연간순이익을 기록하던 \\'보다폰 재팬\\'(보다폰 인수 이전 이름은 \\'J-Phone\\')을 인수해 \\'소프트뱅크 모바일\\'을 설립하였다. 그리고 2008년 말부터 일본 시장에 아이폰을 독점 공급하고 트위터와 제휴하였다. 이러면서 소프트뱅크는 투자기업으로서의 명성을 쌓았으며 일본에서 스마트폰과 모바일 어플리케이션 전파를 선도하였다는 평가와 함께 큰 주목을 받았다. \\n\\n경쟁사 넥스텔(Nextel)을 인수해 몸집을 불린 미국 3위 이동통신업체 스프린트 지분의 78%를 2013년 216억달러에 인수하였다.  소프트뱅크는 곧바로 미국 4위 이통사이자 독일 도이체텔레콤의 자회사인 T-모바일 USA도 인수해서 스프린트와 합병시키려했으나, FCC가 통신시장 과점우려를 들어 금지시켰다. 스프린트는 소프크뱅크 인수 후 7분기 연속으로 적자를 냈고 가입자수도 떨어졌다. 2015년 T 모바일에게 3위 자리도 빼앗겨 4위로 전락했다. 스프린트 인수와 그 후 부진한 경영실적은 소프트뱅크를 본격적으로 빚더미에 올려놓는 원인 가운데 하나가 되었다. \\n\\n한국의 이커머스 기업 쿠팡에 2015년 1조 1천여억원에 이어 2018년 2조 2천억원을 추가로 투자하였다.\\n\\n2018년 12월 19일에 그룹의 이동통신사업인 소프트뱅크 모바일을 \\'소프트뱅크 주식회사\\'로 도쿄증권거래소에 상장했다. 이는 NTT를 뛰어넘는 일본 역대 최대규모의 기업공개이나 첫날에 공모가보다 약 14% 낮은 종가로 마감했다. \\n\\n2019년들어 투자한 미국 애견 산책기업 왝(Wag), 사무실 공유기업 위워크, 카셰어링 컴퍼니 우버, 중국 금융회사 원커넥트금융기술 등에서 연속으로 투자가 실패해, 3분기에만 무려 7001억엔의 적자를 기록했다. 손 마사요시 회장은 \"시뻘건 엄청난 적자로 3개월에 이정도 적자를 낸건 창업이래 처음이다\"라고 하였다.  \\n\\n2020년 2월 소프트뱅크그룹은 스프린트와 T 모바일 USA의 합병안에 동의했다. 스프린트 11주 대 T 모바일 1주의 비율로 교환되는 계약이다. 다시말해 T 모바일이 스프린트를 흡수한다. 합병이 끝난 후 소프트뱅크가 획득하게 되는 T 모바일의 지분은 24%이며, T 모바일 모회사인 도이체 텔레콤의 몫은 43%이다. \\n\\n2020년 3월 손 마사요시 회장은 소프트뱅크그룹이 보유한 자산 중 스프린트, 알리바바 주식 등 약 4조 5천억엔을 매각해서 자사주를 매입하고 부채를 줄이겠다고 발표했다. 소프트뱅크 그룹의 부채는 약 1400억달러이다. 그러나 코로나-19의 세계적 유행으로 인해서 그간 27조엔으로 평가되어오던 보유자산 가치가 급락했다. 따라서 이 자산가치 위에 쌓아올려진 소프트뱅크그룹이라는 투자회사 전체가 위기에 빠진 것이며, 막대한 부채를 동원해 저돌적인 세계경영을 하는 손 회장의 경영방식도 도마 위에 올랐다.  같은 달 소프트뱅크의 투자를 받은 미국의 위성통신기업 원웹(One Web)이 파산하였다. 독일은 자국 대표팀이 월드컵에서 좋은 성적을 거둔 경우가 많아 자국 대표팀에 자부심을 가진 국민들이 많았는데, 이 패배로 인하여 많은 국민들의 실망을 받았다. 독일의 대표적인 신문인 빌트는 1면에 “Ohne Worte!|오네 보르테!”(할 말이 없다!)는 제목으로 4년 전 브라질과의 경기 당시 환호하는 토니 크로스의 모습과 이 날 경기에서 침울해 하는 토니 크로스의 모습을 대조적으로 실었다. 미네이랑의 비극 당시의 \"할 말이 없다.\"는 월드컵 최다 우승국이자 그 당시 홈팀이었던 브라질을 7 – 1이라는 압도적인 점수 차로 격파하자 독일 대표팀의 높은 기량에 찬사를 보내는 뜻으로 쓴 표현이었고 오늘 경기에서의 의미는 피파 랭킹 57위 팀이자 조 최약체로 꼽혔던 대한민국한테 0 – 2로 어이없게 패배하자 충격을 받아서 할 말이 없다는 표현을 쓴 것이다.  독일에서는 이 경기를 \\'Die Schande Von Kasan\\' 즉, 카잔의 치욕이라고 명명하였다. 그만큼 독일 축구계는 이 경기에 매우 큰 충격을 받았음을 알 수 있다. 심지어는 독일에서 지워버리고 싶은 존재인 아돌프 히틀러까지 언급을 해 가면서 분노했다고 한다. \"그 히틀러 조차도 러시아에서 너희들보다는 더 잘했었다.\"며 분노를 표출했다.\\n\\n \\n경기 전만 해도 빌트는 신태용 감독을 향해 \"패션, 헤어스타일 같이 뢰프의 겉모습만 따라한다.\"고 비웃으며 가짜 뢰프라고 조롱하기까지 하였으나  정작 그렇게 자신들이 비웃었던 가짜 뢰프 신태용이 독일을 2 – 0으로 꺾는 이변을 일으키자 곧바로 \"가짜 뢰프가 이겼다.\"며 자국 대표팀을 강도 높게 비난하였다.  덧붙여서 \"월드컵 사상 최고의 불명예\"라는 혹평까지 하였다.  경기 직후엔 독일의 주전 공격형 미드필더 메수트 외질과 일부 팬들 사이에 충돌이 있었다고 한다.  대회 전부터 터키의 에르도안 대통령과의 사진 촬영 등으로 물의를 빚어 안 그래도 독일 축구팬들에게 단단히 찍힌 상태였는데 오늘 경기에서도 부진한 모습을 보이자 독일 축구팬들로부터 심한 질책을 들었고 그에 격분해 충돌까지 일어난 것이다. 일부 독일 팬들은 이 날 경기에 욕설까지 하면서 비난하였고 특히 메수트 외질을 오늘 경기의 워스트 플레이어로 꼽았다. \\n\\n경기 전 날 기자회견 때만 하더라도 아니 경기 당일 전반전 때만 하더라도 여유롭게 경기를 관전하던 독일 선수들은 한국의 골문이 좀처럼 열리지 않자 시간이 갈수록 초조한 모습을 보이더니 결국 0 – 2 패배로 끝나자 매우 큰 충격을 받은 듯한 모습을 보였다. 2010년, 2014년 월드컵에서 각각 5골씩 넣으며 단 2개 대회만에 10골이나 넣었던 토마스 뮐러는 경기가 끝나자 눈물을 보였고 웬만해선 눈물을 보이지 않던 마누엘 노이어까지도 눈물을 보였다. 마르코 로이스는 경기 종료 휘슬이 울리기 직전에 경기가 잘 풀리지 않자 굳은 표정을 짓는 모습이 화면에 비쳤고 경기가 끝난 후에도 한참을 락커룸으로 들어가지 못하고 벤치에 멍하니 앉아 있었다. 마리오 고메스 또한 그라운드에 털썩 주저앉아 혼이 빠진 듯한 모습을 보였고 메수트 외질 역시 혼이 빠진 듯한 모습으로 벤치에 주저앉았다. 벤치에서 대기하고 있던 일카이 귄도안과 율리안 드락슬러 등도 처음엔 여유롭게 경기를 지켜보다가 좀처럼 골이 터지지 않자 표정이 점점 굳어지더니 김영권에게 선취점을 실점하고는 곧바로 머리를 감싸쥐고 괴로워하는 표정을 지었다.\\n\\n \\n이 경기의 패장인 요아힘 뢰프 감독은 \"대한민국의 경기력은 정말 훌륭하였고 패배는 쇼크다.\" 고 경기 소감을 밝혔다. 축구 열성팬으로 유명한 독일의 앙겔라 메르켈 총리도 트위터로 자국 팀의 16강 좌절에 대해 매우 슬프다는 반응을 보였다.  한편, 독일의 주전 수비형 미드필더 사미 케디라는 대한민국과의 경기에서 패배한 이후 독일로 귀국한 뒤에도 잠을 제대로 못 자고 있으며 자다가도 벌떡벌떡 깬다고 인터뷰에서 밝히며 여전히 이 경기에 대해 심리적으로 큰 충격을 받았음을 알 수 있게 하였다.  또 대회 전부터 구설수에 올랐던 메수트 외질과 일카이 귄도안은 국가대표팀에서 은퇴하겠다고 선언하였다.  이 날 카잔 아레나 관중석엔 월드컵 최다 득점자인 독일 축구의 영웅 미로슬라프 클로제와 2차전 스웨덴과의 경기에서 퇴장당해 출전 정지된 제롬 보아텡이 관전하였다. 경기를 직관한 클로제는 경기 내내 대한민국의 골문을 열지 못한 독일 대표팀의 무기력한 모습을 굳은 표정으로 바라보았고 결국 독일이 0 – 2로 참패를 당해 조별리그에서 탈락하는 광경을 목격하게 되었다. 옆에 앉은 보아텡은 후반 중반 이후부터는 아예 경기는 안 보고 스마트폰만 만지작거렸다.\\n\\n전 독일 대표팀 감독이었던 베르티 포크츠는 왜 독일이 위르겐 클린스만이나 울리 슈틸리케와 접촉해서 멕시코와 대한민국에 대한 정보를 구할 생각을 하지 않았는지 모르겠다고 하며 준비를 소홀히 한 자국 대표팀을 강도 높게 비판하였다. 그러면서도 뢰프 감독에게 오늘의 불명예를 씻을 수 있는 기회를 한 번 더 주어야 한다며 뢰프 감독 유임을 지지하였다. 반면, 독일 축구팬들은 이 날 경기로 뢰프 감독에 대한 신뢰를 버렸으며 여론조사 결과 응답자의 75%가 뢰프 감독의 유임을 반대한다고 목소리를 높였다.  한편, 대한민국은 3전 전패할 것이라는 걸 받아들여야 한다며 대한민국 축구에 악담을 퍼부었던 울리 슈틸리케는 이 경기가 끝나자 입을 삭 닦고 침묵을 고수하였다. 이에 대한민국 축구팬들 사이에선 슈틸리케에게 이 경기에 대해 어떻게 생각하냐고 조롱하는 투로 질문을 남겼다.\\n\\n그리고 7월 23일, 레제프 타이이프 에르도안 터키 대통령과 사진 촬영 사건으로 인해 구설수에 올랐던 대표팀 주전 공격형 미드필더 메수트 외질이 독일 축구 국가대표팀 은퇴를 선언하였다. 메수트 외질은 자신의 대표팀 은퇴 결정에 대한 심경을 소셜 미디어에 남겼는데 이에 대한 사회적 파장이 매우 크다고 한다. 메수트 외질은 자신의 소셜 미디어에서 \"나는 독일에서 태어났지만 내 뿌리는 터키다. 나는 독일인의 심장과 터키인의 심장을 동시에 가지고 있다. 어머니는 내 뿌리를 절대 잊지 말라고 가르치셨다.\"고 항변하며 에르도안 대통령과의 만남은 자신이 독일인인 동시에 또 터키인이기도 하므로 터키인인 자신이 자국 대통령과 만난 게 무엇이 큰 잘못이냐고 항의하였다. 자신의 직업은 축구 선수이지 정치인이 아니라고 하며 에르도안과의 만남은 전혀 정치적 의도가 없었다고 외쳤다. 그리고 에르도안이 독재자인데 독재자와 만나 악수하고 사진을 찍었다고 비난하는 이들을 향해서는 \"로타어 마테우스가 블라디미르 푸틴과 악수하는 것은 괜찮고 내가 에르도안과 악수하고 사진 찍는 것은 문제가 되느냐?\"고 외쳤다. 그리고 폴란드계 독일인으로서 대표팀에서 활약한 미로슬라프 클로제와 루카스 포돌스키에게는 \\'폴란드계 독일인\\'이란 사실을 부각시키지 않으면서 자신은 늘 \\'터키계 독일인\\'이란 사실을 부각시켰고 \"독일인들은 내가 플레이를 잘 할 때는 독일인이라 하고 내가 실수하는 날에는 터키인이라 하였다.\"며 독일 축구계의 제노포비아에 대해 격렬하게 항의하였다. \\n\\n이러한 외질의 대표팀 은퇴 선언은 그간 독일 사회에 만연해 있던 이민자 차별 논란에 불을 지폈다. 독일 통신사 DPA는 \"외질의 은퇴 선언 이후 소셜미디어(SNS)에는 인종차별에 항의하는 \\'미투 해시태그(#MeToo)\\'가 번지고 있다.\"며 \"독일에 거주하는 이민자 또는 그 후손이 인종차별 경험을 올리면서 미투 해시태그를 단 사례가 나흘간 6만 건을 넘었다.\"고 29일 보도하였다. 즉, 한 축구 선수가 팀을 떠나며 남긴 항의 한 마디가 독일 사회 전체를 뒤흔들어버린 것이다. 이번 논란이 9월에 실시될 UEFA 유로 2024 개최지 선정 투표에서 독일에 불리하게 작용할 거라는 전망도 나왔다. DPA는 \"공교롭게도 유로2024 개최를 놓고 독일과 터키가 경쟁 중인데 독일이 내세운 \\'축구로 하나 되다(#UnitedbyFootball)\\'라는 슬로건이 외질 논란 탓에 머쓱해졌다.\"고 보도하였다. 이어 \"논란 이전까지도 독일이 우세하다는 분석이었지만, 최근 동유럽 국가 다수가 대회 유치 5수생인 터키 지지로 돌아선 것 같다.\"고 덧붙였다. 이런 분위기를 읽은 터키축구협회는 지난 25일 \"국제 축구계 구성원 모두가 \\'하나 되어(united)\\' 인종차별과 무관용을 척결해야 한다.\"고 성명을 발표하였다. \\'축구로 하나가 되자\\'고 외치면서 인종에 따라 차별의 선을 긋는 독일의 이중적 모습을 제대로 지적하였다는 분석이다. 돈 디에고 데 카스트로 티투 쿠시 유팡키(Don Diego de Castro Titu Cusi Yupanqui)는 신잉카국의 왕이자 망코 잉카 유팡키의 아들로 태어났다. 1563년에 선왕이자 형제였던 사이리 투팍의 뒤를 이어 즉위했고, 1571년에 폐렴으로 사망할 때까지 왕국을 다스렸다.\\n\\n그가 반란을 일으켜 빌카밤바에서 신잉카국을 다스릴 동안, 스페인의 총독과 관리들은 최대한 협상을 통해 그를 복속시키고 싶어했다. 협상의 주 내용은 그가 빌카밤바에서 내려오고 왕위를 내려놓는 것에 대한 것이었다. 1568년 즈음에 협상이 타결된 이후, 티투 쿠시 유팡키는 가톨릭으로 개종하여 \\'디에고 데 카스트로\\'라는 이름을 얻게 된다. \\n\\n티투 쿠시 유팡키는 투팍 아마루를 잉카 제국의 황제였던 망코 잉카의 시체를 운구하는 대사제로 임명했다.\\n\\n투팍 아마루는 1571년에 티투 쿠시가 급사한 이후 잉카인들의 지도자 자리에 오르는데, 이때 티투 쿠시 옆에서 시중을 들었던 사람 2명이 스페인 관리의 사주를 받아 그를 독살한 것으로 의심받았고, 결국 2명 다 죽게 된다. \\n\\n그는 스페인인들이 작성한 기록의 번역자, 즉 기록에 자세한 내용을 덧붙여넣는 일을 했으며 선교사들의 도움을 받아 1570년 즈음에 An Inca Account of the Conquest of Peru작성하여 발표하였다. 이 책에는 스페인의 잉카 정복에 대한 다양한 시각을 보여 주고 있는데, 특히 잉카 원주민들의 관점으로 본 스페인의 도래에 대해 설명하고 있어 가치가 높다. 그는 잉카 제국이 처음으로 미지의 스페인 군인들을 조우했을 때 어떤 느낌이었는지, 어떤 생각으로 그들을 대했는지에 대하여 나와있다. 아래에 나온 부분은 망코 잉카가 전령에게서 바다 건너에서 새로운 사람들이 나타났다는 소식을 전해들었을 당시의 내용이다.\\n\\n\"나의 아버지가 이 소식을 들으셨을 때, 아버지는 크게 격노하셨다. 그 분은 \"어떻게 이방인이 감히 자신의 허락을 받지 않고 내 영토에 들어올 수 있느냐? 대체 그들은 누구이며 무엇을 위해 온 것이냐?\"라고 말씀하셨고, 전령은 이렇게 말했다. \"폐하, 그 사람들은 신이 틀림없습니다. 그들은 자신들이 바람을 타고 왔다고 말하며, 수염을 길렀으며 매우 아름답고 하얀 피부를 갖고 있습니다. 그들은 은으로 만든 옷을 입고 있으며, 심지어 그들이 타고 온 짐승들까지도 은으로 만든 옷에 신발을 신고 있습니다. 또한 그들은 천둥을 갖고 있어 원할 때마다 그 힘을 발휘합니다. 게다가 저희는 저희 눈으로 직접 그자들이 두꺼운 사각형 모양의 하얀 옷뭉치에다 대고 기도를 하는 것을 보았습니다. 그들은 이 얇디 얇은 가죽 뭉치에 대고 기도를 하며, 이 것 없이는 그 무엇도 이루어질 수 없다고 말합니다.... 그러니 폐하, 이들이 대체 신이 아니고 그 무엇일 수 있단 말입니까?\" 운산광산은 오래전부터 알려져 왔으나 본격적으로 개발되기 시작한 것은 구한말의 일이다. 1884년 미국 공사관의 조지 C. 포크(George C. Foulk)와 스미소니언 박물관의 J.B. 버나도우(J.B.Bernadou)가 전국 각지의 유망 금광상을 탐사하여 운산금광에 대한 보고서를 내기 시작한 것이 근대적인 개발의 시발점이다. 이후 호러스 뉴턴 알렌의 주선으로 조선 정부는 미국 실업가 J.R. 모스(J.R.Morse)에게 1895년 운산금광 채굴특허를 주었다. 그러나 모스가 자본 부족으로 개발에 소극적 태도를 보이자 알렌은 레이 S. J. 헌트|en|Leigh S. J. Hunt를 끌어들여 운산금광 개발권을 인수하도록 하고 헌트는 1897년 동양광업개발주식회사(東洋鑛業開發株式會社, Oriental Consolidated Mining Company)를 설립하였다 . 이후 동양광업개발주식회사는 40여년간 900만톤의 광석을 채굴해 5600만 달러의 금을 생산하였고 1500만 달러의 순이익을 올렸다. 이후 일미관계가 악화되면서 1939년 대유동금광을 운영하던 일본광업주식회사에 800만 달러 가격으로 인수되었다. 전후 1958년에는 처리능력 4만~9만톤의 선광조업이 시작되였으며 1986년에는 조총련 계열 상공인들이 투자를 하기도 하였으며 1995년에는 미국 모빌사와 모리슨 앤드커누슨 사가 탐사하기도 하였다. 2004년에는 중국 자오진 그룹이 공동개발을 추진하기도 하였다 . 프랑스의 샤를 8세가 1498년에 사망한후 왕위를 계승한 루이 12세는 밀라노의 계승권을 주장하며 1499년 롬바르디아를 침공해왔다. 루이 12세는 루도비코가 권력을 찬탈했다고 주장했다. 1494년에 밀라노 공작이였던 잔 갈레아초가 죽자 그의 숙부인 루도비코가 독살했을 것이라는 소문이 파다했었기 때문이다. 또한 비스콘티 가문의 마지막 공작인 필리포가 1447년 후계없이 사망하자 프란체스코 스포르차가 사위의 자격으로 통치권을 계승하였다. 그러나 필리포의 유일한 후손인 비앙카(1425~1468)는 사생아였다. 그래서 계승에 정통성이 없다고 보았다. 이에 반해 루이 12세는 할머니 발렌티나(1371~1408)를 통해 혈통을 물려받았으므로 정통성은 자신에게 있다고 주장하였다.\\n\\n루이 12세는 미리 신성로마제국, 영국, 스페인 등과 우호적 조약을 맺은후 1499년 7월 27000명의 군대(1만명의 기병과 5천명의 스위의 용병 포함)를 이끌고 알프스를 넘어 왔다. 사태가 심각하다고 판단한 루도비코는 우선 자신의 두아들(마시밀리아노,프란체스코)를 합스부르크 왕궁이 있는 인스부르크(Innsbruck)로 망명보냈다. 1500년, 루도비코의 기지가 있는 노바라가 프랑스 군에 의해 포위되었다. 스위스 용병을 포함한 양측 군대가 대치하게 되었는데 자국인끼리 싸우고 싶지 않았던 스위스 용병들은 노바라를 내버려 두고 돌아가 버렸다. 1500년 4월, 루도비코는 스위스인으로 가장하고 탈출을 시도하다가 프랑스 군에 붙잡히고 말았다. 10월 6일 밀라노는 농성전 끝에 프랑스군에게 정복 당했다. 이 인터뷰는 2019년 12월 11일 육군사관학교 군사사학과 나종남 교수와 진행하였다.\\n\\n1. 내촌 태릉 전투의 목적이 무엇인가? 단순히 북한군의 진격을 지연시키기 위한 전투인가?\\n\\n목적이 있기보다 이준식 육군사관학교 준장이 뭔가를 하라고 육군본부로부터 지시를 받았다.(일본 육군사관학교 59기 장창국 대령의 자서전에 그렇게 기록되어있다.) 그는 6.25전쟁이 발발했을 때 지휘관이었다. 그는 동원할 수 있는 모든 병력을 전투에 내보내 서울로 들어오는 핵심길목인 내촌을 지키라고 지시받았다. 당장 서울이 함락되는 긴급한 상황이었기 때문에 목적을 가졌다고 말하긴 힘들다.\\n\\n그는 명령에 의해 지시했다. 개인적인 생각으로 여기에 문제가 있다고 생각하지 않는다. 하지만 평가는 이루어져야한다고 생각한다. 과연 당신이라면 어땠을지 생각해보라. 딜레마일 것이다. 좁게 생각하지 말아야한다. 그의 판단을 존중할 필요가 있다.\\n\\n2. 왜 전쟁이 나면 생도들을 전투에 내보내는 것을 꺼려하는가? 내촌-태릉 전투에서도 작전국장 장창국 대령이 과거 태평양 전쟁 말기에 패전을 눈앞에 둔 일제가 사관생도를 전장에 내보내지 않은 사례를 거론하며 반대하지 않았는가?\\n\\n그렇지 않다. 일제도 정규사관생도를 제외한 다른 생도들을 전장에 보냈다. 그들은 사관생도의 남은 교육시간을 3,4개월로 단축시켜 전쟁에 내보내었다.\\n\\n1861년부터 1865까지 일어난 미국 남북전쟁에서도 마찬가지이다. 남군은 사관생도를 \"Bullrun 전투\"에 내보내었다. 당시 남군 쪽에는 6개의 사관학교가 존재했고 버지니아 군사대학(VMI), 시타델(Citadel) 사관학교는 사관생도를 전장에 내보내 승리하였다.\\n \\n3. 당시 내촌-태릉 전투에서 1인당 고작 M1 소총 한정과 탄약 50여발을 지급했다고 하는데 왜 당시에 탄약이 부족했는가?\\n\\n단순하다. 학교가 탄약을 충분히 가지고 있지 않았다. M1소총도 옛날 일본군이 쓰던 무기를 미군부대를 통해 얻은 것이었다. 소총도 병력의 수만큼 있지 않았다. 당시 육군사관학교엔 근무지원단과 생도들이 봉일천 전투로 파견되었다. 1000명의 병력이었지만 소총은 300여 정밖에 없었다. 3인당 1정 꼴로 지급이 되었다.\\n\\n4. 교수님이 작성한 「6.25전쟁 초기 육사 생도 참전전투 연구」 논문을 확인해보면 370고지가 등장하는데 어느 지역인가?\\n\\n내촌지역이다. ‘370산’이라고도 불린다.\\n\\n5. 내촌-태릉 전투의 의의와 이 전투가 다음 전투에 미친 영향이 무엇인가?\\n\\n이 전쟁의 중요성을 물어보는 질문인 것 같다. 여러 관점이 있지만 일반적인 시각으로 본다고 하면 당시 국가는 비상상황이었고 국가의 수도가 무너지는 위급한 상황이었다. 군인들과 청년들이 목숨을 바쳐 싸우는 데 사관생도라고 예외는 아닐 것이다. 당신이 제복을 입은 사람이라면 싸우지 않을 이유가 없다. 매우 자연스러운 것이다.\\n\\n내촌-태릉 전투에서 육군사관학교 1,2기 생도들은 멋진 활약을 했다. 그들은 북한군의 진격을 반나절 지연시켰다. 갑작스럽게 일어난 전쟁이라 시간을 버는 것이 중요했다. 북한군의 진격이 느려졌기 때문에 다른 부대들이 그만큼 후퇴하거나 전쟁태세를 갖추는 시간을 주었다. 그 시간동안 다른 부대들이 많은 ‘무언가’를 할 수 있었을 것이다.\\n\\n6. 태릉전투에서 많은 생도들이 전사했다. 그 이유가 통신 문제라고 하는데 왜 그런 일이 발생했는가?\\n\\n맞다. 태릉전투 이후 후퇴과정에서 많은 문제가 있었다. 당시 육군사관학교 교장 이준식 준장이 생도들에게 명령을 전달했지만 통신에 문제가 있어 많은 생도들은 후퇴하지 않았다. 그들은 대기했고 많은 수가 북한군에게 죽고 생포 당했다. 안타까운 일이다. 이는 잘못된 예시이다. 역사를 통해 배워 앞으로 이런 일이 발생하지 않도록 해야한다. SK에서 방출된 이후 그는 일본을 여행하며 잠시 신변을 정리하고 있었다. 그 때 좌완 선발 투수가 필요하여 니코스키에게 관심을 두고 있었던 두산 베어스가 SK 와이번스에 계약 양도를 신청하여 곧바로 맷 왓슨의 대체 용병으로 두산 베어스로 이적했고 두산 베어스의 선발진에 합류하여 비로소 첫 승을 따 냈으며  승리를 대부분 한화 이글스전에서 기록하여 한화 이글스에는 강력한 천적으로 자리매김했다.  그러나 예전부터 주로 원포인트 및 중간계투로 뛰다가 선발로 전환한 탓에 선발로는 아직 익숙하지 않아 3회까지 투구수가 70개 가까이 되는 경기가 대부분이라는 단점이 있었다.  2009년 9월 13일 KIA 타이거즈전에서 비로소 초반에 급격하게 늘어나는 투구수를 극복하여 승리를 따 내기도 했다.  두산 베어스 합류 후 12경기에 더 나서고 2009년 정규 시즌을 4승 8패, 평균 자책 3.78로 마무리했다. 완전한 선발 전업 후 후반기에 조금씩 나아지는 모습을 보였으나, 잠실야구장에서 롯데 자이언츠와 맞붙었던 2009년 9월 29일 준 플레이오프 1차전에서 무실점으로 3이닝을 호투하던 중 갑작스러운 어깨 통증으로 조기 강판당했고, 그 날 두산 베어스는 롯데 자이언츠에게 1차전을 내주었다. 진단 결과 극상근 손상 판정을 받아 포스트 시즌 전력에서 완전히 이탈하고 말았다. 플레이오프가 끝난 이후 이닝 소화 능력이 떨어진다는 이유로 두산 베어스가 재계약을 포기하면서 대한민국을 떠나게 되었다.  두산에서의 마지막이 좋지 않았지만, 그가 두산에 입단하여 2군에서 불펜 피칭을 했을 때 그를 지켜보았던 박종훈 당시 2군 감독은 니코스키를 두고 성격이 좋았다고 하였다. 일본 시리즈의 인터넷 전송 도입은 2016년에 개국한 Abema TV가 처음이다. 그 해에는 출자회사인 TV 아사히가 중계하는 경기에서 독자적인 실황·해설을 붙이는 방식으로 동시 전달을 실시했다. 그 외의 키국이 출자회사가 되고 있는 인터넷 전송업자에 있어서도 2018년부터 훌루(닛폰 TV), Paravi(TBS·TV 도쿄), 후지 TV ONEsmart(후지 TV)에서 동시 전송을 하게 됐고 2018년엔 모든 경기를 인터넷에 전송되는 첫 사례가 됐다. 더욱이 이 이외의 전달 업자(DAZN 등)에서의 중계는 현 시점에서 이뤄지지 않았다. 이들 스위스 용병들은 동성애자 사제들에게 성추행에 시달리기도 한다. 바티칸의 경비를 맡은 스위스 근위대 출신의 한 용병은 근무 당시 추기경, 주교, 신부는 물론 고위 성직자로부터 동성애를 요구받았다고 폭로하기도 했다. 이 용병은 당시 교황 요한 바오로 2세의 측근인 고위 인사를 포함해 성직자들로부터 20차례 이상 명확한 요구를 받았다는 사실을 폭로했다고 스위스 신문인 슈바이츠 암 존탁이 2014년 1월 6일(현지시간) 보도했다. 지난 1998년 한 젊은 용병이 동성애 관계를 맺어온 것으로 알려진 근위대 대장과 그 부인을 총으로 살해하면서 논란이 된 적이 있다. \\\\n\\\\n이들 스위스 용병과 가톨릭 사제간 동성애는 프란치스코 교황의 측근도 연루되어 파문을 일으키기도 했다. 이탈리아 주간지 레스프레소(L\\'Espresso)는 2013년 7월 19일 바티칸은행 개혁 담당 고위성직자인 바티스타 마리오 살바토레 리카 몬시뇰이 예전 우루과이 주재 대사로 재직하다 동성애 행적이 적발돼 본국으로 강제 소환됐다고 폭로했다. 리카 몬시뇰은 대사 시절인 1999년 한 스위스 용병에게 돈과 숙소를 제공하며 교제했고 게이들의 사교장소를 전전하다 폭행까지 당한 적이 있다고 잡지는 전했다. 이 잡지는 리카 몬시뇰이 강제 소환 이후에도 아무 일이 없던 것처럼 바티칸 고위직을 두루 역임해 바티칸내에 동성애자 고위성직자의 비밀 조직인 \\'게이 로비\\'의 비호가 의심된다고 지적했다. 프란치스코 교황도 \"교황청 내에 부패가 있다\"며 게이 로비의 존재 사실을 인정한 바 있다. \\\\n\\\\n한편 존스홉킨스대학 심리치료사 리처드 사이프의 연구에 따르면 미국 가톨릭 교회 사제들의 1/4이 동성애를 하고 있고 1/10이 소아성애증을 갖고 있다고 한다. 〈Yuuhi wo Miteiruka? - Apakah Kau Melihat Mentari Senja? -|유히 오 미테이루카? - 아파카 카우 믈리핫 믄타리 센자-〉는 AKB48의 자매 그룹인 JKT48의 두 번째 싱글로 2013년 7월 3일에 Hits Records에서 발매됐다. 노래의 센터 포지션은 멜로디 누람다니 라크사니와 제시카 베란다이다.\\n\\n싱글판은 일반과 극장판이 있다. 일반판에는 DVD가 들어있는 것 외에 특전으로 10명 중 랜덤으로 1장의 멤버 생사진 및 팀 KIII 스페셜 포토가 들어있다. 후자는 초도 발매분 10,000장에만 들어있는 한정 아아템이다. 극장판은 CD뿐이며 특전으로 개별 악수권 1장 및 51명 중에서 랜덤으로 1장의 트럼프 카드가 들어있다. 또, 일반판에 대해서는 인도네시아에서의 발매 후에 일본용 특별판이 발매되게 됐으며 특전으로 10명 중 랜덤으로 1장의 선발 멤버 생사진이 추가되며 타카죠 아키, 나카가와 하루카의 생사진 중 하나가 봉입된다. 리그 1(Ligue 1|리그 욍, 파리 방언 ‘리그 앙’)은 프랑스의 1부 축구 리그이다.(2002년까지는 디비지옹 1으로 불림) 리그 2(Ligue 2)와 함께 프랑스 프로축구 리그를 구성한다. 1932년에 시작한 이후 지금까지 계속 프로 리그 였으나, 1943/44 시즌에는 비시 정권이 프로스포츠를 금지하여 잠시 프로 리그가 아니었다. 제2차 세계 대전 이후로는 리그 소속팀의 숫자가 18개일 때와 20개일 때가 번갈아 있었으나 지금은 선수들의 부담에도 불구하고 클럽들의 주장에 의해 20개 팀으로 운영되고 있으며 현재는 프랑스 가전 및 가구회사인 콩포라마가 리그 1의 타이틀 스폰서를 맡고 있다.\\n\\n20개의 리그 1 팀들은 시즌 중에 각 팀들과 홈과 원정에서 두 번씩 경기를 가져 총 38경기를 한다. 시즌이 끝나면, 하위 세 팀은 리그 2로 강등된다. 이러한 승강 제도는 1995년에 도입이 되었다. 이전까지는 리그 1의 19, 20위 팀은 바로 강등을 당하고, 리그 1의 18위 팀과 리그 2의 1위 팀간의 플레이오프를 통해 강등되는 팀이 정해졌다.\\n\\n현재 상위 세 팀은 UEFA 챔피언스리그에 참가자격을 얻으며, 상위 두 팀은 조별예선으로 직행하고 3위 팀은 3차예선 부터 단계를 거친다. 4위 팀은 유로파리그에 참가할 수 있으며, 5위와 6위의 팀들도 국내 컵대회의 성적에 따라 유럽대회에 참가할 수 있다.\\n\\n점수 제도는 국제적인 기준인 승리 3점, 무승부 1점, 패배 0점을 따른다. 이 제도는 1988/89 시즌에 처음으로 시범 운영된 뒤 1994년에 도입이 되었다. 1973년부터 1976년까지는 좀 더 활발한 경기를 위하여 승패 여부와 관계없이 한 경기에서 3골 이상을 넣은 팀에게 추가로 1점을 주는 방안이 도입이 되었었다. 그 효과에 대해서는 아직까지 의견이 분분하다.\\n\\n리그 1에서 승점이 동점일 경우, 골득실로 순위를 결정한다. 만약 같을 경우 다득점으로 순위를 결정한다. 1966년까지는 골득실 대신 득점대 실점의 비율이 골득실을 대신하였었다. 이 제도는 공격적인 팀들보다 수비적인 팀들에 유리했다. 1961/62시즌에는 RC 파리가 86득점 63실점을 하여 83득점 60실점을 한 스타드 드 랭스에게 골비율의 0.018의 차이로 우승을 빼앗긴 적이 있었다. 개최 일정 및 개최 장소가 변칙적인 형태가 된 사례는 아래와 같다.\\n\\n* 1950년에는 개최 장소를 매 경기마다 변경했는데 개최 구장은 메이지 진구 야구장(1차전), 고라쿠엔 구장(2차전), 한신 고시엔 구장(3차전), 한큐 니시노미야 구장(4차전), 주니치 구장(5차전), 오사카 구장(6차전) 순으로 열렸다. 이 해에는 4승 2패로 마이니치 오리온스가 일본 시리즈 초대 우승 팀이 됐지만 6차전에서 쇼치쿠 로빈스가 이겨 3승 3패가 됐을 때 7차전은 고라쿠엔 구장에서 열릴 예정이었다(연속해서 경기를 치렀는지 이동일을 끼었는지 상세한 것은 알려지지 않았음).\\n* 1953년에는 4차전까지 평상시대로 진행됐지만 5차전부터 7차전은 오사카 구장, 한신 고시엔 구장 , 고라쿠엔 구장 순으로 열렸는데 전년도부터 프랜차이즈 제도가 시행 됐다. 이것은 당시의 규정에 ‘1, 3, 5, 7경기와 2, 4, 6경기의 사용 구장은 매년 양대 리그가 교대로 지정한다. 다만 1, 2경기와 3, 4경기, 5, 6경기의 사용 구장은 각각 연속하여 동일 지역에 있는 구장을 지정한다’라고 돼있었기 때문이다. 그 해의 짝수 경기의 구장 지정권은 센트럴 리그에 있으며 ‘오사카보다도 수용 능력이 큰 고시엔이라면 엄청나게 벌 수 있다’라고 예상하고 있었는데 그 예상은 보기좋게 빗나가고 입장자 수는 6,346명이었다. 또 그 해에는 미일 야구가 두 경기로 구성된  영향도 있어서 본래 만들어질 이동·휴양일이 없어 두 경기마다 경기 당일에 이동한 강행군이었다.\\n* 1962년, 도에이 주최에 의한 5차전과 1978년 야쿠르트 주최의 4경기 모두 메이지 진구 야구장의 학생 야구 개최라는 사정으로 고라쿠엔 구장에서 대신 개최됐다.\\n* 1974년 롯데 주최의 3차전 ~ 5차전은 시설상의 문제로 현영 미야기 구장이 아닌 고라쿠엔 구장을 사용하였다. \\n* 1979년, 1980년의 긴테쓰가 주최하는 모든 경기는 닛폰 생명 구장의 수용 인원 수가 일본 시리즈 개최 기준인 3만 명에 달하지 않았고  마찬가지로 긴테쓰가 보유한 후지이데라 구장도 야간 시설이 설치돼 있지 않았다는 이유로 당시 난카이의 홈구장이었던 오사카 구장에서 대신 개최됐다.\\n* 1981년에는 양대 리그의 출전 팀이 연고지가 모두 고라쿠엔 구장인 요미우리와 닛폰햄이었기 때문에 6경기 모두 같은 구장에서 개최되면서 일명 ‘고라쿠엔 시리즈’(後楽園シリーズ)라고 불렸다.\\n* 1986년의 1차전은 무승부로 시작됐는데 히로시마가 3연승을 올리면서 승패가 결정난 듯 보였다. 하지만 이어서 세이부도 3연승하는 바람에 7차전 종료 시점에서 양팀 전적이 3승 1무 3패의 상황이 됐는데 급기야 7차전이 치러졌던 히로시마 시민 구장에서 처음으로 8차전 이후의 경기를 펼쳐 승패를 결정짓기로 했다. 결과는 8차전에서 세이부가 승리하여 승패는 마무리 지어졌는데 당시 룰에 따르면 8차전 이후로도 횟수 무제한이 아니라서 만일 무승부였더라면 9차전 이후의 경기도 계속 치렀어야 했다.\\n* 2000년에는 요미우리와 다이에가 맞붙게 됐지만 3년 전인 1997년에 대규모 국제 학술 대회 장소를 물색 중이던 일본 뇌신경외과학회의 장소 대여 의뢰를 받아 후쿠오카 돔측이 구단의 허가도 없이 일본 시리즈의 일정과 겹치는 2000년 10월 24일부터 27일까지 구장을 빌려주기로 결정하였다. 이것은 1997년 당시 다이에는 난카이 시절부터 계속된 20년 연속으로 B클래스에 머물고 있었기 때문에 리그 우승 가능성이 희박하다고 내다보면서 이 같은 결정을 내리게 된 것이다. 그러나 이듬해인 1998년에 연고지를 후쿠오카로 이전한 후 처음으로 A클래스 진입에 성공하여 돌연 일본 시리즈 개최 가능성이 높아졌기 때문에 구단이 일본 뇌신경외과학회에 일정 변경을 요구했으나 일본 국내외에서 2만 명 이상의 인원이 집결하는 대규모 총회인지라 이미 관련 준비가 여러모로 끝난 상태라면서 거절당했다. 그래서 다이에 구단의 나카우치 다다시 구단주 대행(당시)이 일본 야구 기구측에 ‘일본 시리즈 개최지의 센트럴·퍼시픽 교체’, ‘일본 시리즈 일정 자체의 변경’, ‘다른 퍼시픽 리그 팀의 연고지 구장에서의 경기 개최’, ‘기타큐슈 시민 구장이나 나가사키 빅N 스타디움 등을 비롯한 규슈 지역 내의 타 구장에서의 개최’ 등을 제의했지만 모두 거절당했다. 일본 뇌신경외과학회측도 일부 일정을 단축해서 야간 시간대를 비워주는 등 협조에 나섰다. 그 결과 ‘도쿄 돔·도쿄 돔·후쿠오카 돔·휴일·휴일·후쿠오카 돔·후쿠오카 돔·도쿄 돔·도쿄 돔’ 이라는 방식으로 이렇게 이동일 없이 9일간의 변칙 일정으로 경기를 개최한다고 8월 21일에 발표했다. 또한 일본 시리즈 종료 후 다이에 구단은 개최 일정 확보를 소홀히 했다는 이유로 일본 야구 기구로부터 제재금 3,000만 엔(구단 또는 개인에게 부과된 제재금으로서는 최고 액수)을 부과받았다.\\n* 2010년에는 태풍 14호가 접근할 우려가 있다는 지적이 제기되면서 10월 30일, 10월 31일 나고야 돔에서 열리는 경기가 취소될 경우 원래대로라면 2·3차전 사이의 이동일은 그대로 두고 5·6차전의 이동일을 할애하여 실질적으로 최대 5연전이 되는 일정을 변경해서 2·3차전의 이동일을 할애해 최대 5연전으로 하고 5·6차전의 이동일은 그대로 두기로 했다. 여기에는 텔레비전 전국 중계가 4경기밖에 없다(1·2·5차전은 위성방송만 중계하며 지상파는 현역 방송만 하기로)는 점에 대한 배려도 들어갔다. 그러나 태풍으로 인한 영향은 없었고 개최 일정은 변경되지 않았다.\\n* 2020년에는 요미우리의 홈구장인 도쿄 돔이 타 대회 개최 때문에 사용하지 못하고 요미우리의 홈 경기는 교세라 돔 오사카에서 개최하게 됐다. 이 해에는 원래 도쿄 올림픽 일정 때문에 11월 7일부터 열릴 예정이었으나 세계적으로 확산되고 있는 코로나 19로 인해 공식전 개막이 크게 늦어짐에 따라 일본 시리즈도 예정보다 2주 늦은 11월 21일에 개막했다. 그러나 도쿄 돔에서는 11월 22일부터 도시 대항 야구 대회를 이미 개최가 확정된 상태였기 때문에 사용할 수 없었고 교세라 돔 오사카를 사용하게 됐다. 열네 살 예쁜 얼굴의 사춘기 소녀가 충분히 겪을 수 있는 가사이다. 그 가사는 \"그 애가 나에게 준 꼬마인형 / 좋아한단 그 말이 써 있었다오 / 날 보며 웃음 짓는 귀여운 그 인형이 / 그 애를 닮았네 작은 꼬마인형 / 한방울 두방울 봄비가 내릴 때 / 그 애는 말했다네 날 좋아한다고 / 지금은 떠나버린 그 애는 잊었지만 / 아직은 모른다고 웃는 꼬마인형 / 한송이 두송이 흰 눈이 내릴 때 / 그 애는 말했다네 날 좋아한다고\". 어느 날 소녀는 알고 지내던 한 남자아이에게 \\'좋아해\\'라는 메시지와 함께 조그만 꼬마인형을 선물받는다. 그리고 이후에도 소년은 계속 소녀에게 좋아한다고 고백한다. 소녀는 방 안에서 자신을 바라보는 듯한 그 인형이 그 남자아이와 닮았다고 생각한다. 하지만 시간이 흘러 그 남자아이를 볼 수 없는 상황이 벌어지게 되고 자연스레 그 애는 잊혀지게 되는데 자신 앞에 놓여진 인형 만큼은 \\'아직 모른다\\'며 답해주는 것 같다는 이야기의 곡이다.'], 'id': ['mrc-1-000653', 'mrc-1-001113'], 'question': [\"유령'은 어느 행성에서 지구로 왔는가?\", '용병회사의 경기가 좋아진 것은 무엇이 끝난 이후부터인가?']}\n"
     ]
    }
   ],
   "source": [
    "# Check the type and sample from reranked_result\n",
    "print(\"reranked_result type:\", type(reranked_result))\n",
    "\n",
    "# Select 'validation' split and then slice the first two examples\n",
    "print(\"reranked_result sample:\", reranked_result['validation'][:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimybjg2\u001b[0m (\u001b[33mkimybjg2-boostcampaitech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/test_mrc/wandb/run-20241021_110222-wel7fbw0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wel7fbw0' target=\"_blank\">light-plasma-668</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wel7fbw0' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/wel7fbw0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: klue/bert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default train dataset.\n",
      "Using the default validation dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3952/3952 [00:13<00:00, 296.04 examples/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 304.41 examples/s]\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7978\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3990\n",
      "  Number of trainable parameters = 110028290\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 06:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.346400</td>\n",
       "      <td>1.133017</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>64.705488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>1.669040</td>\n",
       "      <td>55.833333</td>\n",
       "      <td>66.448578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 474\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:01<00:00, 126.61it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 474\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:02<00:00, 114.94it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-3990\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-3990/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-3990/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/checkpoint-1995 (score: 1.133016586303711).\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: monologg/koelectra-base-v3-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /data/ephemeral/home/.cache/huggingface/hub/models--monologg--koelectra-base-v3-discriminator/snapshots/68b30cd259f34a4b5aa8786392612ba2a2617fcc/pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default train dataset.\n",
      "Using the default validation dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3952/3952 [00:13<00:00, 292.23 examples/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 295.86 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 8088\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4044\n",
      "  Number of trainable parameters = 112332290\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4044' max='4044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4044/4044 06:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.054000</td>\n",
       "      <td>1.611928</td>\n",
       "      <td>52.083333</td>\n",
       "      <td>60.252393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.058900</td>\n",
       "      <td>1.916150</td>\n",
       "      <td>53.750000</td>\n",
       "      <td>62.509259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 484\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:01<00:00, 124.71it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 484\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:01<00:00, 125.40it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-4044\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-4044/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-4044/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-4044/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-4044/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/checkpoint-2022 (score: 1.611928105354309).\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: klue/roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/tokenizer_config.json\n",
      "loading weights file model.safetensors from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--roberta-base/snapshots/02f94ba5e3fcb7e2a58a390b8639b0fac974a8da/model.safetensors\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default train dataset.\n",
      "Using the default validation dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3952/3952 [00:13<00:00, 302.55 examples/s]\n",
      "Map: 100%|██████████| 7978/7978 [00:07<00:00, 1000.72 examples/s]\n",
      "Map: 100%|██████████| 240/240 [00:00<00:00, 312.50 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7978\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3990\n",
      "  Number of trainable parameters = 110029058\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3990' max='3990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3990/3990 05:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.357700</td>\n",
       "      <td>1.206103</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>68.223966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>1.305059</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>72.097869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 474\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:01<00:00, 135.92it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 474\n",
      "  Batch size = 4\n",
      "100%|██████████| 240/240 [00:01<00:00, 139.57it/s]\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-3990\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-3990/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-3990/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/checkpoint-1995 (score: 1.2061028480529785).\n",
      "Saving model checkpoint to /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base\n",
      "Configuration saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/config.json\n",
      "Model weights saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/pytorch_model.bin\n",
      "tokenizer config file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/tokenizer_config.json\n",
      "Special tokens file saved in /data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import MRC\n",
    "importlib.reload(MRC)\n",
    "from MRC import Extraction_based_MRC\n",
    "\n",
    "# Initialize the MRC model\n",
    "mrc_model = Extraction_based_MRC()\n",
    "\n",
    "# List of model names or paths to train\n",
    "model_names = [\n",
    "    \"klue/bert-base\",\n",
    "    \"monologg/koelectra-base-v3-discriminator\",\n",
    "    \"klue/roberta-base\",\n",
    "]\n",
    "\n",
    "# Train each model separately\n",
    "mrc_model.train_multiple_models(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_full column names: ['question', 'id']\n",
      "retrieved_dataset column names: ['context', 'id', 'question']\n",
      "First example in test_dataset_full: {'question': \"유령'은 어느 행성에서 지구로 왔는가?\", 'id': 'mrc-1-000653'}\n",
      "First example in retrieved_dataset: {'context': '니스 모형(Nice model, Nice)은 태양계의 역학적 진화에 대한 하나의 모형으로, 2005년 모형이 처음 개발된 코트다쥐르 천문대의 위치인 프랑스의 도시 니스의 이름을 땄다.    니스 모형은 목성형 행성이 원시 행성계 원반의 소멸 이후 기존의 밀집 분포에서 행성 전이를 통해 현재의 위치로 옮겨갔다는 내용으로, 태양계 형성에 대한 기존 모형과 다르다. 행성 전이는 후기 대폭격, 오르트 구름의 형성, 카이퍼대와 목성 트로이군의 형성, 공명 해왕성 바깥 천체의 존재에 대한 설명을 위해 도입되었다. 니스 모형은 관측되는 태양계의 모습 대부분을 재현해낼 수 있어 현재 가장 가능성이 높은 모형으로 받아들여지고 있으나 이후 연구에서 지구형 행성과 소행성의 궤도 등, 니스 모형에서의 예측과 현재 태양계의 차이가 발견되어, 모형 일부가 수정되었다. 화가자리 베타 운동성단은 지구에서 가깝고 젊은 운동성단이다. 천문학에서 운동성단은 한 곳에서 태어나서 같은 고유 운동을 보이면서 우주 공간을 움직이는 항성의 무리이다. 이 집단의 대표별은 화가자리 베타이다.\\n\\n화가자리 베타 운동성단은 천문학적으로 중요한 천체인데, 그 이유는 지구에서 가장 가까우면서 젊은 집단이기 때문이다.  대표별 화가자리 베타는 원시행성계원반으로 추측되는 거대한 먼지 원반을 지닌 항성이며, 최근 베타 항성계에서 목성 질량의 8배에 이르는 가스 행성이 발견되었다.   PSO J318.5-22로 명명된 떠돌이 행성도 발견된바 있다.  지구로부터 이들 구성원까지의 거리가 가깝기 때문에 다른 별보다 상대적으로 이들에 대한 관측이 용이한 면이 있으며, 이는 화가자리 베타 b처럼 직접 사진을 찍어 외계 행성을 발견하는 원동력이 되고 있다. e가 발견되기 전까지 글리제 581 계(系)에는 이미 3개의 행성이 발견되어 있는 상태였기 때문에 e는 글리제 581의 네 번째 행성으로 기록되었다. 발견 시점 기준으로 글리제 581 e는 직전까지 발견된 \\'평범한 항성 주위를 도는 외계 행성들\\' 중 질량이 가장 작은데, 구체적인 값은 최소 지구 질량의 1.9배이며, 아무리 무거워도 지구 질량의 3배는 넘지 않을 것으로 보고 있다.\\n\\n덩치는 지구형 행성으로 보임에도 불구하고, 이 행성은 생명체가 존재할 환경으로는 부적합할 것으로 추측된다. 그 이유로 글리제 581 e는 어머니 항성으로부터 태양~수성 간격의 10분의 1도 안 되는 아주 가까운 거리를 돌고 있으며, 자체 질량이 작고, 항성의 복사 에너지가 강렬하여 표면에 액체 물이 존재할 수 없는 바싹 마른 환경이 형성되어 있을 확률이 높기 때문이다 영화를 시작할 때 해설자는 돌고래가 지구가 철거될 것을 알고 있었다고 설명한다. 돌고래는 사람들에게 뒤공중제비로 지구가 파괴될 것을 알렸지만, 사람들은 이를 잘못 이해하였고 결국 돌고래들은 안녕히, 그리고 물고기는 고마웠어요란 노래를 부르면서, 지구를 떠나게 된다.\\n\\n그러던 어느 날, 불도저가 집으로 다가오는 소리를 들은 아서 덴트는 자기 집이 우회로를 건설하기 위해서 철거될 것이라는 것을 깨닫게 된다. 아서는 집의 철거를 막기 위해, 그의 집을 철거 하려는 불도저 앞에 드러눕게 된다. 아서의 시도는 그의 친구인 포드 프리펙트 때문에 방해를 받았고, 포드는 아서를 데리고 술집으로 가게 된다. 술집에서 포드는 자기가 길포드에서 온 것이 아니라, 외계인이란 걸 아서에게 말하였다.\\n\\n포드는 아서가 그에게 보여준 친절(포드가 자동차를 지구를 지배하는 생물이라고 생각하고, 자동차와 악수하려고 할 때, 아서가 그를 길 밖으로 밀어내었다)로 보곤인이 초공간 고속도로를 놓기 위해 지구를 파괴하려는 순간 아서를 구하게 된다. 둘은 겨우 보곤 우주선에 히치하이킹을 하게 되고, 거기서 그들은 함장의 시(보곤의 시는 전우주에서 3번째로 최악이다)를 듣고 난 뒤, 우주로 던져지게 된다.\\n\\n그들은 순수한 마음호에 의해서 구조되었는데, 순수한 마음호에는 이 우주선을 훔친 은하계 대통령 자포드 비블브락스와, 지금은 트릴리언이라 불리는 트리시아 맥밀런과, 우울증을 앓고 있는 안드로이드 마빈이 타고 있었다. 자포드는 순수한 마음호로 마그라테아에 가서 삶, 우주, 그리고 모든 것에 대한 답을 얻으려고 한다.\\n\\n그들은 마그라테아로 진로를 돌렸으나, 순수한 마음호에 실린 무한 불가능 확률 추진기는 그들을 비트보들 6행성의 궤도에 올려놓는다. 비트보틀 6행성은 자트라바티드인들과 자포드와 대통령자리를 다투던 후마 카블라의 고향별이다. 후마 카블라는 마그라테아로 가는 좌표가 담겨있는 붉은 육면체를 가지고, 자포드에게 깊은 생각근처에 있는 모든 관점 총을 가져다 준다고 하면 좌표를 주겠다고 한다. 그리고 후마 카블라는 자포드가 총을 갖고 오도록 하기 위해 인질로 자포드의 다른 머리를 인질로 잡아둔다. 그들이 비트보틀 6행성에서 탈출할 때, 트릴리언은 그들의 본 행성인 보그스피어로 끌려가게 된다. 아서와 포드, 그리고 자포드는 트랄행성의 버그블래터 비스트에게 먹힐뻔한 트릴리언을 우여곡절 끝에 구하게 되고, 그들은 마그라테아로 향하게 된다. 마그라테아에서 자포드와 트릴리언 그리고 포드 프리펙트는 깊은 생각에게서 지구가 최호의 질문을 계산하는 컴퓨터였으며, 거기에 살고 있던 생명체들은 계산의 일부였다는 것을 알게 된다. 그들은 깊은 생각의 바로 밑에서 은하계의 화난 주부들의 회의에서 만들었고, 이 총을 맞은 사람은 이 총을 쏜 사람의 관점을 경험하게 되는 모든-관점 총을 찾게 된다.\\n\\n아서는 슬라티바트패스트를 만나게 되고, 슬라티바트패스트는 아서에게 자신들이 지구를 만들었다고 설명하고, 또한 자신이 지구에 만든 피요르드 때문에 상을 탔다는 것을 말하였다. 그 뒤에 그는 아서와 함께 마그라테아의 행성 제작 공장으로 데리고 가게 된다. 마그라테아 행성 제작 공장에서, 아서는 지구의 백업본인 지구 마크 2를 보게 되었고 결국 그는 재창조된 영국의 자신의 집으로 들어가게 된다. 아서는 그의 집에 들어가서, 쥐들과 친구들을 다시 만나게 된다. 그가 자리에 앉고 난 뒤에, 쥐들은 질문의 한 부분을 구상하는 그의 뇌를 떼어내려고 하였다. 우여곡절끝에 아서는 쥐들의 속박에서 풀려나게 되고, 쥐들을 찻주전자로 짓눌러 버린다. 쥐포가 되머린 쥐들은, 깊은 생각의 제작자로 변하였다.\\n\\n아서 일행이 집 밖에 나오니, 보곤인 부대가 아서의 집을 둘러싸고 있었고, 보곤 대대장은 그들을 죽이라는 명령을 내린다. 자포드는 아서의 우주선(사실 아서의 트레일러)을 조종해서 이 상황에서 탈출하려고 시도하지만 실패한다. 아서와 트릴리언은 모든-관점 총을 회수하려고 시도하였으나, 보곤 대대의 공격때문에 실패로 돌아가게 되었다. 그 총을 주우러 간 마빈은 보곤인의 공격을 맞고 쓰러졌고, 그들은 곧 죽을 것처럼 보였다. 그 때, 마빈이 그 총을 들어 보곤인들에게 모든-관점 총을 쏘았다. 그가 쏜 총으로 인해서 모든 보곤인들은 그의 관점을 느끼게 되었고, 그들은 모두 우울증에 빠지게 되었다.\\n\\n보곤인의 공격이 분쇄되고, 지구의 모든 생명주기가 보곤인이 파괴하기 전으로 돌아가게 되었고, 아서와 그의 친구들은 순수한 마음호를 타고 우주저편의 레스토랑으로 다시 여행을 떠나게 되었다. 지구 근접 소행성(地球近接小行星)은 지구 궤도와 교차하는 궤도를 가진 소행성으로, 지구와 충돌할 위험이 높은 천체이다. 우주선으로 쉽게 다가갈 수 있으며, 실제로 미국 항공우주국의 니어 슈메이커가 433 에로스에 착륙한 적이 있다.\\n\\n지금까지 수백 개의 지구 근접 소행성이 발견되었으며, 큰 것은 4 킬로미터에 이른다. 발견되지 않은 것을 포함하여 모두 수만 개가 있을 것으로 추정되며, 1 킬로미터 이상되는 것도 2000개는 될 것으로 보인다. 천문학자들은 지구근접소행성의 수명이 천만 년에서 일억 년밖에 되지 않을 것으로 믿는다. 그것들은 내행성과의 충돌이나 행성 접근에 뒤따르는 태양계 이탈로 그 수명을 다한다. 지구 근접 소행성은 이러한 과정을 거쳐 계속 제거되어 나가지만, 어떠한 방법으로 그 수가 유지되고 있는 것으로 보인다.\\n\\n지구 근접 소행성은 크게 세 부류로 나눌 수 있다. 아텐 족(Aten asteroid)은 장반경이 1 AU보다 짧은 소행성이다. 발견된 아텐 족 중에 몇몇 천체를 제외하고는 원일점의 위치가 1 AU보다 바깥에 있다. 아폴로 족(Apollo asteroid)은 장반경이 1 AU보다 길지만 지구 궤도를 교차하는 소행성들이다. 마지막으로 아모르 족(Amor asteroid)은 장반경이 지구와 화성 궤도 사이에 있으며 지구에 가까이 다가올 수 있지만, 근일점의 위치가 1AU보다 바깥에 있어 충돌의 위험이 없는 소행성이다. 1. 공룡클럽 친구들\\n\\n(1) 렉스: 이 시리즈에서도 주인공인 소년, 다이노 행성의 공주인 \\'라키나\\'를 만나서 다시 한 번 다이노 마스터가 되고 다이노 행성으로 가게 되었다. 다이노 행성에서는 \\'스피노 족\\'의 새친구 \\'레오\\'를 만나게 되었다. 그러다 가는 중에 또 다른 다이노 마스터아자 오랜 친구였던 \\'아칸 웨이\\'를 오랜만에 만났다. 게다가 지구에서 다이노 행성으로 옮겨진 다이노 아일랜드에서 렉스는 지신의 아빠인 \\'파브로\\'와 친구 \\'존\\'도 다시 만나고, 북쪽으로 약 7,200km 떨어진 \\'북쪽마녀 호\\'에서 어른 모습이지만 북쪽마녀가 된 \\'에밀리\\'를 다시 만났다.  \\n\\n(2) 파브로: 렉스의 아빠로 최고의 재료와 최고의 레시피로 \\'다이노 피자\\' 가게에서 피자를 만드는 사장, \\'에볼루션 시즌 1 1화~ 5화\\' 까지는 행방이 묘연했다. 하지만 \\'6화\\'부터 렉스를 다시 만나서 피자 사업을 계속 이어가는 중이다. \\n\\n(3) 에밀리: \\'에볼루션 1기 1화\\'에 출연하다가 \\'2화~ 8화\\'까지는 출연이 묘연했던 소녀인 렉스의 친구, 그러다가 어른모습이고 북쪽마녀가 되었지만 \\'9화\\'에서 \\'렉스\\'를 다시 만나게 되었다. 그러다가 \\'시즌 2 11화\\' 에서 다시 어린애 모습으로 돌아가게 되는데.....\\n\\n(4) 존: \\'에밀리\\'와 마찬가지로 \\'시즌 1 1화\\'에 출연하다가 출연에 잘 안나온 소년인 렉스의 친구, 그러다 \\'6화\\'에서 \\'렉스\\'를 다시 만나고 \\'렉스\\'에게 에밀리가 사라졌다고 말했다. 그리고 \\'시즌 2 5화\\'부터 모험을 함께 하고 있다. \\n\\n(5) 아칸 웨이: \\'시즌 1 5화\\' 에서 다시 만난 다이노 마스터이자 렉스의 또 다른 오랜 친구, \\'라키나\\'와 \\'렉스\\'를 다시 만나고 \\'다크 킹\\'과 \\'코스모 사우르스\\'를 막기 위해 친구들과 함께 모험을 하고있다. \\n\\n(6) 라키나: \\'시즌 1 1화\\'부터 등장한 다이노 행성의 공주, 렉스와 아칸을 데리고 \\'다크 킹\\'과 \\'코스모 사우르스\\'를 막기 위해 노력 중인가 하면 \\'시즌 1 13화\\'에서는 \\'다이노 피자\\'의 점원이 되어 렉스의 아빠인  \\'파브로\\' 로 부터 피자 만드는 법을 배우곤 했다. \\n\\n(7) 레오: \\'시즌 1 2화\\'에서 출연한 스피노 족 소년, 렉스를 스승처럼 따르며, 평소에는 주황색 늑대 형 로봇을 몰았다. 그런데 \\'시즌 2 1화\\' 부터 자기도 모르는 사이에 다이노 마스터가 되어 렉스와 아칸에게 큰 도움을 준다. 렉스처럼 엉뚱하긴 하지만, 마스터인 렉스와 그 친구들을 지키려는 마음은 같다.  \\n\\n(8) 케인: \\'시즌 2 6화\\' 부터 등장한 정체불명의 남자, 알고 보니 \\'카야\\'의 친오빠였던 것이다. 처음에는 악랄했던 메카몬스터로 변신하는 \\'다이노헌터\\'였다. 하지만 렉스 일행이 \\'시즌 2 11화\\'에서 10년 전 과거를 바꾸면서 케인은 이제 렉스 편이 되었고 \\'드래곤\\'의 \\'드래곤 스톤\\'을 돌려준다. \\n\\n(9) 카야: \\'시즌 2 12화\\'에서 등장한 렉스의 친구 중 1명, 어른 모습으로 오랜만에 출연했다. 그런데 10년 전의 일이 우주를 어둠으로 물들였다고 알려준다. 하지만 렉스가 \\'코스모 사우르스\\'를 완전히 소멸시킨 후, 친구들과 함께 피자파티에 참가한다. \\n\\n(10) 빌& 테드: \\'썬썬 피자\\' 가게를 운영하는 부잣집 아들과 그의 친구, \\'에볼루션 시즌 1 7화\\' 부터 나왔지만 렉스를 바보 취급하는 것은 여전하다. 그런데 \\'테드\\'는 \\'8화\\'에서 \\'빌\\'과 \\'다이노 피자\\'문제로 싸워서 \\'다크 킹\\'의 메카몬스터가 되었다. 하지만 몇 분뒤, \\'빌\\'이 \\'다이노 피자\\'의 피자로 \\'테드\\'의 마음을 달래자 테드는 원래대로 돌아온다.  \\n\\n\\n\\n2. 못된 악당들 \\n\\n(1) 다크 킹: 한 때 다이노 행성을 위해 싸우던 전사, 하지만 \\'코스모사우르스\\'에게 조종 당하여 악당이 되어서 메카몬스터를 조종해서 렉스와 아칸의 인피닛 튜너 갤럭시 스톤을 뺏으려고 했다. 하지만 \\'시즌 1 12화\\' 에서 렉스가 정화해주려던 찰나에 \\'코스모 사우르스\\'가 다크킹을 갤럭시 스톤으로 바꿨다. \\n\\n(2) 코스모사우르스: 이 작품 최종 보스로 우주를 지배하려고 했던 자다. \\'시즌 2 12화\\'에서 우주를 아예 없애려고 했으나 \\'렉스\\'가 그를 검안에 가두고\\'드래곤\\'이 완전 정화시켜서 결국 사망하고 만다. 목성의 대기에서 보이는 줄무늬는 적도와 평행하면서 행성을 둘러싸는 대(zone)와 띠(belt)라고 불리는 물질의 반대 순환류에 의한 것이다. 대는 밝은 줄무늬로, 대기에서 상대적으로 고도가 높은 곳에 있다. 이들은 내부의 상승 기류를 가지고 있는 고기압 영역이다. 띠는 어두운 줄무늬로, 대기에서 상대적으로 고도가 낮은 곳에 있으며, 내부의 하강 기류를 가진다. 이들은 저기압 영역이다. 이러한 구조는 지구 대기의 고기압 및 저기압 세포와 어느정도 유사하나, 국지 작은 기압 세포와 상반되는 행성 전체를 둘러싸는 위도 줄무늬로서 매우 다른 구조를 가지고 있다. 이는 행성의 빠른 자전과 근본적인 대칭으로 인한 결과로 보인다. 행성에는 국지적인 가열을 일으키는 바다나 육지가 없으며 자전 속도는 지구보다 훨씬 빠르다.\\n\\n행성에는 서로 다른 크기와 색상을 갖는 점과 같은 작은 구조들이 있다. 목성에서, 그러한 특색 중에서 가장 유명한 것은 대적점으로, 적어도 300년 동안 존재해 왔다. 이러한 구조의 실체는 거대한 폭풍이다. 그러한 점 중에 일부는 적란운이기도 하다.\\x7f 어느 날 푸딩 때문에 신노스케는 히마와리와 크게 싸웠다. 결국 분에 못 이겨 신노스케는 아빠인 히로시와 엄마인 미사에의 만류에도 아랑곳하지 않고 집을 뛰쳐나가고 마는데. 그런 신노스케 앞에 갑자기 정체를 알 수 없는 수수께끼의 남자들이 나타난다. 그들은 겉으로 보기에 인간 족들과는 사뭇 달라보였으며, 신노스케는 그들이 우주계와 은하계 저 너머에 온 외계인들이라는 것을 알아챘다. 신노스케는 깜짝 놀라 다시 집으로 되돌아가려고 했지만, 그들은 신노스케에게 다정하게 대하면서 신노스케를 안심시킨다.\\n\\n그러자 그와 동시에 그들은 여동생인 히마와리를 데리고 가겠다고 말하며, 처음에 신노스케는 고민에 휩싸이게 된다. 그러나 방금 푸딩 때문에 히마와리와 크게 싸웠기 때문에 신노스케는 서슴없이 히마와리를 그들에게 넘긴다는 계약서를 받았다. 엉겁결에 우주계 계약서에 싸인을 해 버린 신노스케! 때마침 나타난 우주 비행선에 외계인들이 모조리 도착하여 노하라 가족들의 집에 침투하였고, 노하라 가족들은 저항할 틈도 없이 모두 납치되고 말았다.\\n\\n노하라 가족들이 엄청난 고생을 해서 도착한 곳의 이름은 바로 \\'히마와리 별\\'이라는 곳이었고, 지구 행성에서 한참 떨어진 몇 광년이나 되는 어마어마한 거리였다. 그곳에서 노하라 가족들은 대원들의 인도를 받으면서 히마와리 별의 대도시 중심부에 있는 황궁에 도착하였다. 노하라 가족들은 히마와리 별을 다스리는 대천황을 보자마자 큰 절을 하였으며, 대천황도 노하라 가족들을 따뜻하게 맞이했다. 그리고 대천황은 지구 행성의 평화를 위해 히마와리가 황태녀가 되지 않으면 안 된다는 사실을 노하라 가족들에게 조목조목 말하고 히마와리를 자신에게 넘겨달라고 명령한다. 기상(氣象)은 강수, 바람, 구름 등 대기 중에서 일어나는 각종 물리적인 현상을 통틀어 이르는 말이다. 대기 현상과는 달리 태풍, 구름 등의 대규모 현상도 포함한다. \\'날씨\\'(날거리, 일세)나 \\'일기\\'(日氣)와 같은 의미로 쓰이기도 하나, 날씨 또는 일기(日氣)는 그날그날의 기상 상태를 일컫는 말로, 엄밀히 말하면 다른 의미이다. 최근에는 다른 행성에 대한 연구가 활발해지면서 지구 외의 천체의 대기도 기상의 범주에 포함하게 되었지만, 기상이라고 하면 보통 지구 내의 기상을 의미한다.\\n\\n이 용어는 짧은 기간(여러 시간, 여러 날)에 걸친 현상을 일컫는 것이 보통이다. 그에 비해 기후는 오랜 기간에 걸친 평균적인 대기 상태를 일컫는다.\\n\\n지구의 대기는 1000km 이상까지 존재하고 있어 어디까지를 대기로 보는가에 따라 기상의 범위가 달라지게 되지만, 일반적으로 구름, 강수 등의 대부분의 기상 현상은 대기의 가장 하층인 대류권에서 일어난다.\\n\\n기상의 관측은 높이에 따라 지상에서는 우량계, 적설계, 레이다등을 이용하며, 고층에서는 인공위성, 라디오존데 등을 이용한다. 이 항성의 시선속도는 매우 일정해서 천문학자이자 행성 사냥꾼 제프리 마시는 랄랑드 21185를 적색왜성의 안정성 단계들 중 \\'평범함\\'의 완벽한 예시로 취급하고 있다.  이런 부정적인 결과들과 기타 연구들이 랄랑드 21185에 행성계가 존재할 가능성을 전적으로 차단하는 것은 아니지만, 존재 가능한 행성의 질량에 상한선을 설정한다. 앞으로 수행될 지상 및 우주 기반 관측들은 이 상한선을 분명히 낮출 것이며 질량 작은 행성을 발견할 수도 있을 것이다.\\n\\n랄랑드 21185의 생명체 거주가능 영역(지구 비슷한 행성에 액체 물이 존재할 수 있는 위치)은 0.11 ~ 0.24 천문단위 영역에 걸쳐 형성된다. 여기에서 1 천문단위는 지구로부터 태양까지의 평균 거리이다 페가수스자리 51은 플램스티드 명명법에 따른 명칭이다. 이 별을 도는 외계 행성 발견 당시 과학자들은 행성에 어머니 항성 명명법에 따라 \\'페가수스자리 51 b\\'를, 비공식적으로는 그리스-로마 신화의 등장인물 이름을 따오는 전통에 따라 \\'벨레로폰\\' 이름을 붙여 주었다.(벨레로폰은 신화에서 날개 달린 말 페가수스를 모는 자이다.\\n\\n2014년 국제천문연맹은 특정 외계행성들과 어머니 항성들에 고유명칭을 부여하는 절차를 개시했으 천체들에 붙일 고유명칭을 공모했다 2015년 12월 IAU는 페가수스자리 51의 고유명칭으로 헬베티우스(Helvetios)를, 행성 b 명칭으로 디미디움(Dimidium)을 선정했다\\n\\n선정된 명칭들은 스위스 루체른 천문학회가 제출했다. \\'헬베티우스\\'는 헬베티족의 라틴어 표기로 오래 전 스위스에 살았던 켈트족을 지칭하는 단어이다. 한편 행성의 명칭 \\'디미디움\\' 역시 라틴어로 \\'절반\\'이라는 뜻이며 이는 b의 질량이 목성의 최소 절반이라는 점에 착안한 것이다\\n\\n2016년 IAU는 항성들의 고유명칭을 목록화, 표준화할 목적으로 항성명칭 워킹그룹(WGSN)을 조직하였다 2016년 7월 최초 공고에 WGSN은 \\'행성 및 행성 위성의 공개 명명에 관한 실무위원회 워킹그룹\\'이 승인한 행성/어머니 항성의 명칭들 및 2015년 NameExoWorlds 캠페인에서 채택한 항성 명칭들을 명시적으로 인정했다. 현재 페가수스자리 51은 IAU 항성명칭목록에 수록되어 있다 은하수를 여행하는 히치하이커를 위한 안내서는 방대한 지식의 은하대백과사전보다 더 유명한 책인데, 그 이유는 은하대백과사전보다 더 싸다는 것, 그리고 앞에다가 큼지막고 친근하게 당황하지 마시오(Don\\'t Panic)라고 적어놓은 것이었다. 이 전은하적인 베스트셀러인 이 책은 아직도 전자시계가 좋은 아이디어라고 생각하는 지구에서는 알려지지 않았다. 포드 프리펙트(Ford Prefect)는 안내서의 이동 조사원이었는데, 지구에서 머무는 동안 안내서는 그에 대해서 완전히 잊어버리게 되었다. 포드의 가장 친한 친구는 겸손한 지구인인 아서 덴트(Arthur Dent)였다. 그는 웨스트 컨트리에서 지루한 삶을 보내고 있었는데, 어느날 아침 포드가 아서에게 자기가 베텔게우스(Betelgeuse) 근처의 작은 행성에서 왔고, 지구가 곧 파괴될거라고 경고했다. 실로, 관료적인 외계인 종족인 보고인은 초은하 고속도로의 우회로를 건설하기 위해 지구를 부숴버릴 작정이었다.\\\\n\\\\n지구가 파괴되려고 할 때, 간신히 보고인 우주선에 히치하이킹을 하고, 다시 다른 곳으로 히치하이킹을 하려고 할 때, 그들은 사로잡혀서 보고인의 시(\"전 우주에서 3번째로 못쓴 시\")를 듣게 되고, 아서와 포드는 공허한 우주로 방출되게 된다. 그들은 질식사 직전에 가까스로 은하계 대통령 자포드가 진수전 훔친 순수한 마음호에 의해 구출된다. 그 배안에는 자포드(Zaphod Beeblebrox)와 트릴리언(Trillian)이 있었는데, 아서는 그녀는 영국 이즐링턴에서 열린 파티에서 보고 다시 보는 것이다. 그리고 심각한 우울증에 시달리는 안드로이드 마빈까지 있었다. 포드 프리펙트와 아서는 자포드가 배를 모는 이유가 뭔지 알게 된다. 전설속의 초호화 행성을 만든 마그네시아를 찾는 것을 말이다. 마그라테아에 도착해서 오류가 난 자동 방어시스템의 핵미사일을 순수한 마음호의 무한 불가능 확률 추진으로 다른 물체로 만들어버리고(고래와 페튜니아 화분으로 바꿔 버린다), 마그라테아에 도착해서 아서는 피오르를 설계한 늙은 슬라티바트패스트를 만나게 되고, 그는 아서에게 지구인에게는 쥐라고 알려졌지만, 실상으로는 초지능적인 범차원존재가 지구를 계획, 설계하였다고 이야기를 해주었다. 쥐들은 그들 차원에서 인생의 의미를 찾도록 깊은 생각이라는 컴퓨터를 750만 년 동안 돌리고, 답을 얻어낸것이 42였다. 그래서 답에 대한 질문을 얻기 위해서 행성만한 컴퓨터인 지구를 설계했고, 그 질문이 나오기 5분 전에 보고인에 의해서 지구가 파괴 되었다. 그리고 그는 그 질문을 찾기 위해서 지구2를 만들고 있다고 하며, 그들을 쥐에게 데려가 준다. 쥐들은 아서의 뇌파에 최후의 질문이 암호화돼 있다고 결론 내리고, 그의 뇌를 꺼내려 한다. 그때 은하경찰이 자포드를 체포하러 왔고, 편집증 안드로이드인 마빈이 자신의 우주에 대한 견해를 은하경찰 우주선의 컴퓨터에게 넣어 은하경찰 우주선의 컴퓨터를 자살시키고, 그 컴퓨터와 서브-에사로 연결된 그들은 죽게 된다. 그래서 자포드와 그의 일행은 구사일생으로 살아나게 된다. 자포드는 순수한 마음호로 우주 끝에 있는 레스토랑인 밀리웨이즈(Milliways)로 가자고 하고, 이야기는 2권인 우주의 끝에 있는 레스토랑으로 이어지게 된다. 주인공인 비론 파릴(Biron Farrill)은 네페로스 행성의 지도자의 아들로서 지구의 대학에 유학을 왔다. 어느날 동급생 샌더 존티어(Sander Jonti)는 네페로스 행성이 속한 말머리 성운을 지배하는 타이란 제국에 의해 비론의 아버지가 체포를 당했다고 말하며 다른 별인 로디아(Rhodia)로 도망치기를 권한다. 그곳에 가서도 어려움을 겪다가 총독의 딸인 아르타(Artemisia) 그녀의 삼촌 질브레트(Gillbret)와 함께 우주선을 탈취해고 린겐(Lingane)별을 찾아간다. 거기서 아우타치(Autarch)(=샌더 존티어)의 정체를 알게되고 질브레트(Gillbret)가 말하는 숨겨진 혁명세력을 찾으러 같이 말머리 성운 안을 탐험하러 간다. 그곳의 한 행성에서 아우타치는 자신이 밀고자임을 말하며 비론을 죽이려고한다. 그러나 그가 배신자임을 알게된 측근에 의해 아우타치는 죽게되고 일행은 타이란 제국측에 체포된다. 질브레트는 결국 숨겨진 혁명 세력이란 자신의 공상임을 밝히고 처형 직전에 사면을 받아 일행은 목숨을 건지게된다. \\n그러나 혁명세력은 존재하였고 중심은 바로 로디아(Rhodia)였다고 로디아의 힌리크 총독이 밝히며 마지막에 이상적인 정치를 위한 고대문서로 \\'미국 헌법\\'이 언급한다. 평범한 별은 주로 가벼운 원소인 수소와 헬륨으로 이뤄져 있다. 이들에는 보다 무거운 원소도 소량 섞여 있으며 이 비율을 항성의 중원소함량(비록 이 원소들이 우리가 익숙하게 생각하는 금속 형태는 아니지만)으로 부르 [m/H]로 표시하고 대수 계산자(여기서 태양의 중원소함량은 0)로 표현한다.\\\\n\\\\n2012년 케플러 우주선의 연구자료에 따르면 해왕성보다 반지름이 작은 행성은 중원소량이 −0.6 < [m/H] < +0.5 범위임에 비해(태양의 4분의 1에서 3배 범위) ≈ 1/4), (10 ≈ 3)]이 된다.</ref> 이보다 큰 행성은 대부분 중원소량이 상기 범위의 높은 쪽으로 몰려 있었다. 이 연구를 통해 작은 행성은 무거운 행성보다 태양보다 중원소량이 높은 항성에서 약 세 배 더 많이 태어나나, 태양보다 중원소가 부족한 항성 주위에서는 무거운 행성보다 여섯 배 더 많이 태어나는 것으로 드러났다. 중원소가 부족한 항성 주위에 가스 행성이 드문 이유로 원시행성계원반에 중원소가 풍부할수록 원시행성의 핵이 빠르게 뭉쳐서 원반 내 가스가 소실되기 전에 포획할 확률이 큰 것으로 해석할 수 있다. 다만 케플러 우주선은 항성에 아주 가까이 있는 행성만 관측할 수 있다. 케플러가 찾은 가스행성들은 지금보다 바깥 궤도에 있다가 항성 가까이 끌려 온 것으로 보인다. 중원소가 부족한 원반에서 태어난 가스 행성은 풍부한 원반 내 가스행성보다 항성 쪽으로 끌려들어갈 확률이 적은데, 이 이론이 케플러의 연구 결과를 일부 해명해 주고 있다. \\\\n\\\\n2014년 연구로 거대 가스행성뿐 아니라 작은 행성까지도 중원소가 부족한 항성보다 풍부한 항성 주위에서 태어날 확률이 높아짐을 알아냈다. 다만 중원소 함량이 높아질수록 가스행성의 질량은 보다 커지는 경향을 보였다. 이 연구에서 행성은 지구질량 1.7배, 3.9배를 경계로 세 집단으로 구분된다.(지구형 행성, 난쟁이 가스행성, 거인 가스행성) 이 세 집단에서 행성의 탄생 확률은 중원소가 높은 별 근처가 낮은 별 근처보다 각각 1.72배, 2.03배, 9.3배 높았다. 중원소가 풍부한 별 주위 가스행성은 질량이 커지는 성향이 있어 작은 행성을 찾는 데 장애요소로 작용하므로, 위 발생확률 증가분은 하한선이다. \\\\n\\\\n행성을 거느린 항성은 그렇지 않은 항성에 비해 리튬 함량이 적은 것으로 밝혀졌다. 안드로메다자리 웁실론 b(안드로메다자리 웁실론 B는 적색 왜성이기 때문에, 안드로메다자리 웁실론 Ab로 불릴 때도 있다)는 지구에서 44광년 떨어져 있는 외계 행성으로, 안드로메다자리에 있다. 이 행성의 어머니 항성인 안드로메다자리 웁실론 A는 태양과 거의 비슷한 항성으로, 공전주기는 약 5일이다. 발견자는 제오프리 마시와 R. 폴 버틀러이며, 발견년도는 1996년이다. 이 행성은 외계 행성들이 발견되기 시작했던 초기 시기의 \\'뜨거운 목성\\' 중 하나이다. 안드로메다자리 웁실론 b는 이 행성계의 구성원들 중 어머니 항성에 가장 가까이 붙어 있다. 후기 대폭격\\n달 및 다른 지구형 행성에 남아있는 충돌구는 후기 대폭격의 대표적인 증거로, 태양계가 형성된 후 6억 년 후에 행성과 충돌한 소행성의 수가 급증한 현상이다. 니스 모형에서는 초기에 외곽의 미행성대가 천왕성 및 해왕성의 영향으로 흐트러져 내행성의 궤도로 침투하여 얼음 미행성에 의한 충돌이 급증하고, 이후 목성형 행성의 영향으로 소행성대의 소행성도 태양계 안쪽으로 향해 암석질 미행성과의 충돌이 급증한다 이 과정을 통해 달과 충돌하리라 예상되는 미행성 수는 후기 대폭격의 충돌구 기록에서 계산되는 수와 비슷하지만 남아있는 소행성의 분포는 계산이 관측에 부합하지 않는다\\n\\n후기 대폭격로 인해 목성의 위성 가니메데에서는 행성 분화가 촉진되었지만, 칼리스토에서는 행성 분화가 일어나지 않았다.  토성의 위성에는 얼음 미행성이 대량으로 충돌했지만, 결과적으로 얼음은 모두 승화되어 없어졌다. 이 정의는 엄밀해야 할 정의가 모호성을 띄고 있다는 점에서 비판을 많이 받았다. 천문학자 필 플래이트와 NCSE 작가 닉 마츠케는 이 정의에 따르면 행성이 항성 궤도 바깥으로 밀려나거나 떠돌이 행성처럼 항성 궤도상에서 형성되지 않았다면 행성이라고 부르지 못한다는 점을 비판했다.   하지만 같은 맥락에서 위성은 행성 궤도 바깥으로 나가면 행성으로 여긴다는 점에서, 이 의견은 논란이 있다.\\n\\n또한 이중행성의 정의도 반론이 있었다. 현재 지구와 달의 질량중심은 지구 내부에 있지만 조석 가속을 통해 결과적으로 지구 바깥으로 밀려나게 되는데, 만약 그렇다면 달 또한 행성으로 분류될 수 있었다. 하지만 실질적으로 이 과정은 시간이 아주 오래 걸려, 태양이 적색거성이 되어 지구와 달 모두 소멸되기까지 일어나지는 않을 것으로 예측된다. \\n\\n마이클 브라운은 2006년 8월 18일 라디오 프로그램 \"Science Friday\"에서 이 문제는 \"대륙\"이라는 단어와 비슷하며, \"대륙이라는 단어 자체는 과학적 의미가 없는 문화적인 단어일 뿐이고, 자신은 지질학계에서 대륙이라는 단어의 엄밀한 정의를 부여하려고 하지 않은 것이 잘 한 일\"이라고 생각한다고 언급하였다. \\n\\n8월 18일 오언 깅거리치는 자신이 받은 서신에서 찬반이 고르게 갈렸다고 말했다. 시계자리 요타(ι Hor / ι Horologii)는 시계자리 방향으로 지구에서 약 56광년 떨어진 곳에 있는, 태양과 비슷한 항성이다. 분광형은 G0V 로 황색 왜성이다(예전에는 분광형 G3에 준거성으로 분류되었다). 질량과 크기는 태양보다 조금씩 더 크며 밝기는 1.5배 정도이다.\\n\\n1998년 기준으로 이 별 주위를 외계 행성 하나가 돌고 있는 것이 확인되었다. 이 행성의 궤도는 지구 궤도 정도 크기이기 때문에 시계자리 요타는 NASA가 선정한 지구형 행성 탐사 계획의 69번째 항성으로 지목되었다. 2000년 항성 주위에 원시행성계 원반이 있다는 발표가 있었으나 이후 관측 기구의 결함 때문에 생긴 착오로 밝혀졌다. 더크 젠틀리의 성스러운 탐정사무소의 줄거리는 이야기의 중추적인 부위에 자리잡은 시간 여행이란 주제 때문에 줄거리가 이어져 있지 않다.\\\\n\\\\n40억년전 지구에서, 사락사라의 사람들이 자신들만의 낙원을 만들기 위해 지구에 착륙하였다. 그러나, 기술자의 게으름으로 인해서 착륙선은 폭발하였으며, 안에 타고 있던 사락사라 사람들은 모두다 죽어버렸으며 그 폭발에 의한 에너지가 아미노산을 만들어 지구에 생명이 생기게 되었다. 그 게으른 엔지니어는 유령이 되어 그가 저지른 잘못을 돌리기 위해 돌아다니면서, 인류의 발전에 영향을 미쳐왔다. 1800년 초에 유령은 케임브리지 세인트 체드 단과대학의 리즈 교수가 타임머신을 가졌다는 걸 알게 되고, 리즈교수를 손에 넣을려고 하였으나 실패하고 만다. 그리고 유령은 새뮤얼 테일러 콜리지에게 붙어, 그의 시 쿠빌라이 칸과 늙은 뱃사람의 노래에 착륙선을 고칠방법을 적게 만든다. 유령은 200여 년 동안 콜리지의 작품에 감명을 받은 영혼을 찾아다녔으며, 21세기무렵 마이클 웬튼 윅스라는 전 예술잡지 편집장을 찾게 된다. 유령은 마이클이 콜리지의 작품을 읽게 만들었으며, 그를 조종하기 위해 마이클로 하여금 마이클의 후임 편집장인 알버트 로스를 살해하게 된다.\\\\n\\\\n유령의 다음 행동은 다른 사람에게 영향을 끼칠 숙주를 찾는 일이었다. 리즈는 어린 소녀를 재미있게 해주기 위해 타임머신을 가동하였으며, 유령이 사용하기위해 데려온 전자 수도사는 웨이포인트 테크놀러지 II의 사장인 고든 웨이를 죽여버린다. 리즈의 학생이었으며, 웨이포인트 테크놀러지 II의 컴퓨터 프로그래머인 리처드 맥더프는 그의 여자친구인 수잔 웨이의 집의 자동응답기에 저장된 메시지를 지우기 위해서 몰래 집에 침입하게 되는데, 그때 맥더프는 알지 못하게 유령에게 이용되었다.\\\\n\\\\n이건 만물의 상호 연관성을 믿는 맥더프의 친구인 더크 젠틀리를 끌여들이게 되었다. 더크 젠틀리와 맥더프는 여럿 우연한 장소에서 정보를 얻고, 유령이 개입했다는 것과, 리즈 교수가 타임머신을 가졌다는 걸 알게 된다. 그들은 리즈교수를 찾아가게 되었고, 리즈 교수는 타임머신이 있다는 걸 그들에게 말해준다. 그때, 그들은 유령에 빙의된 마이클을 만나게 되고, 보호장구를 갖춘 마이클은 교수에게 부탁하여 40억년 전으로 거슬러 올라가게 된다.\\\\n\\\\n마이클이 우주선을 수리하러 밖에 나가고, 리처드는 현대의 수잔으로부터 마이클이 로스를 살해했다는 전화를 받게 된다. 더크는 유령이 마이클을 조종하기 위해 로스를 살해했다고 추리하게 된다. 더크는 유령을 막지 못한다면 인류가 없어질 것이고, 사락사라인들이 지구를 차지하게 될거라고 경고하게 된다. 리즈교수는 더크 젠틀리를 데리고 1800년대로 데리고 간다. 거기서 더크는 콜리지에게 폴락에서 콜리지를 취재하러 왔다고 하고, 그가 착륙선을 고칠방법을 적지 못하게 막아 유령의 계획을 저지하게 된다. 현대로 돌아온 더크 젠틀리는 그가 맡고 있던 고양이 실종 사건이 없다는 걸 알게 되고, 비서에게 \"인류를 멸종위기에서 구했음 - 무료\"라고 청구서를 보내라고 지시한후 밖으로 나가게 된다. 서기 2154년, 지구는 에너지 고갈 문제를 해결하기 위해 지구로부터 멀리 떨어진 행성  \\'판도라\\'에서 대체 자원을 채굴하기 시작한다. 하지만 판도라의 독성을 지닌 대기로 인해 자원 획득에 어려움을 겪게 된 인류는 판도라의 토착민 \\'나비\\'(Na\\'vi)의 외형에 인간의 의식을 주입, 원격 조정이 가능한 새로운 생명체 \\'아바타\\'를 탄생시키는 프로그램을 개발한다. 이 생명체 아바타는 나비 족의 유전자와 아바타 주인의 유전자 일부를 섞어서 만들어지며, 그렇기 때문에 한 사람당 하나의 아바타만을 가지게 되며 그들의 신경 또한 서로 연결되어 있다. 이러한 아바타는 인간이 아바타의 신경에 접속한 상태에서 활동하며, 접속이 끊어졌을 때는 잠들어 있는 상태가 된다.\\n\\n한편, 하반신이 마비된 전직 해병대원 \\'제이크 설리\\'는 아바타 프로그램에 참가할 것을 제안받아 판도라 행성으로 향한다. 그러나 본래는 과학자인 그의 쌍둥이 형이 아바타 프로그램에 참가할 예정이었고, 아바타 역시 그의 쌍둥이 형의 유전자로 만들어진 것이다. 그런데 형이 사고로 죽자, 어쩔 수 없이 절름발이이며 아바타 프로그램 훈련조차 받지 않은 제이크 설리를 데려오게 된 것이다. 그는 아바타 프로그램에 참여하면 많은 돈을 벌 수 있다는 말을 듣고 아바타 프로그램에 참여하게 된다. 그의 하반신 마비를 치료하기 위한 돈을 마련하기 위해서였다.\\n\\n어쨌거나 형과 유전자가 같은 제이크는 자신의 아바타를 통해서 자유롭게 걸을 수 있게 된다. 하루는 제이크의 아바타가 속한 탐색조가 갑작스런 야생동물의 습격으로인해 위기를 맞게 되고, 가까스로 따돌리지만 제이크는 탐색조에서 떨어진다. 그날 밤, 제이크는 개 형상의 동물들의 공격을 받던 중 네이티리의 도움으로 목숨을 구하게 되고, 그녀로부터 나비 족들이 있는 곳으로 인도된다. 처음에 나비 족들은 \\'악마\\', \\'꿈꾸는 자\\'와 같은 표현으로 그를 기피하였다. 그러나 네이티리가 그의 아버지이자 추장인 에이투칸을 설득한 덕으로 제이크는 그들의 무리에 합류할 수 있었다. 한편, 아바타가 있는 위치에 상관 없이 제이크의 의식이 본래 육신으로 돌아오면 인간들과 접촉할 수 있다. 그가 나비 족들과 접촉하였고, 그들의 무리와 합류한 것을 안 마일즈 쿼리치 대령은 지구에 가서 다리를 치료받게 해준다는 조건으로 자원 채굴을 막으려는 나비족을 원래 서식지로부터 이주시키라는 임무를 받게 된다. 그들이 살고 있는 서식지 땅 속에는 \\'언옵타늄\\'이라는 대체 자원이 가득했기 때문이다. 나비족이 계속 거기서 살고 있었기 때문에 자원을 캐내는 데 많은 어려움이 따랐었다. 따라서 그는 나비족과 같이 생활하며 그들의 신뢰를 얻기 위해 그들의 문화와 전통을 배우고, 전사가 되기 위한 노력을 한다. 그들의 신뢰를 얻어 그들이 다른 곳으로 이주하게 설득을 하고, 그렇게 대체 자원을 캐내기 위해서였다. 군사적 침략도 가능했지만 원주민들을 죽이면 지구에서 여론이 좋지 않을 것을 우려한 것이었다.\\n\\n제이크는 처음에는 나비 족의 무리에 적응하는 데 어려움을 겪는다. 말을 잘 타지도 못하고 나비 족의 언어도 잘 익히지 못했다. 하지만 제이크는 \\'네이티리\\'와 함께 지구에서는 겪을 수 없었던 다채로운 모험을 경험하면서 네이티리와 사랑에 빠지고, 나비 족들과 하나가 되어간다. 하지만 쿼리치 대령이 제이크가 네이티리와 나비 족과 사랑에 빠진 것을 알게 되었고, 평화적 방법으로 자원을 캐내지 못할 것이라는 것을 알고 판도라의 자원을 강탈하기 위한 지구인들의 군사 침략이 시작된다. 하지만 제이크는 판도라의 생활에 익숙해져가고 네이티리와의 사랑에까지 빠져 결국 지구인들의 자원 채굴계획에 반감을 가지게 되어 같은 생각을 가진 동료들과 함께 싸우며 판도라를 지켜낸다. 그리고 나비족의 의식을 통해 그는 인간의 육신에서 나비족의 육신으로 다시 부활한다.', 'id': 'mrc-1-000653', 'question': \"유령'은 어느 행성에서 지구로 왔는가?\"}\n"
     ]
    }
   ],
   "source": [
    "from MRC import Extraction_based_MRC\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the datasets\n",
    "test_dataset = load_from_disk(\"/data/ephemeral/data/test_dataset\")\n",
    "\n",
    "# Access the 'validation' split\n",
    "test_dataset_validation = test_dataset['validation']\n",
    "\n",
    "# Display the column names of the datasets\n",
    "print(\"test_dataset_full column names:\", test_dataset_validation.column_names)\n",
    "print(\"retrieved_dataset column names:\", reranked_result['validation'].column_names)\n",
    "\n",
    "# Display the first example from each dataset\n",
    "print(\"First example in test_dataset_full:\", test_dataset_validation[0])\n",
    "print(\"First example in retrieved_dataset:\", reranked_result['validation'][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/model.safetensors\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/vocab.txt\n",
      "loading file tokenizer.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /data/ephemeral/home/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4m0gs1hu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-plant-709</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/4m0gs1hu' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/4m0gs1hu</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_173149-4m0gs1hu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4m0gs1hu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/test_mrc/wandb/run-20241021_174238-z5h4txp5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/z5h4txp5' target=\"_blank\">noble-star-710</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/z5h4txp5' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Dense_embedding_retrieval/runs/z5h4txp5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 'validation' split from test_dataset.\n",
      "Extracted 'validation' split from retrieved_dataset.\n",
      "Added retrieved contexts.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column validation not in the dataset. Current columns in the dataset: ['question', 'id', 'context']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 33\u001b[0m\n\u001b[1;32m     26\u001b[0m model_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 앙상블 추론 수행\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmrc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# 'validation' 분할 없이 전달\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieved_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieved_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 'validation' 분할 전달\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_paths\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ephemeral/test_mrc/MRC.py:494\u001b[0m, in \u001b[0;36mExtraction_based_MRC.ensemble_inference\u001b[0;34m(self, test_dataset, retrieved_dataset, model_paths)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded retrieved contexts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# MRC 데이터셋 준비\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m test_dataset_prepared, test_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mrc_test_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m all_start_logits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    497\u001b[0m all_end_logits \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/data/ephemeral/test_mrc/All_dataset.py:216\u001b[0m, in \u001b[0;36mprepare_dataset.get_mrc_test_dataset\u001b[0;34m(self, test_dataset)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mrc_test_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_dataset):\n\u001b[0;32m--> 216\u001b[0m     column_names \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    218\u001b[0m         test_examples \u001b[38;5;241m=\u001b[39m test_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2850\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2848\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2852\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2853\u001b[0m )\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:584\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    582\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 584\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 521\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column validation not in the dataset. Current columns in the dataset: ['question', 'id', 'context']\""
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from MRC import Extraction_based_MRC\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# MRC 모듈 리로드 및 Import\n",
    "importlib.reload(MRC)\n",
    "from MRC import Extraction_based_MRC\n",
    "\n",
    "# MRC 모델 초기화\n",
    "mrc_model = Extraction_based_MRC()\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "test_dataset_full = load_from_disk(\"/data/ephemeral/data/test_dataset\")\n",
    "\n",
    "# # 'validation' 분할만 추출\n",
    "# if 'validation' in reranked_result:\n",
    "#     retrieved_dataset = reranked_result['validation']\n",
    "#     print(\"Successfully extracted 'validation' split from reranked_result.\")\n",
    "# else:\n",
    "#     raise KeyError(\"'validation' split not found in reranked_result.\")\n",
    "\n",
    "retrieved_dataset = reranked_result\n",
    "\n",
    "\n",
    "# 훈련된 모델들의 경로 리스트\n",
    "model_paths = [\n",
    "    \"/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_bert-base\",\n",
    "    \"/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/monologg_koelectra-base-v3-discriminator\",\n",
    "    \"/data/ephemeral/home/practice/Template/Extraction_based_MRC_outputs/klue_roberta-base\",\n",
    "]\n",
    "\n",
    "# 앙상블 추론 수행\n",
    "mrc_model.ensemble_inference(\n",
    "    test_dataset=test_dataset_full,             # 'validation' 분할 없이 전달\n",
    "    retrieved_dataset=retrieved_dataset,       # 'validation' 분할 전달\n",
    "    model_paths=model_paths\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
